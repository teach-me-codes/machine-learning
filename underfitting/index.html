
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../overfitting/">
      
      
        <link rel="next" href="../cross-validation/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.25">
    
    
      
        <title>Underfitting - Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.6543a935.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config","UA-156178967-1"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","UA-156178967-1",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=UA-156178967-1",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#question" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Machine Learning" class="md-header__button md-logo" aria-label="Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Machine Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Underfitting
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Machine Learning" class="md-nav__button md-logo" aria-label="Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Machine Learning
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../linear_regression/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Linear Regression
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../logistic_regression/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Logistic Regression
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../support_vector_machine/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Support Vector Machine
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../decision_tree/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Decision Tree
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../k-nearest_neighbors/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    K-Nearest Neighbors
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../random_forest/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Random Forest
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../gradient_boosting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Gradient Boosting
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../naive_bayes/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Naive Bayes
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../principal_component_analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Principal Component Analysis
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../k-means_clustering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    K-Means Clustering
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../dimensionality_reduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dimensionality Reduction
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../ensemble_learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ensemble Learning
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../feature_engineering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Feature Engineering
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../overfitting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overfitting
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Underfitting
  </span>
  

      </a>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../cross-validation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Cross-Validation
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../bias-variance_tradeoff/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Bias-Variance Tradeoff
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../feature_selection/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Feature Selection
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../regularization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Regularization
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="question">Question</h1>
<p><strong>Main question</strong>: What is underfitting in the context of machine learning?</p>
<p><strong>Explanation</strong>: The candidate should explain what underfitting is, describing it as a scenario where a machine learning model is too simple, with insufficient capacity to capture the underlying pattern of the data.</p>
<p><strong>Follow-up questions</strong>:</p>
<ol>
<li>
<p>How can you detect underfitting in a machine learning model?</p>
</li>
<li>
<p>What are the typical signs of underfitting when evaluating a model?</p>
</li>
<li>
<p>Can underfitting affect the accuracy of predictions in real-world applications?</p>
</li>
</ol>
<h1 id="answer">Answer</h1>
<h1 id="answer_1">Answer</h1>
<p>In the context of machine learning, <strong>underfitting</strong> refers to a scenario where a model is too simplistic to capture the underlying pattern or structure of the data. This often occurs when the model has low complexity or is too generalized, leading to poor performance on both the training and test datasets. </p>
<p>Mathematically, underfitting can be represented as follows:</p>
<p>Let <span class="arithmatex"><span class="MathJax_Preview">h(x)</span><script type="math/tex">h(x)</script></span> be the hypothesis function of our machine learning model, and <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> be the true output. In the case of underfitting, <span class="arithmatex"><span class="MathJax_Preview">h(x)</span><script type="math/tex">h(x)</script></span> may be too simple, such as a linear function for a non-linear relationship, resulting in high bias and low variance.</p>
<p>This can be illustrated by a linear regression example where the true relationship between the features and target variable is non-linear, but the model is fitted with a linear line, as shown below:</p>
<div class="arithmatex">
<div class="MathJax_Preview"> y = \theta_0 + \theta_1 x </div>
<script type="math/tex; mode=display"> y = \theta_0 + \theta_1 x </script>
</div>
<p>To address underfitting, more complex models with higher capacity, such as adding polynomial features, increasing model complexity, or using more advanced algorithms, can be utilized.</p>
<h2 id="follow-up-questions">Follow-up Questions</h2>
<ol>
<li>
<p><strong>How can you detect underfitting in a machine learning model?</strong></p>
</li>
<li>
<p>One way to detect underfitting is by analyzing the model performance on both the training and validation datasets. If the model performs poorly on both sets, it might be a case of underfitting.</p>
</li>
<li>
<p>Another method is to plot learning curves, where the training and validation errors are plotted against the number of training instances. In the case of underfitting, both errors will remain high and close to each other.</p>
</li>
<li>
<p><strong>What are the typical signs of underfitting when evaluating a model?</strong></p>
</li>
<li>
<p>High training and validation errors that are close to each other.</p>
</li>
<li>
<p>Poor generalization of the model to unseen data.</p>
</li>
<li>
<p>The model fails to capture the underlying pattern in the data.</p>
</li>
<li>
<p><strong>Can underfitting affect the accuracy of predictions in real-world applications?</strong></p>
</li>
<li>
<p>Yes, underfitting can significantly impact the accuracy of predictions in real-world applications.</p>
</li>
<li>
<p>A model that is underfit will fail to capture the complexities and nuances present in the data, leading to inaccurate predictions and poor performance.</p>
</li>
<li>
<p>This can have serious consequences, especially in critical applications such as healthcare, finance, or autonomous vehicles, where accurate predictions are essential for decision-making.</p>
</li>
</ol>
<h1 id="question_1">Question</h1>
<p><strong>Main question</strong>: What are the common causes of underfitting?</p>
<p><strong>Explanation</strong>: The candidate should discuss the factors that typically lead to underfitting in machine learning models, including overly simplistic model choice and insufficient data features.</p>
<p><strong>Follow-up questions</strong>:</p>
<ol>
<li>
<p>How does the choice of model complexity contribute to underfitting?</p>
</li>
<li>
<p>Can the size and quality of training data play a role in underfitting?</p>
</li>
<li>
<p>What impact does feature selection have on the likelihood of underfitting?</p>
</li>
</ol>
<h1 id="answer_2">Answer</h1>
<h3 id="main-question-what-are-the-common-causes-of-underfitting">Main Question: What are the common causes of underfitting?</h3>
<p>Underfitting occurs when a machine learning model is too simple to capture the underlying patterns in the data. Some common causes of underfitting include:</p>
<ol>
<li>
<p><strong>Overly Simplistic Model Choice</strong>: Using a model that is too basic to represent the complexities in the data can lead to underfitting. For example, using a linear regression model for data with non-linear relationships could result in underfitting.</p>
</li>
<li>
<p><strong>Insufficient Data Features</strong>: When the dataset provided to train the model lacks important features or relevant information, the model may not have enough information to learn the underlying patterns effectively.</p>
</li>
</ol>
<h3 id="follow-up-questions_1">Follow-up Questions:</h3>
<ul>
<li>
<p><strong>How does the choice of model complexity contribute to underfitting?</strong></p>
<p>The choice of model complexity is crucial in determining whether a model will underfit or overfit the data. When a model is too simple (low complexity), it may struggle to capture the complexities in the data and result in underfitting. On the other hand, excessively complex models can lead to overfitting, where the model learns noise in the data rather than the underlying patterns. Finding the right balance in model complexity is key to avoiding underfitting.</p>
</li>
<li>
<p><strong>Can the size and quality of training data play a role in underfitting?</strong></p>
<p>Yes, the size and quality of the training data can significantly impact the occurrence of underfitting. Insufficient training data may not provide the model with enough examples to learn the underlying patterns effectively, leading to underfitting. Moreover, if the training data is not representative of the overall population or contains biases, the model may generalize poorly to unseen data, also contributing to underfitting.</p>
</li>
<li>
<p><strong>What impact does feature selection have on the likelihood of underfitting?</strong></p>
<p>Feature selection plays a crucial role in determining the model's ability to learn from the data. If important features are excluded during the feature selection process, the model may not have the necessary information to capture the underlying patterns adequately, leading to underfitting. Therefore, careful consideration needs to be given to feature selection to ensure that the model is provided with the relevant information to make accurate predictions.</p>
</li>
</ul>
<p>By addressing these factors and ensuring an appropriate level of model complexity, sufficient data features, and thoughtful feature selection, the likelihood of underfitting in machine learning models can be minimized.</p>
<h1 id="question_2">Question</h1>
<p><strong>Main question</strong>: How can you address underfitting in a machine learning model?</p>
<p><strong>Explanation</strong>: The candidate should describe various strategies to mitigate underfitting, such as selecting a more complex model or adding more features.</p>
<p><strong>Follow-up questions</strong>:</p>
<ol>
<li>
<p>What techniques can be used to increase the complexity of a model?</p>
</li>
<li>
<p>How does adding more training data help in reducing underfitting?</p>
</li>
<li>
<p>Can feature engineering be beneficial in addressing underfitting?</p>
</li>
</ol>
<h1 id="answer_3">Answer</h1>
<h3 id="addressing-underfitting-in-a-machine-learning-model">Addressing Underfitting in a Machine Learning Model</h3>
<p>Underfitting occurs when a model is too simple to accurately capture the underlying patterns in the data. This leads to poor performance on both the training and test datasets. To address underfitting, several strategies can be employed:</p>
<ol>
<li>
<p><strong>Increase Model Complexity</strong>: One of the primary ways to tackle underfitting is by using a more complex model that can capture the complexities in the data. This could involve switching to a more sophisticated algorithm or increasing the capacity of the current model.</p>
</li>
<li>
<p><strong>Adding More Features</strong>: By incorporating additional relevant features into the dataset, we can provide the model with more information to learn from. This added complexity can help the model better fit the data and reduce underfitting.</p>
</li>
<li>
<p><strong>Hyperparameter Tuning</strong>: Adjusting the hyperparameters of the model, such as the learning rate, regularization strength, or tree depth, can significantly impact the model's complexity and its ability to capture the underlying patterns in the data.</p>
</li>
</ol>
<h3 id="follow-up-questions_2">Follow-up Questions</h3>
<ul>
<li><strong>What techniques can be used to increase the complexity of a model?</strong></li>
<li>One technique is to increase the number of layers or neurons in a neural network.</li>
<li>Another approach is to use more advanced models like ensemble methods or deep learning architectures.</li>
<li>
<p>Feature transformations such as polynomial features can also introduce complexity.</p>
</li>
<li>
<p><strong>How does adding more training data help in reducing underfitting?</strong></p>
</li>
<li>Adding more training data provides the model with a larger and more diverse set of examples to learn from.</li>
<li>
<p>With more data, the model can better capture the underlying patterns and relationships in the dataset, reducing the chances of underfitting.</p>
</li>
<li>
<p><strong>Can feature engineering be beneficial in addressing underfitting?</strong></p>
</li>
<li>Feature engineering plays a crucial role in enhancing the model's ability to extract meaningful insights from the data.</li>
<li>Creating new features based on domain knowledge or through transformation techniques can introduce additional information that can help reduce underfitting.</li>
</ul>
<h1 id="question_3">Question</h1>
<p><strong>Main question</strong>: What role does feature engineering play in combating underfitting?</p>
<p><strong>Explanation</strong>: The candidate should explain the process and significance of feature engineering in enhancing model performance, particularly how it can help in overcoming underfitting.</p>
<p><strong>Follow-up questions</strong>:</p>
<ol>
<li>
<p>Can you give examples of feature engineering techniques that can help reduce underfitting?</p>
</li>
<li>
<p>How do you decide which features to engineer to address underfitting?</p>
</li>
<li>
<p>What is the impact of interaction terms in features with regard to underfitting?</p>
</li>
</ol>
<h1 id="answer_4">Answer</h1>
<h3 id="main-question-what-role-does-feature-engineering-play-in-combating-underfitting">Main question: What role does feature engineering play in combating underfitting?</h3>
<p>Underfitting occurs when a model is too simple to capture the underlying patterns in the data. Feature engineering plays a crucial role in combating underfitting by enriching the dataset with more meaningful and relevant features, allowing the model to better capture the underlying structure. </p>
<ul>
<li>
<p><strong>Feature Engineering Significance</strong>:</p>
<ul>
<li><strong>Enhanced Model Performance</strong>: Feature engineering helps in improving model performance by providing the model with more information to learn from.</li>
<li><strong>Addressing Underfitting</strong>: By adding new features or transforming existing ones, feature engineering helps the model to capture complex patterns in the data, thereby reducing underfitting.</li>
</ul>
</li>
<li>
<p><strong>Process of Feature Engineering</strong>:</p>
<ol>
<li><strong>Creation of New Features</strong>: Generating new features by combining or transforming existing ones.</li>
<li><strong>Handling Missing Data</strong>: Imputing missing values or creating new features indicating missingness.</li>
<li><strong>Scaling and Normalization</strong>: Ensuring all features are on a similar scale to avoid bias towards certain features.</li>
<li><strong>Encoding Categorical Variables</strong>: Converting categorical variables into numerical representations for model compatibility.</li>
<li><strong>Feature Selection</strong>: Identifying and selecting the most relevant features to be used in the model.</li>
</ol>
</li>
<li>
<p><strong>Significance in Overcoming Underfitting</strong>:</p>
<ul>
<li><strong>Increased Model Complexity</strong>: Feature engineering allows for more complex models to be built, enabling the model to capture intricate patterns in the data.</li>
<li><strong>Improved Generalization</strong>: By providing additional insights through engineered features, the model can generalize better to unseen data.</li>
</ul>
</li>
</ul>
<h3 id="follow-up-questions_3">Follow-up questions:</h3>
<ul>
<li>
<p><strong>Can you give examples of feature engineering techniques that can help reduce underfitting?</strong></p>
<ul>
<li><strong>Polynomial Features</strong>: Introducing polynomial features can help capture non-linear relationships in the data, increasing model complexity.</li>
<li><strong>Interaction Terms</strong>: Creating interaction terms between features can capture dependency relationships that the model might be missing.</li>
<li><strong>Feature Decomposition</strong>: Techniques like Principal Component Analysis (PCA) can help in reducing the dimensionality of the data while retaining important information.</li>
</ul>
</li>
<li>
<p><strong>How do you decide which features to engineer to address underfitting?</strong></p>
<ul>
<li><strong>Analyzing Correlations</strong>: Identifying correlations between features and the target variable can help in selecting features with strong predictive power.</li>
<li><strong>Domain Knowledge</strong>: Understanding the domain and the problem can guide the selection of features that are likely to be influential.</li>
<li><strong>Model Performance</strong>: Iteratively testing different sets of engineered features and evaluating model performance can help in selecting the most effective ones.</li>
</ul>
</li>
<li>
<p><strong>What is the impact of interaction terms in features with regard to underfitting?</strong></p>
<ul>
<li>Interaction terms can introduce non-linear relationships between features, allowing the model to capture more complex patterns in the data.</li>
<li>By incorporating interaction terms, the model's ability to fit the training data increases, reducing underfitting.</li>
<li>However, careful consideration is required to prevent overfitting, as interaction terms can also introduce noise if not properly selected and engineered.</li>
</ul>
</li>
</ul>
<p>By leveraging feature engineering techniques judiciously, machine learning models can combat underfitting and better capture the underlying structure of the data, leading to improved performance and generalization capabilities.</p>
<h1 id="question_4">Question</h1>
<p><strong>Main question</strong>: Why is choosing the right algorithm important to prevent underfitting?</p>
<p><strong>Explanation</strong>: The candidate should discuss how the choice of algorithm influences the likelihood of underfitting, emphasizing the need for matching model complexity with the complexity of the dataset.</p>
<p><strong>Follow-up questions</strong>:</p>
<ol>
<li>
<p>Can you compare two algorithms and their susceptibility to underfitting?</p>
</li>
<li>
<p>What criteria would you use to select an appropriate algorithm to prevent underfitting?</p>
</li>
<li>
<p>How do ensemble methods help in reducing the risk of underfitting?</p>
</li>
</ol>
<h1 id="answer_5">Answer</h1>
<h3 id="main-question-why-is-choosing-the-right-algorithm-important-to-prevent-underfitting">Main Question: Why is choosing the right algorithm important to prevent underfitting?</h3>
<p>When it comes to preventing underfitting in machine learning models, selecting the appropriate algorithm plays a crucial role in ensuring the model's ability to capture the underlying patterns in the data. Below are the reasons why choosing the right algorithm is important in preventing underfitting:</p>
<ol>
<li>
<p><strong>Model Complexity</strong>: The choice of algorithm determines the level of complexity the model can handle. If a too simple algorithm is chosen for a complex dataset, it may result in underfitting as the model fails to capture the intricate patterns present in the data.</p>
</li>
<li>
<p><strong>Flexibility of Model</strong>: Different algorithms have varying levels of flexibility in capturing complex relationships within the data. Selecting an algorithm with higher flexibility is essential for datasets with intricate structures to prevent underfitting.</p>
</li>
<li>
<p><strong>Feature Representation</strong>: Algorithms differ in their ability to represent and interpret features. Choosing an algorithm that can effectively represent the features of the dataset helps prevent underfitting by ensuring the model captures the relevant information.</p>
</li>
<li>
<p><strong>Bias-Variance Tradeoff</strong>: The bias-variance tradeoff is a critical concept in machine learning that influences the performance of models. Underfitting is often a result of high bias due to the oversimplified nature of the model. Selecting the right algorithm helps in striking a balance between bias and variance, reducing the risk of underfitting.</p>
</li>
</ol>
<p>By carefully selecting an algorithm that aligns with the complexity of the dataset and the underlying patterns, one can mitigate the risk of underfitting and build models that generalize well to unseen data.</p>
<h3 id="follow-up-questions_4">Follow-up Questions:</h3>
<ul>
<li><strong>Can you compare two algorithms and their susceptibility to underfitting?</strong></li>
</ul>
<p>Yes, I can compare two algorithms in terms of their susceptibility to underfitting. For example, a simple algorithm like Linear Regression is more prone to underfitting when dealing with highly non-linear data, as its linear nature may not capture the complex patterns effectively. On the other hand, decision tree-based algorithms such as Random Forest tend to be less susceptible to underfitting due to their ability to capture non-linear relationships in the data.</p>
<ul>
<li><strong>What criteria would you use to select an appropriate algorithm to prevent underfitting?</strong></li>
</ul>
<p>To select an appropriate algorithm to prevent underfitting, I would consider the following criteria:</p>
<ul>
<li>The complexity of the dataset</li>
<li>The flexibility of the algorithm</li>
<li>The feature representation capabilities</li>
<li>The tradeoff between bias and variance</li>
<li>
<p>The size of the dataset and presence of noisy data</p>
</li>
<li>
<p><strong>How do ensemble methods help in reducing the risk of underfitting?</strong></p>
</li>
</ul>
<p>Ensemble methods combine multiple base learners to create a strong predictive model. They help in reducing the risk of underfitting by aggregating the predictions of multiple models, thereby capturing a more comprehensive view of the data. By combining the strengths of individual models, ensemble methods can overcome the limitations of underfitting that may arise from using a single weak learner. Techniques such as bagging and boosting in ensemble methods contribute to enhancing the model's predictive performance and reducing the likelihood of underfitting.</p>
<h1 id="question_5">Question</h1>
<p><strong>Main question</strong>: How does model complexity relate to underfitting?</p>
<p><strong>Explanation</strong>: The candidate should define model complexity and explain its relationship with underfitting, particularly how insufficient complexity can lead to this issue.</p>
<p><strong>Follow-up questions</strong>:</p>
<ol>
<li>
<p>What are some indicators that a model may not be complex enough for a given dataset?</p>
</li>
<li>
<p>How do you balance model complexity to avoid both overfitting and underfitting?</p>
</li>
<li>
<p>What techniques can be used to incrementally increase model complexity during the development process?</p>
</li>
</ol>
<h1 id="answer_6">Answer</h1>
<h3 id="main-question-how-does-model-complexity-relate-to-underfitting">Main question: How does model complexity relate to underfitting?</h3>
<p>Model complexity refers to the sophistication or intricacy of a machine learning model in capturing the underlying patterns and relationships within the data. In the context of underfitting, model complexity plays a crucial role. When a model is too simple, it may not have enough capacity to capture the complexity of the data, leading to underfitting.</p>
<p>Mathematically, the relationship between model complexity and underfitting can be understood as follows:</p>
<ul>
<li>Let <span class="arithmatex"><span class="MathJax_Preview">h_{\theta}(x)</span><script type="math/tex">h_{\theta}(x)</script></span> represent a machine learning model with parameters <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>.</li>
<li>The model tries to learn a mapping from input features <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> to the output <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>.</li>
<li>If the model is too simple (low complexity), it may struggle to capture the true relationship between <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>.</li>
<li>This results in high bias and underfitting, as the model fails to generalize well on both the training and test datasets.</li>
</ul>
<p>To address underfitting, increasing the complexity of the model is necessary. This can be achieved by using more sophisticated models or increasing the model capacity to better capture the underlying patterns in the data.</p>
<h3 id="follow-up-questions_5">Follow-up questions:</h3>
<ul>
<li>
<p><strong>What are some indicators that a model may not be complex enough for a given dataset?</strong></p>
</li>
<li>
<p>High training error and high test error, indicating poor performance on both the training and unseen data.</p>
</li>
<li>The model shows limited improvement with additional training data.</li>
<li>
<p>The model struggles to capture the intricacies and nuances of the dataset, leading to oversimplified representations.</p>
</li>
<li>
<p><strong>How do you balance model complexity to avoid both overfitting and underfitting?</strong></p>
</li>
</ul>
<p>To balance model complexity effectively, one can employ techniques such as:</p>
<ul>
<li>Regularization methods like L1 (LASSO) and L2 (Ridge) regularization to prevent overfitting.</li>
<li>Cross-validation to tune hyperparameters and find the optimal complexity level.</li>
<li>
<p>Using techniques like early stopping to prevent overfitting by stopping training when performance on a validation set starts to degrade.</p>
</li>
<li>
<p><strong>What techniques can be used to incrementally increase model complexity during the development process?</strong></p>
</li>
</ul>
<p>Increasing model complexity incrementally is crucial to avoid sudden jumps that can lead to overfitting. Techniques to achieve this include:</p>
<ul>
<li>Adding more layers or neurons in neural networks gradually.</li>
<li>Increasing the degree of polynomial features in polynomial regression step by step.</li>
<li>Adjusting hyperparameters like depth in decision trees in a controlled manner.</li>
</ul>
<p>By incrementally increasing model complexity and regularly evaluating performance, one can strike a balance between underfitting and overfitting, leading to optimal model performance.</p>
<h1 id="question_6">Question</h1>
<p><strong>Main question</strong>: What is the role of cross-validation in addressing underfitting?</p>
<p><strong>Explanation</strong>: The candidate should describe how cross-validation can be used as a technique to gauge the effectiveness of a model in order to detect and manage underfitting.</p>
<p><strong>Follow-up questions</strong>:</p>
<ol>
<li>
<p>How does cross-validation help identify underfitting?</p>
</li>
<li>
<p>What cross-validation strategies are most effective for detecting underfitting?</p>
</li>
<li>
<p>Can you explain the process of k-fold cross-validation and its relevance to underfitting?</p>
</li>
</ol>
<h1 id="answer_7">Answer</h1>
<h3 id="main-question-what-is-the-role-of-cross-validation-in-addressing-underfitting">Main question: What is the role of cross-validation in addressing underfitting?</h3>
<p>Underfitting occurs when a model is too simplistic to capture the underlying patterns in the data, resulting in poor performance on both the training and test datasets. Cross-validation plays a crucial role in assessing the performance of a model and detecting underfitting by allowing multiple training and testing iterations on different subsets of the data. This technique helps in evaluating how well a model generalizes to unseen data and can guide us in determining whether the model is underfitting.</p>
<div class="arithmatex">
<div class="MathJax_Preview">\text{Underfitting} \rightarrow \text{Simple model} \rightarrow \text{Poor performance}</div>
<script type="math/tex; mode=display">\text{Underfitting} \rightarrow \text{Simple model} \rightarrow \text{Poor performance}</script>
</div>
<p>By using cross-validation, we can effectively assess the model's performance across various data splits, providing insights into whether the model is too simple and fails to capture the complexities present in the data.</p>
<h3 id="follow-up-questions_6">Follow-up questions:</h3>
<ul>
<li><strong>How does cross-validation help identify underfitting?</strong></li>
</ul>
<p>Cross-validation helps identify underfitting by repeatedly splitting the data into training and validation sets, training the model on the training set, and evaluating its performance on the validation set. If the model consistently performs poorly on the validation sets across multiple iterations, it indicates that the model is too simple and unable to capture the underlying patterns in the data.</p>
<ul>
<li><strong>What cross-validation strategies are most effective for detecting underfitting?</strong></li>
</ul>
<p>Some of the most effective cross-validation strategies for detecting underfitting include:
  - <strong>K-Fold Cross-Validation:</strong> It involves dividing the data into k subsets or folds and using each fold as a validation set while the remaining folds are used for training. This technique provides a more robust estimate of the model's performance compared to a single train-test split.
  - <strong>Stratified Cross-Validation:</strong> Ensures that each fold maintains the same class distribution as the original dataset, which is beneficial when dealing with imbalanced datasets.
  - <strong>Leave-One-Out Cross-Validation (LOOCV):</strong> In this strategy, a single data point is used as the validation set while the remaining data is used for training. This process is repeated for each data point, providing a comprehensive evaluation but can be computationally expensive.</p>
<ul>
<li><strong>Can you explain the process of k-fold cross-validation and its relevance to underfitting?</strong></li>
</ul>
<p><strong>Process of k-Fold Cross-Validation:</strong></p>
<ol>
<li>The dataset is divided into k subsets/folds of equal size.</li>
<li>The model is trained on k-1 folds and validated on the remaining fold. This process is repeated k times, each time using a different fold as the validation set.</li>
<li>The performance metrics are averaged across all k iterations to obtain a more reliable estimate of the model's performance.</li>
</ol>
<p><strong>Relevance to Underfitting:</strong></p>
<p>K-fold cross-validation is relevant to underfitting as it allows us to assess the model's performance on multiple subsets of data. If the model consistently performs poorly across all folds, it suggests that the model is too simple and is underfitting the data. In contrast, if the model performs well on some folds but poorly on others, it may indicate issues such as overfitting or data leakage. Therefore, k-fold cross-validation is a valuable technique for diagnosing underfitting and selecting a model that captures the data's underlying patterns effectively.</p>
<h1 id="question_7">Question</h1>
<p><strong>Main question</strong>: How do training and validation learning curves help in diagnosing underfitting?</p>
<p><strong>Explanation</strong>: The candidate should explain the use of learning curves and how these graphs can indicate underfitting in a model based on its performance on training and validation datasets.</p>
<p><strong>Follow-up questions</strong>:</p>
<ol>
<li>
<p>What do typical underfitting curves look like on these plots?</p>
</li>
<li>
<p>How can adjustments be made based on insights from learning curves?</p>
</li>
<li>
<p>What are the limitations of using learning curves to diagnose underfitting?</p>
</li>
</ol>
<h1 id="answer_8">Answer</h1>
<h2 id="main-question-how-do-training-and-validation-learning-curves-help-in-diagnosing-underfitting">Main question: How do training and validation learning curves help in diagnosing underfitting?</h2>
<p>In the context of machine learning, learning curves are plots that depict the model's performance on the training and validation datasets as a function of the training dataset size or the training iterations. These learning curves can be a powerful tool in diagnosing underfitting in a model. </p>
<ul>
<li>
<p><strong>Training Learning Curve</strong>: </p>
<ul>
<li>When a model underfits the data, it fails to capture the underlying patterns or relationships in the training data. This leads to high training error as the model is not complex enough to fit the data well. As a result, the training learning curve will show a large training error that remains high even as the training dataset size increases.</li>
</ul>
</li>
<li>
<p><strong>Validation Learning Curve</strong>:</p>
<ul>
<li>Similarly, the validation learning curve reflects the model's performance on unseen data. In the case of underfitting, the validation error will also be high as the model is too simple to generalize well to new data. The validation learning curve will exhibit high error that plateaus or decreases very slowly with more data.</li>
</ul>
</li>
</ul>
<p>When diagnosing underfitting using learning curves:
- If both the training and validation errors are high and plateau, it is a clear indication that the model is underfitting the data.
- By analyzing these learning curves, one can determine that the model's performance can be improved by increasing its complexity or using more sophisticated algorithms.</p>
<h2 id="follow-up-questions_7">Follow-up questions:</h2>
<ol>
<li>
<p><strong>What do typical underfitting curves look like on these plots?</strong></p>
<ul>
<li>Typical underfitting curves on learning plots will show high error values that plateau or decrease very slowly as the training dataset size or iterations increase. Both training and validation curves will exhibit this behavior, indicating that the model is too simple to capture the underlying data patterns effectively.</li>
</ul>
</li>
<li>
<p><strong>How can adjustments be made based on insights from learning curves?</strong></p>
<ul>
<li>Based on insights from learning curves, adjustments to address underfitting can include:<ul>
<li>Increasing model complexity by adding more layers, neurons, or features.</li>
<li>Using more advanced models that are better suited to capture complex patterns in the data.</li>
<li>Adjusting hyperparameters such as learning rate, regularization strength, or optimization algorithms.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>What are the limitations of using learning curves to diagnose underfitting?</strong></p>
<ul>
<li>Limitations of using learning curves for diagnosing underfitting include:<ul>
<li>Learning curves may not be able to differentiate between underfitting and overfitting if the model complexity is not properly tuned.</li>
<li>Noise in the data or outliers can impact the learning curves and lead to incorrect interpretations.</li>
<li>Learning curves provide insights based on the given dataset and may not generalize well to unseen data if the dataset distribution shifts.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h1 id="question_8">Question</h1>
<p><strong>Main question</strong>: What is the impact of the bias-variance tradeoff on underfitting?</p>
<p><strong>Explanation</strong>: The candidate should clarify the concept of the bias-variance tradeoff and explain how a high bias is indicative of underfitting.</p>
<p><strong>Follow-up questions</strong>:</p>
<ol>
<li>
<p>How do you assess if a model has high bias?</p>
</li>
<li>
<p>What steps can be taken to reduce bias in a machine learning model?</p>
</li>
<li>
<p>Can you discuss any techniques specifically aimed at balancing bias and variance to optimize model performance?</p>
</li>
</ol>
<h1 id="answer_9">Answer</h1>
<h1 id="impact-of-bias-variance-tradeoff-on-underfitting-in-machine-learning">Impact of Bias-Variance Tradeoff on Underfitting in Machine Learning</h1>
<p>Underfitting occurs when a model is too simple to capture the underlying structure of the data. This results in poor performance on both the training and test datasets. The bias-variance tradeoff plays a crucial role in underfitting as it influences the model's ability to generalize well to unseen data.</p>
<h3 id="bias-variance-tradeoff">Bias-Variance Tradeoff:</h3>
<p>The bias-variance tradeoff is a fundamental concept in machine learning that aims to find the right balance between bias and variance to minimize the model's prediction error. </p>
<ol>
<li>
<p><strong>Bias</strong>: Bias refers to the error introduced by approximating a real-world problem, which can lead to underfitting. High bias models make strong assumptions about the data and oversimplify the underlying patterns, resulting in poor performance.</p>
</li>
<li>
<p><strong>Variance</strong>: Variance measures the model's sensitivity to fluctuations in the training data. High variance models are complex and flexible, capturing noise in the training data and leading to overfitting.</p>
</li>
</ol>
<h3 id="impact-on-underfitting">Impact on Underfitting:</h3>
<ul>
<li><strong>High Bias</strong>: A model with high bias is indicative of underfitting as it fails to capture the underlying patterns in the data. This results in high errors on both the training and test sets.</li>
</ul>
<h3 id="follow-up-questions_8">Follow-up Questions:</h3>
<ul>
<li><strong>How do you assess if a model has high bias?</strong></li>
<li>
<p>High bias is usually identified by a model's poor performance on both the training and test datasets. Discrepancy between training and validation/test set performance can also indicate high bias.</p>
</li>
<li>
<p><strong>What steps can be taken to reduce bias in a machine learning model?</strong></p>
</li>
<li>
<p>To reduce bias in a model, we can:</p>
<ul>
<li>Increase model complexity by adding more features or increasing the depth of a neural network.</li>
<li>Train the model for more epochs to allow it to learn more complex patterns.</li>
</ul>
</li>
<li>
<p><strong>Can you discuss any techniques aimed at balancing bias and variance to optimize model performance?</strong></p>
</li>
<li><strong>Regularization</strong>: Techniques like L1 and L2 regularization can be used to penalize complex models and prevent overfitting.</li>
<li><strong>Cross-validation</strong>: Utilizing cross-validation helps in evaluating the model's performance on different subsets of the data, ensuring a balance between bias and variance.</li>
<li><strong>Ensemble Methods</strong>: Models like Random Forest and Gradient Boosting combine multiple models to reduce bias and variance, improving overall performance. </li>
</ul>
<p>By understanding the bias-variance tradeoff and its impact on underfitting, we can make informed decisions to optimize machine learning models for better performance.</p>
<h1 id="question_9">Question</h1>
<p><strong>Main question</strong>: How can parameter tuning help resolve underfitting?</p>
<p><strong>Explanation</strong>: The candidate should discuss how adjusting the parameters of a machine learning model can help in increasing its capacity to learn and thus mitigate underfitting.</p>
<h1 id="answer_10">Answer</h1>
<h1 id="how-can-parameter-tuning-help-resolve-underfitting">How can parameter tuning help resolve underfitting?</h1>
<p>When a machine learning model is underfitting, it means that the model is too simplistic and unable to capture the underlying patterns in the data. Parameter tuning plays a crucial role in addressing underfitting by adjusting the parameters of the model to increase its complexity and capacity to learn from the data.</p>
<p>One common approach to address underfitting through parameter tuning is to increase the complexity of the model by adjusting the hyperparameters. By fine-tuning the hyperparameters, we can make the model more flexible and better able to fit the training data, thus reducing underfitting.</p>
<p>Parameter tuning can help resolve underfitting by allowing the model to learn more complex patterns in the data, leading to improved performance on both the training and test datasets.</p>
<h1 id="follow-up-questions_9">Follow-up questions</h1>
<ul>
<li><strong>What parameters are commonly tuned to address underfitting in models?</strong></li>
</ul>
<p>Common parameters that are tuned to address underfitting in models include:</p>
<ul>
<li><strong>Learning rate</strong>: adjusting the rate at which the model updates its parameters during training.</li>
<li><strong>Number of hidden layers</strong>: increasing the number of hidden layers in a neural network to capture more complex patterns.</li>
<li><strong>Number of neurons</strong>: adjusting the number of neurons in each layer to increase the model's capacity.</li>
<li><strong>Regularization</strong>: adding regularization terms like L1 or L2 regularization to prevent overfitting and underfitting.</li>
<li>
<p><strong>Activation functions</strong>: changing the activation functions to introduce non-linearity and capture complex relationships in the data.</p>
</li>
<li>
<p><strong>How does tuning parameters affect the complexity of the model?</strong></p>
</li>
</ul>
<p>Tuning parameters can significantly impact the complexity of the model. By adjusting hyperparameters such as the learning rate, number of layers, neurons, and regularization, we can increase the model's capacity to learn from the data. This increased complexity enables the model to capture more intricate patterns and relationships within the dataset, reducing underfitting.</p>
<ul>
<li><strong>Can you provide an example of a tuning process that helped overcome underfitting?</strong></li>
</ul>
<p>One common example of a tuning process to overcome underfitting is adjusting the learning rate in a gradient descent optimization algorithm. If the learning rate is too low, the model may underfit the data as it updates its parameters very slowly. By increasing the learning rate, the model can learn faster and better fit the training data, thus reducing underfitting. Here is a simple code snippet demonstrating how learning rate tuning can be implemented:</p>
<p>```python
  model = create_model()  # Create your machine learning model
  optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)  # Set initial learning rate
  model.compile(optimizer=optimizer, loss='mse')  # Compile the model</p>
<p># Train the model with adjusted learning rate
  history = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val))
  ```</p>









  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.081f42fc.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  </body>
</html>