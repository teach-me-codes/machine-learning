{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction to Machine Learning This tutorial guide you to fundamentals of Machine Learning with Linear regressin, logistic regression, classifiers, decision tree, supportvector machine, ensamble methods and boosting, Perceptron, Neural Network, CNN and LSTM etc.","title":"Home"},{"location":"#introduction-to-machine-learning","text":"This tutorial guide you to fundamentals of Machine Learning with Linear regressin, logistic regression, classifiers, decision tree, supportvector machine, ensamble methods and boosting, Perceptron, Neural Network, CNN and LSTM etc.","title":"Introduction to Machine Learning"},{"location":"clustering/","text":"Clustering: Agglomerative clustering with and without structure ( Source ) This example shows the effect of imposing a connectivity graph to capture local structure in the data. The graph is simply the graph of 20 nearest neighbors. Two consequences of imposing a connectivity can be seen. First clustering with a connectivity matrix is much faster. Second, when using a connectivity matrix, single, average and complete linkage are unstable and tend to create a few clusters that grow very quickly. Indeed, average and complete linkage fight this percolation behavior by considering all the distances between two clusters when merging them ( while single linkage exaggerates the behaviour by considering only the shortest distance between clusters). The connectivity graph breaks this mechanism for average and complete linkage, making them resemble the more brittle single linkage. This effect is more pronounced for very sparse graphs (try decreasing the number of neighbors in kneighbors_graph) and with complete linkage. In particular, having a very small number of neighbors in the graph, imposes a geometry that is close to that of single linkage, which is well known to have this percolation instability. \"\"\" Authors: Gael Varoquaux, Nelle Varoquaux License: BSD 3 clause import time import matplotlib.pyplot as plt import numpy as np from sklearn.cluster import AgglomerativeClustering from sklearn.neighbors import kneighbors_graph # Generate sample data n_samples = 1500 np . random . seed( 0 ) t = 1.5 * np . pi * ( 1 + 3 * np . random . rand( 1 , n_samples)) x = t * np . cos(t) y = t * np . sin(t) X = np . concatenate((x, y)) X += .7 * np . random . randn( 2 , n_samples) X = X . T # Create a graph capturing local connectivity. Larger number of neighbors # will give more homogeneous clusters to the cost of computation # time. A very large number of neighbors gives more evenly distributed # cluster sizes, but may not impose the local manifold structure of # the data knn_graph = kneighbors_graph(X, 30 , include_self = False ) for connectivity in ( None , knn_graph): for n_clusters in ( 30 , 3 ): plt . figure(figsize = ( 10 , 4 )) for index, linkage in enumerate (( 'average' , 'complete' , 'ward' , 'single' )): plt . subplot( 1 , 4 , index + 1 ) model = AgglomerativeClustering(linkage = linkage, connectivity = connectivity, n_clusters = n_clusters) t0 = time . time() model . fit(X) elapsed_time = time . time() - t0 plt . scatter(X[:, 0 ], X[:, 1 ], c = model . labels_, cmap = plt . cm . nipy_spectral) plt . title( 'linkage= %s \\n (time %.2f s)' % (linkage, elapsed_time), fontdict = dict (verticalalignment = 'top' )) plt . axis( 'equal' ) plt . axis( 'off' ) plt . subplots_adjust(bottom = 0 , top = .83 , wspace = 0 , left = 0 , right = 1 ) plt . suptitle( 'n_cluster= %i , connectivity= %r ' % (n_clusters, connectivity is not None ), size = 17 ) plt . show()","title":"Clustering"},{"location":"clustering/#clustering","text":"Agglomerative clustering with and without structure ( Source ) This example shows the effect of imposing a connectivity graph to capture local structure in the data. The graph is simply the graph of 20 nearest neighbors. Two consequences of imposing a connectivity can be seen. First clustering with a connectivity matrix is much faster. Second, when using a connectivity matrix, single, average and complete linkage are unstable and tend to create a few clusters that grow very quickly. Indeed, average and complete linkage fight this percolation behavior by considering all the distances between two clusters when merging them ( while single linkage exaggerates the behaviour by considering only the shortest distance between clusters). The connectivity graph breaks this mechanism for average and complete linkage, making them resemble the more brittle single linkage. This effect is more pronounced for very sparse graphs (try decreasing the number of neighbors in kneighbors_graph) and with complete linkage. In particular, having a very small number of neighbors in the graph, imposes a geometry that is close to that of single linkage, which is well known to have this percolation instability. \"\"\" Authors: Gael Varoquaux, Nelle Varoquaux License: BSD 3 clause import time import matplotlib.pyplot as plt import numpy as np from sklearn.cluster import AgglomerativeClustering from sklearn.neighbors import kneighbors_graph # Generate sample data n_samples = 1500 np . random . seed( 0 ) t = 1.5 * np . pi * ( 1 + 3 * np . random . rand( 1 , n_samples)) x = t * np . cos(t) y = t * np . sin(t) X = np . concatenate((x, y)) X += .7 * np . random . randn( 2 , n_samples) X = X . T # Create a graph capturing local connectivity. Larger number of neighbors # will give more homogeneous clusters to the cost of computation # time. A very large number of neighbors gives more evenly distributed # cluster sizes, but may not impose the local manifold structure of # the data knn_graph = kneighbors_graph(X, 30 , include_self = False ) for connectivity in ( None , knn_graph): for n_clusters in ( 30 , 3 ): plt . figure(figsize = ( 10 , 4 )) for index, linkage in enumerate (( 'average' , 'complete' , 'ward' , 'single' )): plt . subplot( 1 , 4 , index + 1 ) model = AgglomerativeClustering(linkage = linkage, connectivity = connectivity, n_clusters = n_clusters) t0 = time . time() model . fit(X) elapsed_time = time . time() - t0 plt . scatter(X[:, 0 ], X[:, 1 ], c = model . labels_, cmap = plt . cm . nipy_spectral) plt . title( 'linkage= %s \\n (time %.2f s)' % (linkage, elapsed_time), fontdict = dict (verticalalignment = 'top' )) plt . axis( 'equal' ) plt . axis( 'off' ) plt . subplots_adjust(bottom = 0 , top = .83 , wspace = 0 , left = 0 , right = 1 ) plt . suptitle( 'n_cluster= %i , connectivity= %r ' % (n_clusters, connectivity is not None ), size = 17 ) plt . show()","title":"Clustering:"},{"location":"linear/","text":"Linear models A Simple Linear Regression Example ( source ) This example uses the only the first feature of the diabetes dataset, in order to illustrate a two-dimensional plot of this regression technique. The straight line can be seen in the plot, showing how linear regression attempts to draw a straight line that will best minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation. The coefficients , the residual sum of squares and the variance score are also calculated. import numpy as np from sklearn import datasets, linear_model Import Diabetes Dataset diabetes = datasets . load_diabetes() '''Use only one feature''' diabetes_X = diabetes . data[:, np . newaxis, 2 ] Split Dataset to Train and Test '''Split the data into training/testing sets''' diabetes_X_train = diabetes_X[: - 20 ] diabetes_X_test = diabetes_X[ - 20 :] '''Split the targets into training/testing sets''' diabetes_y_train = diabetes . target[: - 20 ] diabetes_y_test = diabetes . target[ - 20 :] Fit the Model '''Create linear regression object''' regr = linear_model . LinearRegression() '''Train the model using the training sets''' regr . fit(diabetes_X_train, diabetes_y_train) LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) Calculate Regression cofficient, Mean Squared Error and Variance score. '''The coefficients''' print ( 'Coefficients: \\n ' , regr . coef_) '''The mean squared error''' print ( \"Mean squared error: %.2f \" % np . mean((regr . predict(diabetes_X_test) - diabetes_y_test) ** 2 )) '''Explained variance score: 1 is perfect prediction''' print ( 'Variance score: %.2f ' % regr . score(diabetes_X_test, diabetes_y_test)) Coefficients: [938.23786125] Mean squared error: 2548.07 Variance score: 0.47 import matplotlib.pyplot as plt import seaborn as sns sns . set() '''Plot outputs''' plt . figure(figsize = [ 12 , 8 ]) plt . scatter(diabetes_X_test,\\ diabetes_y_test,\\ color = 'black' ) plt . plot(diabetes_X_test,\\ regr . predict(diabetes_X_test),\\ color = 'blue' , linewidth = 3 ) plt . grid( True ) plt . show()","title":"Linear Regression"},{"location":"linear/#linear-models","text":"A Simple Linear Regression Example ( source ) This example uses the only the first feature of the diabetes dataset, in order to illustrate a two-dimensional plot of this regression technique. The straight line can be seen in the plot, showing how linear regression attempts to draw a straight line that will best minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation. The coefficients , the residual sum of squares and the variance score are also calculated. import numpy as np from sklearn import datasets, linear_model Import Diabetes Dataset diabetes = datasets . load_diabetes() '''Use only one feature''' diabetes_X = diabetes . data[:, np . newaxis, 2 ] Split Dataset to Train and Test '''Split the data into training/testing sets''' diabetes_X_train = diabetes_X[: - 20 ] diabetes_X_test = diabetes_X[ - 20 :] '''Split the targets into training/testing sets''' diabetes_y_train = diabetes . target[: - 20 ] diabetes_y_test = diabetes . target[ - 20 :] Fit the Model '''Create linear regression object''' regr = linear_model . LinearRegression() '''Train the model using the training sets''' regr . fit(diabetes_X_train, diabetes_y_train) LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) Calculate Regression cofficient, Mean Squared Error and Variance score. '''The coefficients''' print ( 'Coefficients: \\n ' , regr . coef_) '''The mean squared error''' print ( \"Mean squared error: %.2f \" % np . mean((regr . predict(diabetes_X_test) - diabetes_y_test) ** 2 )) '''Explained variance score: 1 is perfect prediction''' print ( 'Variance score: %.2f ' % regr . score(diabetes_X_test, diabetes_y_test)) Coefficients: [938.23786125] Mean squared error: 2548.07 Variance score: 0.47 import matplotlib.pyplot as plt import seaborn as sns sns . set() '''Plot outputs''' plt . figure(figsize = [ 12 , 8 ]) plt . scatter(diabetes_X_test,\\ diabetes_y_test,\\ color = 'black' ) plt . plot(diabetes_X_test,\\ regr . predict(diabetes_X_test),\\ color = 'blue' , linewidth = 3 ) plt . grid( True ) plt . show()","title":"Linear models"},{"location":"logistic/","text":"Logistic Regression 3-class Classifier ( source ) Show below is a logistic-regression classifiers decision boundaries on the iris <https://en.wikipedia.org/wiki/Iris_flower_data_set> _ dataset. The datapoints are colored according to their labels. import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model, datasets # import some data to play with iris = datasets . load_iris() X = iris . data[:, : 2 ] # we only take the first two features. Y = iris . target h = .02 # step size in the mesh logreg = linear_model . LogisticRegression(C = 1e5 ) # we create an instance of Neighbours # Classifier and fit the data. logreg . fit(X, Y) # Plot the decision boundary. # For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. x_min, x_max = X[:, 0 ] . min() - .5 , X[:, 0 ] . max() + .5 y_min, y_max = X[:, 1 ] . min() - .5 , X[:, 1 ] . max() + .5 xx, yy = np . meshgrid(np . arange(x_min, x_max, h),\\ np . arange(y_min, y_max, h)) Z = logreg . predict(np . c_[xx . ravel(), yy . ravel()]) # Put the result into a color plot Z = Z . reshape(xx . shape) plt . figure( 1 , figsize = ( 10 , 6 )) plt . pcolormesh(xx, yy, Z, cmap = plt . cm . Paired) # Plot also the training points plt . scatter(X[:, 0 ], X[:, 1 ], c = Y,\\ edgecolors = 'k' , cmap = plt . cm . Paired) plt . xlabel( 'Sepal length' ) plt . ylabel( 'Sepal width' ) plt . xlim(xx . min(), xx . max()) plt . ylim(yy . min(), yy . max()) plt . xticks(()) plt . yticks(()) plt . show()","title":"Logistic Regression"},{"location":"logistic/#logistic-regression","text":"3-class Classifier ( source ) Show below is a logistic-regression classifiers decision boundaries on the iris <https://en.wikipedia.org/wiki/Iris_flower_data_set> _ dataset. The datapoints are colored according to their labels. import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model, datasets # import some data to play with iris = datasets . load_iris() X = iris . data[:, : 2 ] # we only take the first two features. Y = iris . target h = .02 # step size in the mesh logreg = linear_model . LogisticRegression(C = 1e5 ) # we create an instance of Neighbours # Classifier and fit the data. logreg . fit(X, Y) # Plot the decision boundary. # For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. x_min, x_max = X[:, 0 ] . min() - .5 , X[:, 0 ] . max() + .5 y_min, y_max = X[:, 1 ] . min() - .5 , X[:, 1 ] . max() + .5 xx, yy = np . meshgrid(np . arange(x_min, x_max, h),\\ np . arange(y_min, y_max, h)) Z = logreg . predict(np . c_[xx . ravel(), yy . ravel()]) # Put the result into a color plot Z = Z . reshape(xx . shape) plt . figure( 1 , figsize = ( 10 , 6 )) plt . pcolormesh(xx, yy, Z, cmap = plt . cm . Paired) # Plot also the training points plt . scatter(X[:, 0 ], X[:, 1 ], c = Y,\\ edgecolors = 'k' , cmap = plt . cm . Paired) plt . xlabel( 'Sepal length' ) plt . ylabel( 'Sepal width' ) plt . xlim(xx . min(), xx . max()) plt . ylim(yy . min(), yy . max()) plt . xticks(()) plt . yticks(()) plt . show()","title":"Logistic Regression"},{"location":"nnc/","text":"Nearest Neighbors Nearest Neighbors Classification ( Source ) Sample usage of Nearest Neighbors classification. It will plot the decision boundaries for each class. import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap from sklearn import neighbors, datasets n_neighbors = 15 # import some data to play with iris = datasets . load_iris() # we only take the first two features. We could avoid this ugly # slicing by using a two-dim dataset X = iris . data[:, : 2 ] y = iris . target h = .02 # step size in the mesh # Create color maps cmap_light = ListedColormap([ 'orange' , 'cyan' , 'cornflowerblue' ]) cmap_bold = ListedColormap([ 'darkorange' , 'c' , 'darkblue' ]) for weights in [ 'uniform' , 'distance' ]: # we create an instance of Neighbours Classifier and fit the data. clf = neighbors . KNeighborsClassifier(n_neighbors, weights = weights) clf . fit(X, y) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. x_min, x_max = X[:, 0 ] . min() - 1 , X[:, 0 ] . max() + 1 y_min, y_max = X[:, 1 ] . min() - 1 , X[:, 1 ] . max() + 1 xx, yy = np . meshgrid(np . arange(x_min, x_max, h), np . arange(y_min, y_max, h)) Z = clf . predict(np . c_[xx . ravel(), yy . ravel()]) # Put the result into a color plot Z = Z . reshape(xx . shape) plt . figure(figsize = [ 10 , 6 ]) plt . pcolormesh(xx, yy, Z, cmap = cmap_light) # Plot also the training points plt . scatter(X[:, 0 ], X[:, 1 ], c = y, cmap = cmap_bold, edgecolor = 'k' , s = 20 ) plt . xlim(xx . min(), xx . max()) plt . ylim(yy . min(), yy . max()) plt . title( \"3-Class classification (k = %i , weights = ' %s ')\" % (n_neighbors, weights)) plt . show()","title":"Nearest Neighbours"},{"location":"nnc/#nearest-neighbors","text":"Nearest Neighbors Classification ( Source ) Sample usage of Nearest Neighbors classification. It will plot the decision boundaries for each class. import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap from sklearn import neighbors, datasets n_neighbors = 15 # import some data to play with iris = datasets . load_iris() # we only take the first two features. We could avoid this ugly # slicing by using a two-dim dataset X = iris . data[:, : 2 ] y = iris . target h = .02 # step size in the mesh # Create color maps cmap_light = ListedColormap([ 'orange' , 'cyan' , 'cornflowerblue' ]) cmap_bold = ListedColormap([ 'darkorange' , 'c' , 'darkblue' ]) for weights in [ 'uniform' , 'distance' ]: # we create an instance of Neighbours Classifier and fit the data. clf = neighbors . KNeighborsClassifier(n_neighbors, weights = weights) clf . fit(X, y) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. x_min, x_max = X[:, 0 ] . min() - 1 , X[:, 0 ] . max() + 1 y_min, y_max = X[:, 1 ] . min() - 1 , X[:, 1 ] . max() + 1 xx, yy = np . meshgrid(np . arange(x_min, x_max, h), np . arange(y_min, y_max, h)) Z = clf . predict(np . c_[xx . ravel(), yy . ravel()]) # Put the result into a color plot Z = Z . reshape(xx . shape) plt . figure(figsize = [ 10 , 6 ]) plt . pcolormesh(xx, yy, Z, cmap = cmap_light) # Plot also the training points plt . scatter(X[:, 0 ], X[:, 1 ], c = y, cmap = cmap_bold, edgecolor = 'k' , s = 20 ) plt . xlim(xx . min(), xx . max()) plt . ylim(yy . min(), yy . max()) plt . title( \"3-Class classification (k = %i , weights = ' %s ')\" % (n_neighbors, weights)) plt . show()","title":"Nearest Neighbors"},{"location":"perceptron/","text":"Perceptron Varying regularization in Multi-layer Perceptron ( Source ) A comparison of different values for regularization parameter 'alpha' on synthetic datasets. The plot shows that different alphas yield different decision functions. Alpha is a parameter for regularization term, aka penalty term, that combats overfitting by constraining the size of the weights. Increasing alpha may fix high variance (a sign of overfitting) by encouraging smaller weights, resulting in a decision boundary plot that appears with lesser curvatures. Similarly, decreasing alpha may fix high bias (a sign of underfitting) by encouraging larger weights, potentially resulting in a more complicated decision boundary. Author: Issam H. Laradji License: BSD 3 clause import numpy as np from matplotlib import pyplot as plt from matplotlib.colors import ListedColormap from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.datasets import make_moons, make_circles, make_classification from sklearn.neural_network import MLPClassifier from sklearn.pipeline import make_pipeline h = .02 # step size in the mesh alphas = np . logspace( - 5 , 3 , 5 ) names = [ 'alpha ' + str (i) for i in alphas] classifiers = [] for i in alphas: classifiers . append(make_pipeline( StandardScaler(), MLPClassifier(solver = 'lbfgs' , alpha = i, random_state = 1 , max_iter = 2000 , early_stopping = True , hidden_layer_sizes = [ 100 , 100 ]) )) X, y = make_classification(n_features = 2 , n_redundant = 0 , n_informative = 2 , random_state = 0 , n_clusters_per_class = 1 ) rng = np . random . RandomState( 2 ) X += 2 * rng . uniform(size = X . shape) linearly_separable = (X, y) datasets = [make_moons(noise = 0.3 , random_state = 0 ), make_circles(noise = 0.2 , factor = 0.5 , random_state = 1 ), linearly_separable] figure = plt . figure(figsize = ( 17 , 9 )) i = 1 # iterate over datasets for X, y in datasets: # preprocess dataset, split into training and test part X = StandardScaler() . fit_transform(X) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .4 ) x_min, x_max = X[:, 0 ] . min() - .5 , X[:, 0 ] . max() + .5 y_min, y_max = X[:, 1 ] . min() - .5 , X[:, 1 ] . max() + .5 xx, yy = np . meshgrid(np . arange(x_min, x_max, h), np . arange(y_min, y_max, h)) # just plot the dataset first cm = plt . cm . RdBu cm_bright = ListedColormap([ '#FF0000' , '#0000FF' ]) ax = plt . subplot( len (datasets), len (classifiers) + 1 , i) # Plot the training points ax . scatter(X_train[:, 0 ], X_train[:, 1 ], c = y_train, cmap = cm_bright) # and testing points ax . scatter(X_test[:, 0 ], X_test[:, 1 ], c = y_test, cmap = cm_bright, alpha = 0.6 ) ax . set_xlim(xx . min(), xx . max()) ax . set_ylim(yy . min(), yy . max()) ax . set_xticks(()) ax . set_yticks(()) i += 1 # iterate over classifiers for name, clf in zip (names, classifiers): ax = plt . subplot( len (datasets), len (classifiers) + 1 , i) clf . fit(X_train, y_train) score = clf . score(X_test, y_test) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. if hasattr (clf, \"decision_function\" ): Z = clf . decision_function(np . c_[xx . ravel(), yy . ravel()]) else : Z = clf . predict_proba(np . c_[xx . ravel(), yy . ravel()])[:, 1 ] # Put the result into a color plot Z = Z . reshape(xx . shape) ax . contourf(xx, yy, Z, cmap = cm, alpha = .8 ) # Plot also the training points ax . scatter(X_train[:, 0 ], X_train[:, 1 ], c = y_train, cmap = cm_bright, edgecolors = 'black' , s = 25 ) # and testing points ax . scatter(X_test[:, 0 ], X_test[:, 1 ], c = y_test, cmap = cm_bright, alpha = 0.6 , edgecolors = 'black' , s = 25 ) ax . set_xlim(xx . min(), xx . max()) ax . set_ylim(yy . min(), yy . max()) ax . set_xticks(()) ax . set_yticks(()) ax . set_title(name) ax . text(xx . max() - .3 , yy . min() + .3 , ( ' %.2f ' % score) . lstrip( '0' ), size = 15 , horizontalalignment = 'right' ) i += 1 figure . subplots_adjust(left = .02 , right = .98 ) plt . show()","title":"Perceptron"},{"location":"perceptron/#perceptron","text":"Varying regularization in Multi-layer Perceptron ( Source ) A comparison of different values for regularization parameter 'alpha' on synthetic datasets. The plot shows that different alphas yield different decision functions. Alpha is a parameter for regularization term, aka penalty term, that combats overfitting by constraining the size of the weights. Increasing alpha may fix high variance (a sign of overfitting) by encouraging smaller weights, resulting in a decision boundary plot that appears with lesser curvatures. Similarly, decreasing alpha may fix high bias (a sign of underfitting) by encouraging larger weights, potentially resulting in a more complicated decision boundary. Author: Issam H. Laradji License: BSD 3 clause import numpy as np from matplotlib import pyplot as plt from matplotlib.colors import ListedColormap from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.datasets import make_moons, make_circles, make_classification from sklearn.neural_network import MLPClassifier from sklearn.pipeline import make_pipeline h = .02 # step size in the mesh alphas = np . logspace( - 5 , 3 , 5 ) names = [ 'alpha ' + str (i) for i in alphas] classifiers = [] for i in alphas: classifiers . append(make_pipeline( StandardScaler(), MLPClassifier(solver = 'lbfgs' , alpha = i, random_state = 1 , max_iter = 2000 , early_stopping = True , hidden_layer_sizes = [ 100 , 100 ]) )) X, y = make_classification(n_features = 2 , n_redundant = 0 , n_informative = 2 , random_state = 0 , n_clusters_per_class = 1 ) rng = np . random . RandomState( 2 ) X += 2 * rng . uniform(size = X . shape) linearly_separable = (X, y) datasets = [make_moons(noise = 0.3 , random_state = 0 ), make_circles(noise = 0.2 , factor = 0.5 , random_state = 1 ), linearly_separable] figure = plt . figure(figsize = ( 17 , 9 )) i = 1 # iterate over datasets for X, y in datasets: # preprocess dataset, split into training and test part X = StandardScaler() . fit_transform(X) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .4 ) x_min, x_max = X[:, 0 ] . min() - .5 , X[:, 0 ] . max() + .5 y_min, y_max = X[:, 1 ] . min() - .5 , X[:, 1 ] . max() + .5 xx, yy = np . meshgrid(np . arange(x_min, x_max, h), np . arange(y_min, y_max, h)) # just plot the dataset first cm = plt . cm . RdBu cm_bright = ListedColormap([ '#FF0000' , '#0000FF' ]) ax = plt . subplot( len (datasets), len (classifiers) + 1 , i) # Plot the training points ax . scatter(X_train[:, 0 ], X_train[:, 1 ], c = y_train, cmap = cm_bright) # and testing points ax . scatter(X_test[:, 0 ], X_test[:, 1 ], c = y_test, cmap = cm_bright, alpha = 0.6 ) ax . set_xlim(xx . min(), xx . max()) ax . set_ylim(yy . min(), yy . max()) ax . set_xticks(()) ax . set_yticks(()) i += 1 # iterate over classifiers for name, clf in zip (names, classifiers): ax = plt . subplot( len (datasets), len (classifiers) + 1 , i) clf . fit(X_train, y_train) score = clf . score(X_test, y_test) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. if hasattr (clf, \"decision_function\" ): Z = clf . decision_function(np . c_[xx . ravel(), yy . ravel()]) else : Z = clf . predict_proba(np . c_[xx . ravel(), yy . ravel()])[:, 1 ] # Put the result into a color plot Z = Z . reshape(xx . shape) ax . contourf(xx, yy, Z, cmap = cm, alpha = .8 ) # Plot also the training points ax . scatter(X_train[:, 0 ], X_train[:, 1 ], c = y_train, cmap = cm_bright, edgecolors = 'black' , s = 25 ) # and testing points ax . scatter(X_test[:, 0 ], X_test[:, 1 ], c = y_test, cmap = cm_bright, alpha = 0.6 , edgecolors = 'black' , s = 25 ) ax . set_xlim(xx . min(), xx . max()) ax . set_ylim(yy . min(), yy . max()) ax . set_xticks(()) ax . set_yticks(()) ax . set_title(name) ax . text(xx . max() - .3 , yy . min() + .3 , ( ' %.2f ' % score) . lstrip( '0' ), size = 15 , horizontalalignment = 'right' ) i += 1 figure . subplots_adjust(left = .02 , right = .98 ) plt . show()","title":"Perceptron"},{"location":"svm/","text":"Support Vector Machine (SVM): Maximum margin separating hyperplane ( Source )_ Plot the maximum margin separating hyperplane within a two-class separable dataset using a Support Vector Machine classifier with linear kernel. import numpy as np import matplotlib.pyplot as plt from sklearn import svm from sklearn.datasets import make_blobs # we create 40 separable points X, y = make_blobs(n_samples = 40 , centers = 2 , random_state = 6 ) # fit the model, don't regularize for illustration purposes clf = svm . SVC(kernel = 'linear' , C = 1000 ) clf . fit(X, y) plt . figure(figsize = [ 12 , 8 ]) plt . scatter(X[:, 0 ], X[:, 1 ], c = y, s = 30 , cmap = plt . cm . Paired) # plot the decision function ax = plt . gca() xlim = ax . get_xlim() ylim = ax . get_ylim() # create grid to evaluate model xx = np . linspace(xlim[ 0 ], xlim[ 1 ], 30 ) yy = np . linspace(ylim[ 0 ], ylim[ 1 ], 30 ) YY, XX = np . meshgrid(yy, xx) xy = np . vstack([XX . ravel(), YY . ravel()]) . T Z = clf . decision_function(xy) . reshape(XX . shape) # plot decision boundary and margins ax . contour(XX, YY, Z, colors = 'k' , levels = [ - 1 , 0 , 1 ], alpha = 0.5 , linestyles = [ '--' , '-' , '--' ]) # plot support vectors ax . scatter(clf . support_vectors_[:, 0 ], clf . support_vectors_[:, 1 ], s = 100 , linewidth = 1 , facecolors = 'none' , edgecolors = 'k' ) plt . show()","title":"Support Vector Machine"},{"location":"svm/#support-vector-machine-svm","text":"Maximum margin separating hyperplane ( Source )_ Plot the maximum margin separating hyperplane within a two-class separable dataset using a Support Vector Machine classifier with linear kernel. import numpy as np import matplotlib.pyplot as plt from sklearn import svm from sklearn.datasets import make_blobs # we create 40 separable points X, y = make_blobs(n_samples = 40 , centers = 2 , random_state = 6 ) # fit the model, don't regularize for illustration purposes clf = svm . SVC(kernel = 'linear' , C = 1000 ) clf . fit(X, y) plt . figure(figsize = [ 12 , 8 ]) plt . scatter(X[:, 0 ], X[:, 1 ], c = y, s = 30 , cmap = plt . cm . Paired) # plot the decision function ax = plt . gca() xlim = ax . get_xlim() ylim = ax . get_ylim() # create grid to evaluate model xx = np . linspace(xlim[ 0 ], xlim[ 1 ], 30 ) yy = np . linspace(ylim[ 0 ], ylim[ 1 ], 30 ) YY, XX = np . meshgrid(yy, xx) xy = np . vstack([XX . ravel(), YY . ravel()]) . T Z = clf . decision_function(xy) . reshape(XX . shape) # plot decision boundary and margins ax . contour(XX, YY, Z, colors = 'k' , levels = [ - 1 , 0 , 1 ], alpha = 0.5 , linestyles = [ '--' , '-' , '--' ]) # plot support vectors ax . scatter(clf . support_vectors_[:, 0 ], clf . support_vectors_[:, 1 ], s = 100 , linewidth = 1 , facecolors = 'none' , edgecolors = 'k' ) plt . show()","title":"Support Vector Machine (SVM):"},{"location":"tree/","text":"Decision Tree Decision Tree Regression ( Source ) A 1D regression with decision tree. The :ref: decision trees <tree> is used to fit a sine curve with addition noisy observation. As a result, it learns local linear regressions approximating the sine curve. We can see that if the maximum depth of the tree (controlled by the max_depth parameter) is set too high, the decision trees learn too fine details of the training data and learn from the noise, i.e. they overfit. # Import the necessary modules and libraries import numpy as np from sklearn.tree import DecisionTreeRegressor import matplotlib.pyplot as plt # Create a random dataset rng = np . random . RandomState( 1 ) X = np . sort( 5 * rng . rand( 80 , 1 ), axis = 0 ) y = np . sin(X) . ravel() y[:: 5 ] += 3 * ( 0.5 - rng . rand( 16 )) # Fit regression model regr_1 = DecisionTreeRegressor(max_depth = 2 ) regr_2 = DecisionTreeRegressor(max_depth = 5 ) regr_1 . fit(X, y) regr_2 . fit(X, y) # Predict X_test = np . arange( 0.0 , 5.0 , 0.01 )[:, np . newaxis] y_1 = regr_1 . predict(X_test) y_2 = regr_2 . predict(X_test) # Plot the results plt . figure(figsize = [ 12 , 8 ]) plt . scatter(X, y, s = 20 , edgecolor = \"black\" , c = \"darkorange\" , label = \"data\" ) plt . plot(X_test, y_1, color = \"cornflowerblue\" , label = \"max_depth=2\" , linewidth = 2 ) plt . plot(X_test, y_2, color = \"yellowgreen\" , label = \"max_depth=5\" , linewidth = 2 ) plt . xlabel( \"data\" ) plt . ylabel( \"target\" ) plt . title( \"Decision Tree Regression\" ) plt . legend() plt . show()","title":"Decision Tree"},{"location":"tree/#decision-tree","text":"Decision Tree Regression ( Source ) A 1D regression with decision tree. The :ref: decision trees <tree> is used to fit a sine curve with addition noisy observation. As a result, it learns local linear regressions approximating the sine curve. We can see that if the maximum depth of the tree (controlled by the max_depth parameter) is set too high, the decision trees learn too fine details of the training data and learn from the noise, i.e. they overfit. # Import the necessary modules and libraries import numpy as np from sklearn.tree import DecisionTreeRegressor import matplotlib.pyplot as plt # Create a random dataset rng = np . random . RandomState( 1 ) X = np . sort( 5 * rng . rand( 80 , 1 ), axis = 0 ) y = np . sin(X) . ravel() y[:: 5 ] += 3 * ( 0.5 - rng . rand( 16 )) # Fit regression model regr_1 = DecisionTreeRegressor(max_depth = 2 ) regr_2 = DecisionTreeRegressor(max_depth = 5 ) regr_1 . fit(X, y) regr_2 . fit(X, y) # Predict X_test = np . arange( 0.0 , 5.0 , 0.01 )[:, np . newaxis] y_1 = regr_1 . predict(X_test) y_2 = regr_2 . predict(X_test) # Plot the results plt . figure(figsize = [ 12 , 8 ]) plt . scatter(X, y, s = 20 , edgecolor = \"black\" , c = \"darkorange\" , label = \"data\" ) plt . plot(X_test, y_1, color = \"cornflowerblue\" , label = \"max_depth=2\" , linewidth = 2 ) plt . plot(X_test, y_2, color = \"yellowgreen\" , label = \"max_depth=5\" , linewidth = 2 ) plt . xlabel( \"data\" ) plt . ylabel( \"target\" ) plt . title( \"Decision Tree Regression\" ) plt . legend() plt . show()","title":"Decision Tree"},{"location":"callback/callback/","text":"Neural Network: Callback and Checkpoints For classifying MNIST digits. Load dependencies #pip install tensorflow==2.0.0-beta0 #pip install --upgrade tensorflow==2.0.0-beta0 import tensorflow as tf from tensorflow import keras # tf.keras import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import os import pandas as pd import sklearn import sys import time sns . set() % matplotlib inline % load_ext tensorboard print ( \"python\" , sys . version) for module in mpl, np, pd, sklearn, tf, keras: print (module . __name__ , module . __version__) python 3.7.1 (default, Dec 14 2018, 13:28:58) [Clang 4.0.1 (tags/RELEASE_401/final)] matplotlib 3.0.2 numpy 1.15.4 pandas 0.23.4 sklearn 0.20.1 tensorflow 2.0.0-beta0 tensorflow.python.keras.api._v2.keras 2.2.4-tf Load data fashion_mnist = keras . datasets . fashion_mnist (X_train_full, y_train_full), (X_test, y_test) = ( fashion_mnist . load_data()) X_valid, X_train = X_train_full[: 5000 ], X_train_full[ 5000 :] y_valid, y_train = y_train_full[: 5000 ], y_train_full[ 5000 :] X_train . shape, y_train . shape ((55000, 28, 28), (55000,)) class_names = [ \"T-shirt/top\" , \"Trouser\" , \"Pullover\" , \"Dress\" , \"Coat\" , \"Sandal\" , \"Shirt\" , \"Sneaker\" , \"Bag\" , \"Ankle boot\" ] #X_train[0] n_rows = 5 n_cols = 10 plt . figure(figsize = (n_cols * 1.4 , n_rows * 1.6 )) for row in range (n_rows): for col in range (n_cols): index = n_cols * row + col plt . subplot(n_rows, n_cols, index + 1 ) plt . imshow(X_train[index], cmap = \"binary\" , interpolation = \"nearest\" ) plt . axis( 'off' ) plt . title(class_names[y_train[index]]) plt . show() y_train[ 0 : 50 ] array([4, 0, 7, 9, 9, 9, 4, 4, 3, 4, 0, 1, 8, 6, 3, 6, 4, 3, 2, 8, 7, 3, 4, 7, 1, 3, 4, 2, 0, 8, 5, 5, 9, 1, 5, 3, 5, 9, 0, 3, 9, 6, 4, 2, 9, 0, 8, 3, 3, 2], dtype=uint8) class_names[y_train[ 0 ]] 'Coat' Preprocess data : Feature Scaling When using Gradient Descent, it is usually best to ensure that the features all have a similar scale, preferably with a Normal distribution. Try to standardize the pixel values and see if this improves the performance of your neural network. Tips : * For each feature (pixel intensity), you must subtract the mean() of that feature (across all instances, so use axis=0 ) and divide by its standard deviation ( std() , again axis=0 ). Alternatively, you can use Scikit-Learn's StandardScaler . * Make sure you compute the means and standard deviations on the training set, and use these statistics to scale the training set, the validation set and the test set (you should not fit the validation set or the test set, and computing the means and standard deviations counts as \"fitting\"). from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train_scaled = scaler . fit_transform(X_train . astype(np . float32)\\ . reshape( - 1 , 28 * 28 )) . reshape( - 1 , 28 , 28 ) X_valid_scaled = scaler . transform(X_valid . astype(np . float32)\\ . reshape( - 1 , 28 * 28 )) . reshape( - 1 , 28 , 28 ) X_test_scaled = scaler . transform(X_test . astype(np . float32)\\ . reshape( - 1 , 28 * 28 )) . reshape( - 1 , 28 , 28 ) Design neural network architecture model = keras . models . Sequential([ keras . layers . Flatten(input_shape = [ 28 , 28 ]), keras . layers . Dense( 300 , activation = \"relu\" ), keras . layers . Dense( 100 , activation = \"relu\" ), keras . layers . Dense( 10 , activation = \"softmax\" ) ]) model . summary() Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 784) 0 _________________________________________________________________ dense (Dense) (None, 300) 235500 _________________________________________________________________ dense_1 (Dense) (None, 100) 30100 _________________________________________________________________ dense_2 (Dense) (None, 10) 1010 ================================================================= Total params: 266,610 Trainable params: 266,610 Non-trainable params: 0 _________________________________________________________________ Configure model: Callbacks and Checkpoints The fit() method accepts a callbacks argument. Try training your model with a large number of epochs, a validation set, and with a few callbacks from keras.callbacks : * TensorBoard : specify a log directory. It should be a subdirectory of a root logdir, such as ./my_logs/run_1 , and it should be different every time you train your model. You can use a timestamp in the subdirectory's path to ensure that it changes at every run. * EarlyStopping : specify patience=5 * ModelCheckpoint : specify the path of the checkpoint file to save (e.g., \"my_mnist_model.h5\" ) and set save_best_only=True Notice that the EarlyStopping callback will interrupt training before it reaches the requested number of epochs. This reduces the risk of overfitting. model . compile(loss = \"sparse_categorical_crossentropy\" , optimizer = keras . optimizers . SGD( 1e-3 ),\\ metrics = [ \"accuracy\" ]) Train! mkdir log mkdir: log: File exists logdir = os . path . join( \"./log/run_1\" , \"run_ {} \" . format(time . time())) callbacks = [ keras . callbacks . TensorBoard(logdir), keras . callbacks . EarlyStopping(patience = 5 ), keras . callbacks . ModelCheckpoint( \"my_mnist_model.h5\" , save_best_only = True ), ] history = model . fit(X_train_scaled,\\ y_train,\\ batch_size = 128 ,\\ epochs = 20 , validation_data = (X_valid_scaled, y_valid), callbacks = callbacks) WARNING: Logging before flag parsing goes to stderr. W1228 16:58:45.364671 4328199616 deprecation.py:323] From /Users/admin/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Train on 55000 samples, validate on 5000 samples Epoch 1/20 55000/55000 [==============================] - 2s 40us/sample - loss: 1.4332 - accuracy: 0.5387 - val_loss: 0.9500 - val_accuracy: 0.7058 Epoch 2/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.8561 - accuracy: 0.7255 - val_loss: 0.7478 - val_accuracy: 0.7598 Epoch 3/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.7228 - accuracy: 0.7617 - val_loss: 0.6602 - val_accuracy: 0.7816 Epoch 4/20 55000/55000 [==============================] - 2s 32us/sample - loss: 0.6535 - accuracy: 0.7809 - val_loss: 0.6091 - val_accuracy: 0.7990 Epoch 5/20 55000/55000 [==============================] - 2s 35us/sample - loss: 0.6085 - accuracy: 0.7931 - val_loss: 0.5736 - val_accuracy: 0.8116 Epoch 6/20 55000/55000 [==============================] - 2s 31us/sample - loss: 0.5760 - accuracy: 0.8018 - val_loss: 0.5479 - val_accuracy: 0.8172 Epoch 7/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.5510 - accuracy: 0.8096 - val_loss: 0.5275 - val_accuracy: 0.8250 Epoch 8/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.5311 - accuracy: 0.8150 - val_loss: 0.5122 - val_accuracy: 0.8302 Epoch 9/20 55000/55000 [==============================] - 2s 30us/sample - loss: 0.5147 - accuracy: 0.8208 - val_loss: 0.4988 - val_accuracy: 0.8346 Epoch 10/20 55000/55000 [==============================] - 2s 34us/sample - loss: 0.5010 - accuracy: 0.8256 - val_loss: 0.4877 - val_accuracy: 0.8364 Epoch 11/20 55000/55000 [==============================] - 2s 31us/sample - loss: 0.4893 - accuracy: 0.8291 - val_loss: 0.4782 - val_accuracy: 0.8390 Epoch 12/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.4790 - accuracy: 0.8322 - val_loss: 0.4704 - val_accuracy: 0.8404 Epoch 13/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.4699 - accuracy: 0.8352 - val_loss: 0.4628 - val_accuracy: 0.8444 Epoch 14/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.4618 - accuracy: 0.8380 - val_loss: 0.4565 - val_accuracy: 0.8450 Epoch 15/20 55000/55000 [==============================] - 2s 29us/sample - loss: 0.4545 - accuracy: 0.8401 - val_loss: 0.4505 - val_accuracy: 0.8482 Epoch 16/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.4480 - accuracy: 0.8426 - val_loss: 0.4453 - val_accuracy: 0.8498 Epoch 17/20 55000/55000 [==============================] - 2s 29us/sample - loss: 0.4419 - accuracy: 0.8446 - val_loss: 0.4406 - val_accuracy: 0.8508 Epoch 18/20 55000/55000 [==============================] - 2s 30us/sample - loss: 0.4363 - accuracy: 0.8468 - val_loss: 0.4362 - val_accuracy: 0.8520 Epoch 19/20 55000/55000 [==============================] - 2s 34us/sample - loss: 0.4312 - accuracy: 0.8484 - val_loss: 0.4318 - val_accuracy: 0.8524 Epoch 20/20 55000/55000 [==============================] - 2s 34us/sample - loss: 0.4264 - accuracy: 0.8499 - val_loss: 0.4286 - val_accuracy: 0.8538 model . evaluate(X_test_scaled, y_test) 10000/10000 [==============================] - 0s 39us/sample - loss: 0.4680 - accuracy: 0.8349 [0.4679875907897949, 0.8349] def plot_learning_curves (history): pd . DataFrame(history . history) . plot(figsize = ( 8 , 5 )) plt . grid( True ) plt . gca() . set_ylim( 0 , 1 ) plt . show() plot_learning_curves(history) The early stopping callback only stopped training after 10 epochs without progress, so your model may already have started to overfit the training set. Fortunately, since the ModelCheckpoint callback only saved the best models (on the validation set), the last saved model is the best on the validation set, so try loading it using keras.models.load_model() . Finally evaluate it on the test set. model = keras . models . load_model( \"my_mnist_model.h5\" ) model . evaluate(X_valid_scaled, y_valid) 5000/5000 [==============================] - 0s 41us/sample - loss: 0.4286 - accuracy: 0.8538 [0.428587375164032, 0.8538] Performing Inference model . evaluate(X_test, y_test) 10000/10000 [==============================] - 0s 38us/sample - loss: 6.0076 - accuracy: 0.6082 [6.007626473999023, 0.6082] X_valid . shape (5000, 28, 28) n_new = 10 X_new = X_test[:n_new] y_proba = model . predict(X_new) y_proba . round( 2 ) array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. ], [0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0. , 0.99, 0. , 0.01, 0. , 0. , 0. , 0. , 0. ], [0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. ], [0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. ], [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. ], [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. ]], dtype=float32) y_pred = model . predict_classes(X_new) y_pred array([8, 2, 1, 1, 2, 1, 4, 4, 8, 8]) for item in y_pred: print (class_names[item]) Bag Pullover Trouser Trouser Pullover Trouser Coat Coat Bag Bag plt . imshow(X_valid[ 0 ] . reshape( 28 , 28 )) <matplotlib.image.AxesImage at 0x1a2d4de978>","title":"Neural Network-Callbacks"},{"location":"callback/callback/#neural-network-callback-and-checkpoints","text":"For classifying MNIST digits.","title":"Neural Network: Callback and Checkpoints"},{"location":"callback/callback/#load-dependencies","text":"#pip install tensorflow==2.0.0-beta0 #pip install --upgrade tensorflow==2.0.0-beta0 import tensorflow as tf from tensorflow import keras # tf.keras import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import os import pandas as pd import sklearn import sys import time sns . set() % matplotlib inline % load_ext tensorboard print ( \"python\" , sys . version) for module in mpl, np, pd, sklearn, tf, keras: print (module . __name__ , module . __version__) python 3.7.1 (default, Dec 14 2018, 13:28:58) [Clang 4.0.1 (tags/RELEASE_401/final)] matplotlib 3.0.2 numpy 1.15.4 pandas 0.23.4 sklearn 0.20.1 tensorflow 2.0.0-beta0 tensorflow.python.keras.api._v2.keras 2.2.4-tf","title":"Load dependencies"},{"location":"callback/callback/#load-data","text":"fashion_mnist = keras . datasets . fashion_mnist (X_train_full, y_train_full), (X_test, y_test) = ( fashion_mnist . load_data()) X_valid, X_train = X_train_full[: 5000 ], X_train_full[ 5000 :] y_valid, y_train = y_train_full[: 5000 ], y_train_full[ 5000 :] X_train . shape, y_train . shape ((55000, 28, 28), (55000,)) class_names = [ \"T-shirt/top\" , \"Trouser\" , \"Pullover\" , \"Dress\" , \"Coat\" , \"Sandal\" , \"Shirt\" , \"Sneaker\" , \"Bag\" , \"Ankle boot\" ] #X_train[0] n_rows = 5 n_cols = 10 plt . figure(figsize = (n_cols * 1.4 , n_rows * 1.6 )) for row in range (n_rows): for col in range (n_cols): index = n_cols * row + col plt . subplot(n_rows, n_cols, index + 1 ) plt . imshow(X_train[index], cmap = \"binary\" , interpolation = \"nearest\" ) plt . axis( 'off' ) plt . title(class_names[y_train[index]]) plt . show() y_train[ 0 : 50 ] array([4, 0, 7, 9, 9, 9, 4, 4, 3, 4, 0, 1, 8, 6, 3, 6, 4, 3, 2, 8, 7, 3, 4, 7, 1, 3, 4, 2, 0, 8, 5, 5, 9, 1, 5, 3, 5, 9, 0, 3, 9, 6, 4, 2, 9, 0, 8, 3, 3, 2], dtype=uint8) class_names[y_train[ 0 ]] 'Coat'","title":"Load data"},{"location":"callback/callback/#preprocess-data-feature-scaling","text":"When using Gradient Descent, it is usually best to ensure that the features all have a similar scale, preferably with a Normal distribution. Try to standardize the pixel values and see if this improves the performance of your neural network. Tips : * For each feature (pixel intensity), you must subtract the mean() of that feature (across all instances, so use axis=0 ) and divide by its standard deviation ( std() , again axis=0 ). Alternatively, you can use Scikit-Learn's StandardScaler . * Make sure you compute the means and standard deviations on the training set, and use these statistics to scale the training set, the validation set and the test set (you should not fit the validation set or the test set, and computing the means and standard deviations counts as \"fitting\"). from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train_scaled = scaler . fit_transform(X_train . astype(np . float32)\\ . reshape( - 1 , 28 * 28 )) . reshape( - 1 , 28 , 28 ) X_valid_scaled = scaler . transform(X_valid . astype(np . float32)\\ . reshape( - 1 , 28 * 28 )) . reshape( - 1 , 28 , 28 ) X_test_scaled = scaler . transform(X_test . astype(np . float32)\\ . reshape( - 1 , 28 * 28 )) . reshape( - 1 , 28 , 28 )","title":"Preprocess data : Feature Scaling"},{"location":"callback/callback/#design-neural-network-architecture","text":"model = keras . models . Sequential([ keras . layers . Flatten(input_shape = [ 28 , 28 ]), keras . layers . Dense( 300 , activation = \"relu\" ), keras . layers . Dense( 100 , activation = \"relu\" ), keras . layers . Dense( 10 , activation = \"softmax\" ) ]) model . summary() Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 784) 0 _________________________________________________________________ dense (Dense) (None, 300) 235500 _________________________________________________________________ dense_1 (Dense) (None, 100) 30100 _________________________________________________________________ dense_2 (Dense) (None, 10) 1010 ================================================================= Total params: 266,610 Trainable params: 266,610 Non-trainable params: 0 _________________________________________________________________","title":"Design neural network architecture"},{"location":"callback/callback/#configure-model-callbacks-and-checkpoints","text":"The fit() method accepts a callbacks argument. Try training your model with a large number of epochs, a validation set, and with a few callbacks from keras.callbacks : * TensorBoard : specify a log directory. It should be a subdirectory of a root logdir, such as ./my_logs/run_1 , and it should be different every time you train your model. You can use a timestamp in the subdirectory's path to ensure that it changes at every run. * EarlyStopping : specify patience=5 * ModelCheckpoint : specify the path of the checkpoint file to save (e.g., \"my_mnist_model.h5\" ) and set save_best_only=True Notice that the EarlyStopping callback will interrupt training before it reaches the requested number of epochs. This reduces the risk of overfitting. model . compile(loss = \"sparse_categorical_crossentropy\" , optimizer = keras . optimizers . SGD( 1e-3 ),\\ metrics = [ \"accuracy\" ])","title":"Configure model: Callbacks and Checkpoints"},{"location":"callback/callback/#train","text":"mkdir log mkdir: log: File exists logdir = os . path . join( \"./log/run_1\" , \"run_ {} \" . format(time . time())) callbacks = [ keras . callbacks . TensorBoard(logdir), keras . callbacks . EarlyStopping(patience = 5 ), keras . callbacks . ModelCheckpoint( \"my_mnist_model.h5\" , save_best_only = True ), ] history = model . fit(X_train_scaled,\\ y_train,\\ batch_size = 128 ,\\ epochs = 20 , validation_data = (X_valid_scaled, y_valid), callbacks = callbacks) WARNING: Logging before flag parsing goes to stderr. W1228 16:58:45.364671 4328199616 deprecation.py:323] From /Users/admin/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Train on 55000 samples, validate on 5000 samples Epoch 1/20 55000/55000 [==============================] - 2s 40us/sample - loss: 1.4332 - accuracy: 0.5387 - val_loss: 0.9500 - val_accuracy: 0.7058 Epoch 2/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.8561 - accuracy: 0.7255 - val_loss: 0.7478 - val_accuracy: 0.7598 Epoch 3/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.7228 - accuracy: 0.7617 - val_loss: 0.6602 - val_accuracy: 0.7816 Epoch 4/20 55000/55000 [==============================] - 2s 32us/sample - loss: 0.6535 - accuracy: 0.7809 - val_loss: 0.6091 - val_accuracy: 0.7990 Epoch 5/20 55000/55000 [==============================] - 2s 35us/sample - loss: 0.6085 - accuracy: 0.7931 - val_loss: 0.5736 - val_accuracy: 0.8116 Epoch 6/20 55000/55000 [==============================] - 2s 31us/sample - loss: 0.5760 - accuracy: 0.8018 - val_loss: 0.5479 - val_accuracy: 0.8172 Epoch 7/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.5510 - accuracy: 0.8096 - val_loss: 0.5275 - val_accuracy: 0.8250 Epoch 8/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.5311 - accuracy: 0.8150 - val_loss: 0.5122 - val_accuracy: 0.8302 Epoch 9/20 55000/55000 [==============================] - 2s 30us/sample - loss: 0.5147 - accuracy: 0.8208 - val_loss: 0.4988 - val_accuracy: 0.8346 Epoch 10/20 55000/55000 [==============================] - 2s 34us/sample - loss: 0.5010 - accuracy: 0.8256 - val_loss: 0.4877 - val_accuracy: 0.8364 Epoch 11/20 55000/55000 [==============================] - 2s 31us/sample - loss: 0.4893 - accuracy: 0.8291 - val_loss: 0.4782 - val_accuracy: 0.8390 Epoch 12/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.4790 - accuracy: 0.8322 - val_loss: 0.4704 - val_accuracy: 0.8404 Epoch 13/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.4699 - accuracy: 0.8352 - val_loss: 0.4628 - val_accuracy: 0.8444 Epoch 14/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.4618 - accuracy: 0.8380 - val_loss: 0.4565 - val_accuracy: 0.8450 Epoch 15/20 55000/55000 [==============================] - 2s 29us/sample - loss: 0.4545 - accuracy: 0.8401 - val_loss: 0.4505 - val_accuracy: 0.8482 Epoch 16/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.4480 - accuracy: 0.8426 - val_loss: 0.4453 - val_accuracy: 0.8498 Epoch 17/20 55000/55000 [==============================] - 2s 29us/sample - loss: 0.4419 - accuracy: 0.8446 - val_loss: 0.4406 - val_accuracy: 0.8508 Epoch 18/20 55000/55000 [==============================] - 2s 30us/sample - loss: 0.4363 - accuracy: 0.8468 - val_loss: 0.4362 - val_accuracy: 0.8520 Epoch 19/20 55000/55000 [==============================] - 2s 34us/sample - loss: 0.4312 - accuracy: 0.8484 - val_loss: 0.4318 - val_accuracy: 0.8524 Epoch 20/20 55000/55000 [==============================] - 2s 34us/sample - loss: 0.4264 - accuracy: 0.8499 - val_loss: 0.4286 - val_accuracy: 0.8538 model . evaluate(X_test_scaled, y_test) 10000/10000 [==============================] - 0s 39us/sample - loss: 0.4680 - accuracy: 0.8349 [0.4679875907897949, 0.8349] def plot_learning_curves (history): pd . DataFrame(history . history) . plot(figsize = ( 8 , 5 )) plt . grid( True ) plt . gca() . set_ylim( 0 , 1 ) plt . show() plot_learning_curves(history) The early stopping callback only stopped training after 10 epochs without progress, so your model may already have started to overfit the training set. Fortunately, since the ModelCheckpoint callback only saved the best models (on the validation set), the last saved model is the best on the validation set, so try loading it using keras.models.load_model() . Finally evaluate it on the test set. model = keras . models . load_model( \"my_mnist_model.h5\" ) model . evaluate(X_valid_scaled, y_valid) 5000/5000 [==============================] - 0s 41us/sample - loss: 0.4286 - accuracy: 0.8538 [0.428587375164032, 0.8538]","title":"Train!"},{"location":"callback/callback/#performing-inference","text":"model . evaluate(X_test, y_test) 10000/10000 [==============================] - 0s 38us/sample - loss: 6.0076 - accuracy: 0.6082 [6.007626473999023, 0.6082] X_valid . shape (5000, 28, 28) n_new = 10 X_new = X_test[:n_new] y_proba = model . predict(X_new) y_proba . round( 2 ) array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. ], [0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0. , 0.99, 0. , 0.01, 0. , 0. , 0. , 0. , 0. ], [0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. ], [0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. ], [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. ], [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. ]], dtype=float32) y_pred = model . predict_classes(X_new) y_pred array([8, 2, 1, 1, 2, 1, 4, 4, 8, 8]) for item in y_pred: print (class_names[item]) Bag Pullover Trouser Trouser Pullover Trouser Coat Coat Bag Bag plt . imshow(X_valid[ 0 ] . reshape( 28 , 28 )) <matplotlib.image.AxesImage at 0x1a2d4de978>","title":"Performing Inference"},{"location":"cnn/cnn/","text":"CNN In this notebook you will learn how to build Convolutional Neural Networks (CNNs) for image processing. Imports % matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import os import pandas as pd import sklearn import sys import tensorflow as tf from tensorflow import keras import time import seaborn as sns sns . set() print ( \"python\" , sys . version) for module in mpl, np, pd, sklearn, tf, keras: print (module . __name__ , module . __version__) python 3.7.1 (default, Dec 14 2018, 13:28:58) [Clang 4.0.1 (tags/RELEASE_401/final)] matplotlib 3.0.2 numpy 1.15.4 pandas 0.23.4 sklearn 0.20.1 tensorflow 2.0.0-beta0 tensorflow.python.keras.api._v2.keras 2.2.4-tf assert sys . version_info >= ( 3 , 5 ) # Python \u22653.5 required assert tf . __version__ >= \"2.0\" # TensorFlow \u22652.0 required Simple CNN Load CIFAR10 using keras.datasets.cifar10.load_data() , and split it into a training set (45,000 images), a validation set (5,000 images) and a test set (10,000 images). Make sure the pixel values range from 0 to 1. Visualize a few images using plt.imshow() . classes = [ \"airplane\" , \"automobile\" , \"bird\" , \"cat\" , \"deer\" , \"dog\" , \"frog\" , \"horse\" , \"ship\" , \"truck\" , ] (X_train_full, y_train_full), (X_test, y_test) = keras . datasets . cifar10 . load_data() X_train = X_train_full[: - 5000 ] / 255 y_train = y_train_full[: - 5000 ] X_valid = X_train_full[ - 5000 :] / 255 y_valid = y_train_full[ - 5000 :] X_test = X_test / 255 plt . figure(figsize = ( 10 , 7 )) n_rows, n_cols = 10 , 15 for row in range (n_rows): for col in range (n_cols): i = row * n_cols + col plt . subplot(n_rows, n_cols, i + 1 ) plt . axis( \"off\" ) plt . imshow(X_train[i]) Let's print the classes of the images in the first row: for i in range (n_cols): print (classes[y_train[i][ 0 ]], end = \" \" ) frog truck truck deer automobile automobile bird horse ship cat deer horse horse bird truck 1. Baseline Model Build and train a baseline model with a few dense layers, and plot the learning curves. Use the model's summary() method to count the number of parameters in this model. Tip : Recall that to plot the learning curves, you can simply create a Pandas DataFrame with the history.history dict, then call its plot() method. model = keras . models . Sequential([ keras . layers . Flatten(input_shape = [ 32 , 32 , 3 ]), keras . layers . Dense( 64 , activation = \"selu\" ), keras . layers . Dense( 64 , activation = \"selu\" ), keras . layers . Dense( 64 , activation = \"selu\" ), keras . layers . Dense( 10 , activation = \"softmax\" ) ]) WARNING: Logging before flag parsing goes to stderr. W1229 09:21:53.523933 4571825600 deprecation.py:323] From /Users/admin/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4149: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where model . compile(loss = \"sparse_categorical_crossentropy\" , optimizer = keras . optimizers . SGD(lr = 0.01 ), metrics = [ \"accuracy\" ]) model . summary() Model: \"sequential_2\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_2 (Flatten) (None, 3072) 0 _________________________________________________________________ dense_8 (Dense) (None, 64) 196672 _________________________________________________________________ dense_9 (Dense) (None, 64) 4160 _________________________________________________________________ dense_10 (Dense) (None, 64) 4160 _________________________________________________________________ dense_11 (Dense) (None, 10) 650 ================================================================= Total params: 205,642 Trainable params: 205,642 Non-trainable params: 0 _________________________________________________________________ history = model . fit(X_train, y_train, epochs = 5 , validation_data = (X_valid, y_valid)) Train on 45000 samples, validate on 5000 samples Epoch 1/5 45000/45000 [==============================] - 4s 82us/sample - loss: 1.8995 - accuracy: 0.3133 - val_loss: 1.8570 - val_accuracy: 0.3368 Epoch 2/5 45000/45000 [==============================] - 3s 77us/sample - loss: 1.7134 - accuracy: 0.3834 - val_loss: 1.7347 - val_accuracy: 0.3782 Epoch 3/5 45000/45000 [==============================] - 4s 79us/sample - loss: 1.6369 - accuracy: 0.4152 - val_loss: 1.6705 - val_accuracy: 0.4022 Epoch 4/5 45000/45000 [==============================] - 4s 79us/sample - loss: 1.5826 - accuracy: 0.4356 - val_loss: 1.6407 - val_accuracy: 0.4118 Epoch 5/5 45000/45000 [==============================] - 3s 77us/sample - loss: 1.5412 - accuracy: 0.4512 - val_loss: 1.5708 - val_accuracy: 0.4322 pd . DataFrame(history . history) . plot() plt . axis([ 0 , 19 , 0 , 1 ]) plt . show() 2. Convolution 2D, Pool2D Build and train a Convolutional Neural Network using a \"classical\" architecture: N * (Conv2D \u2192 Conv2D \u2192 Pool2D) \u2192 Flatten \u2192 Dense \u2192 Dense. Before you print the summary() , try to manually calculate the number of parameters in your model's architecture, as well as the shape of the inputs and outputs of each layer. Next, plot the learning curves and compare the performance with the previous model. Demo: https://cs231n.github.io/convolutional-networks/ model = keras . models . Sequential([ keras . layers . Conv2D(filters = 32 ,\\ kernel_size = 3 ,\\ padding = \"same\" ,\\ activation = \"relu\" ,\\ input_shape = [ 32 , 32 , 3 ]), keras . layers . Conv2D(filters = 32 ,\\ kernel_size = 3 ,\\ padding = \"same\" ,\\ activation = \"relu\" ), keras . layers . MaxPool2D(pool_size = 2 ), keras . layers . Conv2D(filters = 64 ,\\ kernel_size = 3 ,\\ padding = \"same\" ,\\ activation = \"relu\" ), keras . layers . Conv2D(filters = 64 ,\\ kernel_size = 3 ,\\ padding = \"same\" ,\\ activation = \"relu\" ), keras . layers . MaxPool2D(pool_size = 2 ), keras . layers . Flatten(), keras . layers . Dense( 128 ,\\ activation = \"relu\" ), keras . layers . Dense( 10 ,\\ activation = \"softmax\" ) ]) model . summary() Model: \"sequential_3\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 32, 32, 32) 896 _________________________________________________________________ conv2d_1 (Conv2D) (None, 32, 32, 32) 9248 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 16, 16, 32) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 16, 16, 64) 18496 _________________________________________________________________ conv2d_3 (Conv2D) (None, 16, 16, 64) 36928 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64) 0 _________________________________________________________________ flatten_3 (Flatten) (None, 4096) 0 _________________________________________________________________ dense_12 (Dense) (None, 128) 524416 _________________________________________________________________ dense_13 (Dense) (None, 10) 1290 ================================================================= Total params: 591,274 Trainable params: 591,274 Non-trainable params: 0 _________________________________________________________________ # Number of params in a convolutional layer = # (kernel_width * kernel_height * channels_in + 1 for bias) * channels_out ( ( 3 * 3 * 3 + 1 ) * 32 # in: 32x32x3 out: 32x32x32 Conv2D + ( 3 * 3 * 32 + 1 ) * 32 # in: 32x32x32 out: 32x32x32 Conv2D + 0 # in: 32x32x32 out: 16x16x32 MaxPool2D + ( 3 * 3 * 32 + 1 ) * 64 # in: 16x16x32 out: 16x16x64 Conv2D + ( 3 * 3 * 64 + 1 ) * 64 # in: 16x16x64 out: 16x16x64 Conv2D + 0 # in: 16x16x64 out: 8x8x64 MaxPool2D + 0 # in: 8x8x64 out: 4096 Flatten + ( 4096 + 1 ) * 128 # in: 4096 out: 128 Dense + ( 128 + 1 ) * 10 # in: 128 out: 10 Dense ) 591274 model . compile(loss = \"sparse_categorical_crossentropy\" , optimizer = keras . optimizers . SGD(lr = 0.01 ), metrics = [ \"accuracy\" ]) pd . DataFrame(history . history) . plot() plt . axis([ 0 , 19 , 0 , 1 ]) plt . show() Bathch Normalization Looking at the learning curves, you can see that the model is overfitting. Add a Batch Normalization layer after each convolutional layer. Compare the model's performance and learning curves with the previous model. Tip : there is no need for an activation function just before the pooling layers. model = keras . models . Sequential([ keras . layers . Conv2D(filters = 32 , kernel_size = 3 , padding = \"same\" , activation = \"relu\" , input_shape = [ 32 , 32 , 3 ]), keras . layers . BatchNormalization(), keras . layers . Conv2D(filters = 32 , kernel_size = 3 , padding = \"same\" , activation = \"relu\" ), keras . layers . BatchNormalization(), keras . layers . MaxPool2D(pool_size = 2 ), keras . layers . Conv2D(filters = 64 , kernel_size = 3 , padding = \"same\" , activation = \"relu\" ), keras . layers . BatchNormalization(), keras . layers . Conv2D(filters = 64 , kernel_size = 3 , padding = \"same\" , activation = \"relu\" ), keras . layers . BatchNormalization(), keras . layers . MaxPool2D(pool_size = 2 ), keras . layers . Flatten(), keras . layers . Dense( 128 , activation = \"relu\" ), keras . layers . Dense( 10 , activation = \"softmax\" ) ]) model . summary() Model: \"sequential_1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 32, 32, 32) 896 _________________________________________________________________ batch_normalization (BatchNo (None, 32, 32, 32) 128 _________________________________________________________________ conv2d_1 (Conv2D) (None, 32, 32, 32) 9248 _________________________________________________________________ batch_normalization_1 (Batch (None, 32, 32, 32) 128 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 16, 16, 32) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 16, 16, 64) 18496 _________________________________________________________________ batch_normalization_2 (Batch (None, 16, 16, 64) 256 _________________________________________________________________ conv2d_3 (Conv2D) (None, 16, 16, 64) 36928 _________________________________________________________________ batch_normalization_3 (Batch (None, 16, 16, 64) 256 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 4096) 0 _________________________________________________________________ dense_4 (Dense) (None, 128) 524416 _________________________________________________________________ dense_5 (Dense) (None, 10) 1290 ================================================================= Total params: 592,042 Trainable params: 591,658 Non-trainable params: 384 _________________________________________________________________ model . compile(loss = \"sparse_categorical_crossentropy\" , optimizer = keras . optimizers . SGD(lr = 0.01 ), metrics = [ \"accuracy\" ]) history = model . fit(X_train, y_train, epochs = 5 , validation_data = (X_valid, y_valid)) Train on 45000 samples, validate on 5000 samples Epoch 1/5 45000/45000 [==============================] - 201s 4ms/sample - loss: 1.3647 - accuracy: 0.5088 - val_loss: 1.1809 - val_accuracy: 0.5754 Epoch 2/5 45000/45000 [==============================] - 194s 4ms/sample - loss: 0.9513 - accuracy: 0.6615 - val_loss: 1.1434 - val_accuracy: 0.6046 Epoch 3/5 45000/45000 [==============================] - 205s 5ms/sample - loss: 0.7711 - accuracy: 0.7269 - val_loss: 0.9189 - val_accuracy: 0.6814 Epoch 4/5 45000/45000 [==============================] - 203s 5ms/sample - loss: 0.6454 - accuracy: 0.7729 - val_loss: 0.8797 - val_accuracy: 0.6998 Epoch 5/5 45000/45000 [==============================] - 196s 4ms/sample - loss: 0.5381 - accuracy: 0.8112 - val_loss: 1.1316 - val_accuracy: 0.6276 pd . DataFrame(history . history) . plot() plt . axis([ 0 , 19 , 0 , 1 ]) plt . show() Try to estimate the number of parameters in your network, then check your result with model.summary() . Tip : the batch normalization layer adds two parameters for each feature map (the scale and bias). Object Detection Project The Google Street View House Numbers (SVHN) dataset contains pictures of digits in all shapes and colors, taken by the Google Street View cars. The goal is to classify and locate all the digits in large images. * Train a Fully Convolutional Network on the 32x32 images. * Use this FCN to build a digit detector in the large images.","title":"Convolutional Neural Network"},{"location":"cnn/cnn/#cnn","text":"In this notebook you will learn how to build Convolutional Neural Networks (CNNs) for image processing.","title":"CNN"},{"location":"cnn/cnn/#imports","text":"% matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import os import pandas as pd import sklearn import sys import tensorflow as tf from tensorflow import keras import time import seaborn as sns sns . set() print ( \"python\" , sys . version) for module in mpl, np, pd, sklearn, tf, keras: print (module . __name__ , module . __version__) python 3.7.1 (default, Dec 14 2018, 13:28:58) [Clang 4.0.1 (tags/RELEASE_401/final)] matplotlib 3.0.2 numpy 1.15.4 pandas 0.23.4 sklearn 0.20.1 tensorflow 2.0.0-beta0 tensorflow.python.keras.api._v2.keras 2.2.4-tf assert sys . version_info >= ( 3 , 5 ) # Python \u22653.5 required assert tf . __version__ >= \"2.0\" # TensorFlow \u22652.0 required","title":"Imports"},{"location":"cnn/cnn/#simple-cnn","text":"Load CIFAR10 using keras.datasets.cifar10.load_data() , and split it into a training set (45,000 images), a validation set (5,000 images) and a test set (10,000 images). Make sure the pixel values range from 0 to 1. Visualize a few images using plt.imshow() . classes = [ \"airplane\" , \"automobile\" , \"bird\" , \"cat\" , \"deer\" , \"dog\" , \"frog\" , \"horse\" , \"ship\" , \"truck\" , ] (X_train_full, y_train_full), (X_test, y_test) = keras . datasets . cifar10 . load_data() X_train = X_train_full[: - 5000 ] / 255 y_train = y_train_full[: - 5000 ] X_valid = X_train_full[ - 5000 :] / 255 y_valid = y_train_full[ - 5000 :] X_test = X_test / 255 plt . figure(figsize = ( 10 , 7 )) n_rows, n_cols = 10 , 15 for row in range (n_rows): for col in range (n_cols): i = row * n_cols + col plt . subplot(n_rows, n_cols, i + 1 ) plt . axis( \"off\" ) plt . imshow(X_train[i]) Let's print the classes of the images in the first row: for i in range (n_cols): print (classes[y_train[i][ 0 ]], end = \" \" ) frog truck truck deer automobile automobile bird horse ship cat deer horse horse bird truck","title":"Simple CNN"},{"location":"cnn/cnn/#1-baseline-model","text":"Build and train a baseline model with a few dense layers, and plot the learning curves. Use the model's summary() method to count the number of parameters in this model. Tip : Recall that to plot the learning curves, you can simply create a Pandas DataFrame with the history.history dict, then call its plot() method. model = keras . models . Sequential([ keras . layers . Flatten(input_shape = [ 32 , 32 , 3 ]), keras . layers . Dense( 64 , activation = \"selu\" ), keras . layers . Dense( 64 , activation = \"selu\" ), keras . layers . Dense( 64 , activation = \"selu\" ), keras . layers . Dense( 10 , activation = \"softmax\" ) ]) WARNING: Logging before flag parsing goes to stderr. W1229 09:21:53.523933 4571825600 deprecation.py:323] From /Users/admin/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4149: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where model . compile(loss = \"sparse_categorical_crossentropy\" , optimizer = keras . optimizers . SGD(lr = 0.01 ), metrics = [ \"accuracy\" ]) model . summary() Model: \"sequential_2\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_2 (Flatten) (None, 3072) 0 _________________________________________________________________ dense_8 (Dense) (None, 64) 196672 _________________________________________________________________ dense_9 (Dense) (None, 64) 4160 _________________________________________________________________ dense_10 (Dense) (None, 64) 4160 _________________________________________________________________ dense_11 (Dense) (None, 10) 650 ================================================================= Total params: 205,642 Trainable params: 205,642 Non-trainable params: 0 _________________________________________________________________ history = model . fit(X_train, y_train, epochs = 5 , validation_data = (X_valid, y_valid)) Train on 45000 samples, validate on 5000 samples Epoch 1/5 45000/45000 [==============================] - 4s 82us/sample - loss: 1.8995 - accuracy: 0.3133 - val_loss: 1.8570 - val_accuracy: 0.3368 Epoch 2/5 45000/45000 [==============================] - 3s 77us/sample - loss: 1.7134 - accuracy: 0.3834 - val_loss: 1.7347 - val_accuracy: 0.3782 Epoch 3/5 45000/45000 [==============================] - 4s 79us/sample - loss: 1.6369 - accuracy: 0.4152 - val_loss: 1.6705 - val_accuracy: 0.4022 Epoch 4/5 45000/45000 [==============================] - 4s 79us/sample - loss: 1.5826 - accuracy: 0.4356 - val_loss: 1.6407 - val_accuracy: 0.4118 Epoch 5/5 45000/45000 [==============================] - 3s 77us/sample - loss: 1.5412 - accuracy: 0.4512 - val_loss: 1.5708 - val_accuracy: 0.4322 pd . DataFrame(history . history) . plot() plt . axis([ 0 , 19 , 0 , 1 ]) plt . show()","title":"1.  Baseline Model"},{"location":"cnn/cnn/#2-convolution-2d-pool2d","text":"Build and train a Convolutional Neural Network using a \"classical\" architecture: N * (Conv2D \u2192 Conv2D \u2192 Pool2D) \u2192 Flatten \u2192 Dense \u2192 Dense. Before you print the summary() , try to manually calculate the number of parameters in your model's architecture, as well as the shape of the inputs and outputs of each layer. Next, plot the learning curves and compare the performance with the previous model. Demo: https://cs231n.github.io/convolutional-networks/ model = keras . models . Sequential([ keras . layers . Conv2D(filters = 32 ,\\ kernel_size = 3 ,\\ padding = \"same\" ,\\ activation = \"relu\" ,\\ input_shape = [ 32 , 32 , 3 ]), keras . layers . Conv2D(filters = 32 ,\\ kernel_size = 3 ,\\ padding = \"same\" ,\\ activation = \"relu\" ), keras . layers . MaxPool2D(pool_size = 2 ), keras . layers . Conv2D(filters = 64 ,\\ kernel_size = 3 ,\\ padding = \"same\" ,\\ activation = \"relu\" ), keras . layers . Conv2D(filters = 64 ,\\ kernel_size = 3 ,\\ padding = \"same\" ,\\ activation = \"relu\" ), keras . layers . MaxPool2D(pool_size = 2 ), keras . layers . Flatten(), keras . layers . Dense( 128 ,\\ activation = \"relu\" ), keras . layers . Dense( 10 ,\\ activation = \"softmax\" ) ]) model . summary() Model: \"sequential_3\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 32, 32, 32) 896 _________________________________________________________________ conv2d_1 (Conv2D) (None, 32, 32, 32) 9248 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 16, 16, 32) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 16, 16, 64) 18496 _________________________________________________________________ conv2d_3 (Conv2D) (None, 16, 16, 64) 36928 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64) 0 _________________________________________________________________ flatten_3 (Flatten) (None, 4096) 0 _________________________________________________________________ dense_12 (Dense) (None, 128) 524416 _________________________________________________________________ dense_13 (Dense) (None, 10) 1290 ================================================================= Total params: 591,274 Trainable params: 591,274 Non-trainable params: 0 _________________________________________________________________ # Number of params in a convolutional layer = # (kernel_width * kernel_height * channels_in + 1 for bias) * channels_out ( ( 3 * 3 * 3 + 1 ) * 32 # in: 32x32x3 out: 32x32x32 Conv2D + ( 3 * 3 * 32 + 1 ) * 32 # in: 32x32x32 out: 32x32x32 Conv2D + 0 # in: 32x32x32 out: 16x16x32 MaxPool2D + ( 3 * 3 * 32 + 1 ) * 64 # in: 16x16x32 out: 16x16x64 Conv2D + ( 3 * 3 * 64 + 1 ) * 64 # in: 16x16x64 out: 16x16x64 Conv2D + 0 # in: 16x16x64 out: 8x8x64 MaxPool2D + 0 # in: 8x8x64 out: 4096 Flatten + ( 4096 + 1 ) * 128 # in: 4096 out: 128 Dense + ( 128 + 1 ) * 10 # in: 128 out: 10 Dense ) 591274 model . compile(loss = \"sparse_categorical_crossentropy\" , optimizer = keras . optimizers . SGD(lr = 0.01 ), metrics = [ \"accuracy\" ]) pd . DataFrame(history . history) . plot() plt . axis([ 0 , 19 , 0 , 1 ]) plt . show()","title":"2. Convolution 2D, Pool2D"},{"location":"cnn/cnn/#bathch-normalization","text":"Looking at the learning curves, you can see that the model is overfitting. Add a Batch Normalization layer after each convolutional layer. Compare the model's performance and learning curves with the previous model. Tip : there is no need for an activation function just before the pooling layers. model = keras . models . Sequential([ keras . layers . Conv2D(filters = 32 , kernel_size = 3 , padding = \"same\" , activation = \"relu\" , input_shape = [ 32 , 32 , 3 ]), keras . layers . BatchNormalization(), keras . layers . Conv2D(filters = 32 , kernel_size = 3 , padding = \"same\" , activation = \"relu\" ), keras . layers . BatchNormalization(), keras . layers . MaxPool2D(pool_size = 2 ), keras . layers . Conv2D(filters = 64 , kernel_size = 3 , padding = \"same\" , activation = \"relu\" ), keras . layers . BatchNormalization(), keras . layers . Conv2D(filters = 64 , kernel_size = 3 , padding = \"same\" , activation = \"relu\" ), keras . layers . BatchNormalization(), keras . layers . MaxPool2D(pool_size = 2 ), keras . layers . Flatten(), keras . layers . Dense( 128 , activation = \"relu\" ), keras . layers . Dense( 10 , activation = \"softmax\" ) ]) model . summary() Model: \"sequential_1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 32, 32, 32) 896 _________________________________________________________________ batch_normalization (BatchNo (None, 32, 32, 32) 128 _________________________________________________________________ conv2d_1 (Conv2D) (None, 32, 32, 32) 9248 _________________________________________________________________ batch_normalization_1 (Batch (None, 32, 32, 32) 128 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 16, 16, 32) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 16, 16, 64) 18496 _________________________________________________________________ batch_normalization_2 (Batch (None, 16, 16, 64) 256 _________________________________________________________________ conv2d_3 (Conv2D) (None, 16, 16, 64) 36928 _________________________________________________________________ batch_normalization_3 (Batch (None, 16, 16, 64) 256 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 4096) 0 _________________________________________________________________ dense_4 (Dense) (None, 128) 524416 _________________________________________________________________ dense_5 (Dense) (None, 10) 1290 ================================================================= Total params: 592,042 Trainable params: 591,658 Non-trainable params: 384 _________________________________________________________________ model . compile(loss = \"sparse_categorical_crossentropy\" , optimizer = keras . optimizers . SGD(lr = 0.01 ), metrics = [ \"accuracy\" ]) history = model . fit(X_train, y_train, epochs = 5 , validation_data = (X_valid, y_valid)) Train on 45000 samples, validate on 5000 samples Epoch 1/5 45000/45000 [==============================] - 201s 4ms/sample - loss: 1.3647 - accuracy: 0.5088 - val_loss: 1.1809 - val_accuracy: 0.5754 Epoch 2/5 45000/45000 [==============================] - 194s 4ms/sample - loss: 0.9513 - accuracy: 0.6615 - val_loss: 1.1434 - val_accuracy: 0.6046 Epoch 3/5 45000/45000 [==============================] - 205s 5ms/sample - loss: 0.7711 - accuracy: 0.7269 - val_loss: 0.9189 - val_accuracy: 0.6814 Epoch 4/5 45000/45000 [==============================] - 203s 5ms/sample - loss: 0.6454 - accuracy: 0.7729 - val_loss: 0.8797 - val_accuracy: 0.6998 Epoch 5/5 45000/45000 [==============================] - 196s 4ms/sample - loss: 0.5381 - accuracy: 0.8112 - val_loss: 1.1316 - val_accuracy: 0.6276 pd . DataFrame(history . history) . plot() plt . axis([ 0 , 19 , 0 , 1 ]) plt . show() Try to estimate the number of parameters in your network, then check your result with model.summary() . Tip : the batch normalization layer adds two parameters for each feature map (the scale and bias).","title":"Bathch Normalization"},{"location":"cnn/cnn/#object-detection-project","text":"The Google Street View House Numbers (SVHN) dataset contains pictures of digits in all shapes and colors, taken by the Google Street View cars. The goal is to classify and locate all the digits in large images. * Train a Fully Convolutional Network on the 32x32 images. * Use this FCN to build a digit detector in the large images.","title":"Object Detection Project"},{"location":"feature/feature/","text":"Neural Net : Feature Scaling For classifying MNIST digits. Load dependencies #pip install tensorflow==2.0.0-beta0 #pip install --upgrade tensorflow==2.0.0-beta0 import tensorflow as tf from tensorflow import keras # tf.keras import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import os import pandas as pd import sklearn import sys import time sns . set() % matplotlib inline % load_ext tensorboard The tensorboard extension is already loaded. To reload it, use: %reload_ext tensorboard print ( \"python\" , sys . version) for module in mpl, np, pd, sklearn, tf, keras: print (module . __name__ , module . __version__) python 3.7.1 (default, Dec 14 2018, 13:28:58) [Clang 4.0.1 (tags/RELEASE_401/final)] matplotlib 3.0.2 numpy 1.15.4 pandas 0.23.4 sklearn 0.20.1 tensorflow 2.0.0-beta0 tensorflow.python.keras.api._v2.keras 2.2.4-tf Load data fashion_mnist = keras . datasets . fashion_mnist (X_train_full, y_train_full), (X_test, y_test) = ( fashion_mnist . load_data()) X_valid, X_train = X_train_full[: 5000 ], X_train_full[ 5000 :] y_valid, y_train = y_train_full[: 5000 ], y_train_full[ 5000 :] X_train . shape, y_train . shape ((55000, 28, 28), (55000,)) class_names = [ \"T-shirt/top\" , \"Trouser\" , \"Pullover\" , \"Dress\" , \"Coat\" , \"Sandal\" , \"Shirt\" , \"Sneaker\" , \"Bag\" , \"Ankle boot\" ] #X_train[0] n_rows = 5 n_cols = 10 plt . figure(figsize = (n_cols * 1.4 , n_rows * 1.6 )) for row in range (n_rows): for col in range (n_cols): index = n_cols * row + col plt . subplot(n_rows, n_cols, index + 1 ) plt . imshow(X_train[index], cmap = \"binary\" , interpolation = \"nearest\" ) plt . axis( 'off' ) plt . title(class_names[y_train[index]]) plt . show() y_train[ 0 : 50 ] array([4, 0, 7, 9, 9, 9, 4, 4, 3, 4, 0, 1, 8, 6, 3, 6, 4, 3, 2, 8, 7, 3, 4, 7, 1, 3, 4, 2, 0, 8, 5, 5, 9, 1, 5, 3, 5, 9, 0, 3, 9, 6, 4, 2, 9, 0, 8, 3, 3, 2], dtype=uint8) class_names[y_train[ 0 ]] 'Coat' Preprocess data : Feature Scaling When using Gradient Descent, it is usually best to ensure that the features all have a similar scale, preferably with a Normal distribution. Try to standardize the pixel values and see if this improves the performance of your neural network. Tips : * For each feature (pixel intensity), you must subtract the mean() of that feature (across all instances, so use axis=0 ) and divide by its standard deviation ( std() , again axis=0 ). Alternatively, you can use Scikit-Learn's StandardScaler . * Make sure you compute the means and standard deviations on the training set, and use these statistics to scale the training set, the validation set and the test set (you should not fit the validation set or the test set, and computing the means and standard deviations counts as \"fitting\"). pixel_means = X_train . mean(axis = 0 ) pixel_stds = X_train . std(axis = 0 ) X_train_scaled = (X_train - pixel_means) / pixel_stds X_valid_scaled = (X_valid - pixel_means) / pixel_stds from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train_scaled = scaler . fit_transform(X_train . astype(np . float32)\\ . reshape( - 1 , 28 * 28 )) . reshape( - 1 , 28 , 28 ) X_valid_scaled = scaler . transform(X_valid . astype(np . float32)\\ . reshape( - 1 , 28 * 28 )) . reshape( - 1 , 28 , 28 ) X_test_scaled = scaler . transform(X_test . astype(np . float32)\\ . reshape( - 1 , 28 * 28 )) . reshape( - 1 , 28 , 28 ) Design neural network architecture model = keras.models.sequential([ keras.layer.layerType(parameters...), keras.layer.layerType(parameters...), keras.layer.layerType(parameters...), keras.layer.layerType(parameters...) ]) model = keras . models . Sequential([ keras . layers . Flatten(input_shape = [ 28 , 28 ]), keras . layers . Dense( 300 , activation = \"relu\" ), keras . layers . Dense( 100 , activation = \"relu\" ), keras . layers . Dense( 10 , activation = \"softmax\" ) ]) model . summary() Model: \"sequential_5\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_3 (Flatten) (None, 784) 0 _________________________________________________________________ dense_17 (Dense) (None, 300) 235500 _________________________________________________________________ dense_18 (Dense) (None, 100) 30100 _________________________________________________________________ dense_19 (Dense) (None, 10) 1010 ================================================================= Total params: 266,610 Trainable params: 266,610 Non-trainable params: 0 _________________________________________________________________ Configure model model . compile(loss = \"sparse_categorical_crossentropy\" , optimizer = keras . optimizers . SGD( 1e-3 ),\\ metrics = [ \"accuracy\" ]) Train! history = model . fit(X_train_scaled,\\ y_train,\\ batch_size = 128 ,\\ epochs = 20 , validation_data = (X_valid_scaled, y_valid)) Train on 55000 samples, validate on 5000 samples Epoch 1/20 55000/55000 [==============================] - 2s 32us/sample - loss: 1.4371 - accuracy: 0.5344 - val_loss: 0.9933 - val_accuracy: 0.6844 Epoch 2/20 55000/55000 [==============================] - 2s 29us/sample - loss: 0.8873 - accuracy: 0.7155 - val_loss: 0.7911 - val_accuracy: 0.7424 Epoch 3/20 55000/55000 [==============================] - 2s 29us/sample - loss: 0.7508 - accuracy: 0.7513 - val_loss: 0.7003 - val_accuracy: 0.7688 Epoch 4/20 55000/55000 [==============================] - 1s 27us/sample - loss: 0.6783 - accuracy: 0.7703 - val_loss: 0.6451 - val_accuracy: 0.7822 Epoch 5/20 55000/55000 [==============================] - 1s 27us/sample - loss: 0.6306 - accuracy: 0.7848 - val_loss: 0.6072 - val_accuracy: 0.7956 Epoch 6/20 55000/55000 [==============================] - 2s 27us/sample - loss: 0.5958 - accuracy: 0.7960 - val_loss: 0.5785 - val_accuracy: 0.8012 Epoch 7/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.5689 - accuracy: 0.8047 - val_loss: 0.5556 - val_accuracy: 0.8094 Epoch 8/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.5471 - accuracy: 0.8118 - val_loss: 0.5374 - val_accuracy: 0.8176 Epoch 9/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.5290 - accuracy: 0.8168 - val_loss: 0.5221 - val_accuracy: 0.8244 Epoch 10/20 55000/55000 [==============================] - 2s 29us/sample - loss: 0.5138 - accuracy: 0.8212 - val_loss: 0.5089 - val_accuracy: 0.8276 Epoch 11/20 55000/55000 [==============================] - 2s 32us/sample - loss: 0.5007 - accuracy: 0.8255 - val_loss: 0.4979 - val_accuracy: 0.8330 Epoch 12/20 55000/55000 [==============================] - 2s 30us/sample - loss: 0.4892 - accuracy: 0.8293 - val_loss: 0.4879 - val_accuracy: 0.8384 Epoch 13/20 55000/55000 [==============================] - 2s 30us/sample - loss: 0.4792 - accuracy: 0.8322 - val_loss: 0.4793 - val_accuracy: 0.8398 Epoch 14/20 55000/55000 [==============================] - 2s 30us/sample - loss: 0.4702 - accuracy: 0.8355 - val_loss: 0.4713 - val_accuracy: 0.8438 Epoch 15/20 55000/55000 [==============================] - 2s 29us/sample - loss: 0.4622 - accuracy: 0.8381 - val_loss: 0.4644 - val_accuracy: 0.8470 Epoch 16/20 55000/55000 [==============================] - 2s 29us/sample - loss: 0.4549 - accuracy: 0.8397 - val_loss: 0.4583 - val_accuracy: 0.8494 Epoch 17/20 55000/55000 [==============================] - 2s 30us/sample - loss: 0.4482 - accuracy: 0.8425 - val_loss: 0.4527 - val_accuracy: 0.8512 Epoch 18/20 55000/55000 [==============================] - 2s 29us/sample - loss: 0.4421 - accuracy: 0.8443 - val_loss: 0.4476 - val_accuracy: 0.8516 Epoch 19/20 55000/55000 [==============================] - 2s 29us/sample - loss: 0.4365 - accuracy: 0.8468 - val_loss: 0.4428 - val_accuracy: 0.8532 Epoch 20/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.4313 - accuracy: 0.8479 - val_loss: 0.4386 - val_accuracy: 0.8540 model . evaluate(X_test_scaled, y_test) 10000/10000 [==============================] - 0s 39us/sample - loss: 0.4680 - accuracy: 0.8349 [0.4679875907897949, 0.8349] def plot_learning_curves (history): pd . DataFrame(history . history) . plot(figsize = ( 8 , 5 )) plt . grid( True ) plt . gca() . set_ylim( 0 , 1 ) plt . show() plot_learning_curves(history) Performing Inference model . evaluate(X_test, y_test) 10000/10000 [==============================] - 0s 33us/sample - loss: 6.5878 - accuracy: 0.5762 [6.58776294708252, 0.5762] X_valid . shape (5000, 28, 28) n_new = 10 X_new = X_test[:n_new] y_proba = model . predict(X_new) y_proba . round( 2 ) array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]], dtype=float32) y_pred = model . predict_classes(X_new) y_pred array([9, 2, 1, 1, 2, 4, 4, 4, 8, 8]) for item in y_pred: print (class_names[item]) Ankle boot Pullover Trouser Trouser Pullover Coat Coat Coat Bag Bag plt . imshow(valid_0 . reshape( 28 , 28 )) <matplotlib.image.AxesImage at 0x1a38b67550>","title":"Neural Network-FeatureScaling"},{"location":"feature/feature/#neural-net-feature-scaling","text":"For classifying MNIST digits.","title":"Neural Net : Feature Scaling"},{"location":"feature/feature/#load-dependencies","text":"#pip install tensorflow==2.0.0-beta0 #pip install --upgrade tensorflow==2.0.0-beta0 import tensorflow as tf from tensorflow import keras # tf.keras import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import os import pandas as pd import sklearn import sys import time sns . set() % matplotlib inline % load_ext tensorboard The tensorboard extension is already loaded. To reload it, use: %reload_ext tensorboard print ( \"python\" , sys . version) for module in mpl, np, pd, sklearn, tf, keras: print (module . __name__ , module . __version__) python 3.7.1 (default, Dec 14 2018, 13:28:58) [Clang 4.0.1 (tags/RELEASE_401/final)] matplotlib 3.0.2 numpy 1.15.4 pandas 0.23.4 sklearn 0.20.1 tensorflow 2.0.0-beta0 tensorflow.python.keras.api._v2.keras 2.2.4-tf","title":"Load dependencies"},{"location":"feature/feature/#load-data","text":"fashion_mnist = keras . datasets . fashion_mnist (X_train_full, y_train_full), (X_test, y_test) = ( fashion_mnist . load_data()) X_valid, X_train = X_train_full[: 5000 ], X_train_full[ 5000 :] y_valid, y_train = y_train_full[: 5000 ], y_train_full[ 5000 :] X_train . shape, y_train . shape ((55000, 28, 28), (55000,)) class_names = [ \"T-shirt/top\" , \"Trouser\" , \"Pullover\" , \"Dress\" , \"Coat\" , \"Sandal\" , \"Shirt\" , \"Sneaker\" , \"Bag\" , \"Ankle boot\" ] #X_train[0] n_rows = 5 n_cols = 10 plt . figure(figsize = (n_cols * 1.4 , n_rows * 1.6 )) for row in range (n_rows): for col in range (n_cols): index = n_cols * row + col plt . subplot(n_rows, n_cols, index + 1 ) plt . imshow(X_train[index], cmap = \"binary\" , interpolation = \"nearest\" ) plt . axis( 'off' ) plt . title(class_names[y_train[index]]) plt . show() y_train[ 0 : 50 ] array([4, 0, 7, 9, 9, 9, 4, 4, 3, 4, 0, 1, 8, 6, 3, 6, 4, 3, 2, 8, 7, 3, 4, 7, 1, 3, 4, 2, 0, 8, 5, 5, 9, 1, 5, 3, 5, 9, 0, 3, 9, 6, 4, 2, 9, 0, 8, 3, 3, 2], dtype=uint8) class_names[y_train[ 0 ]] 'Coat'","title":"Load data"},{"location":"feature/feature/#preprocess-data-feature-scaling","text":"When using Gradient Descent, it is usually best to ensure that the features all have a similar scale, preferably with a Normal distribution. Try to standardize the pixel values and see if this improves the performance of your neural network. Tips : * For each feature (pixel intensity), you must subtract the mean() of that feature (across all instances, so use axis=0 ) and divide by its standard deviation ( std() , again axis=0 ). Alternatively, you can use Scikit-Learn's StandardScaler . * Make sure you compute the means and standard deviations on the training set, and use these statistics to scale the training set, the validation set and the test set (you should not fit the validation set or the test set, and computing the means and standard deviations counts as \"fitting\"). pixel_means = X_train . mean(axis = 0 ) pixel_stds = X_train . std(axis = 0 ) X_train_scaled = (X_train - pixel_means) / pixel_stds X_valid_scaled = (X_valid - pixel_means) / pixel_stds from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train_scaled = scaler . fit_transform(X_train . astype(np . float32)\\ . reshape( - 1 , 28 * 28 )) . reshape( - 1 , 28 , 28 ) X_valid_scaled = scaler . transform(X_valid . astype(np . float32)\\ . reshape( - 1 , 28 * 28 )) . reshape( - 1 , 28 , 28 ) X_test_scaled = scaler . transform(X_test . astype(np . float32)\\ . reshape( - 1 , 28 * 28 )) . reshape( - 1 , 28 , 28 )","title":"Preprocess data : Feature Scaling"},{"location":"feature/feature/#design-neural-network-architecture","text":"model = keras.models.sequential([ keras.layer.layerType(parameters...), keras.layer.layerType(parameters...), keras.layer.layerType(parameters...), keras.layer.layerType(parameters...) ]) model = keras . models . Sequential([ keras . layers . Flatten(input_shape = [ 28 , 28 ]), keras . layers . Dense( 300 , activation = \"relu\" ), keras . layers . Dense( 100 , activation = \"relu\" ), keras . layers . Dense( 10 , activation = \"softmax\" ) ]) model . summary() Model: \"sequential_5\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_3 (Flatten) (None, 784) 0 _________________________________________________________________ dense_17 (Dense) (None, 300) 235500 _________________________________________________________________ dense_18 (Dense) (None, 100) 30100 _________________________________________________________________ dense_19 (Dense) (None, 10) 1010 ================================================================= Total params: 266,610 Trainable params: 266,610 Non-trainable params: 0 _________________________________________________________________","title":"Design neural network architecture"},{"location":"feature/feature/#configure-model","text":"model . compile(loss = \"sparse_categorical_crossentropy\" , optimizer = keras . optimizers . SGD( 1e-3 ),\\ metrics = [ \"accuracy\" ])","title":"Configure model"},{"location":"feature/feature/#train","text":"history = model . fit(X_train_scaled,\\ y_train,\\ batch_size = 128 ,\\ epochs = 20 , validation_data = (X_valid_scaled, y_valid)) Train on 55000 samples, validate on 5000 samples Epoch 1/20 55000/55000 [==============================] - 2s 32us/sample - loss: 1.4371 - accuracy: 0.5344 - val_loss: 0.9933 - val_accuracy: 0.6844 Epoch 2/20 55000/55000 [==============================] - 2s 29us/sample - loss: 0.8873 - accuracy: 0.7155 - val_loss: 0.7911 - val_accuracy: 0.7424 Epoch 3/20 55000/55000 [==============================] - 2s 29us/sample - loss: 0.7508 - accuracy: 0.7513 - val_loss: 0.7003 - val_accuracy: 0.7688 Epoch 4/20 55000/55000 [==============================] - 1s 27us/sample - loss: 0.6783 - accuracy: 0.7703 - val_loss: 0.6451 - val_accuracy: 0.7822 Epoch 5/20 55000/55000 [==============================] - 1s 27us/sample - loss: 0.6306 - accuracy: 0.7848 - val_loss: 0.6072 - val_accuracy: 0.7956 Epoch 6/20 55000/55000 [==============================] - 2s 27us/sample - loss: 0.5958 - accuracy: 0.7960 - val_loss: 0.5785 - val_accuracy: 0.8012 Epoch 7/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.5689 - accuracy: 0.8047 - val_loss: 0.5556 - val_accuracy: 0.8094 Epoch 8/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.5471 - accuracy: 0.8118 - val_loss: 0.5374 - val_accuracy: 0.8176 Epoch 9/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.5290 - accuracy: 0.8168 - val_loss: 0.5221 - val_accuracy: 0.8244 Epoch 10/20 55000/55000 [==============================] - 2s 29us/sample - loss: 0.5138 - accuracy: 0.8212 - val_loss: 0.5089 - val_accuracy: 0.8276 Epoch 11/20 55000/55000 [==============================] - 2s 32us/sample - loss: 0.5007 - accuracy: 0.8255 - val_loss: 0.4979 - val_accuracy: 0.8330 Epoch 12/20 55000/55000 [==============================] - 2s 30us/sample - loss: 0.4892 - accuracy: 0.8293 - val_loss: 0.4879 - val_accuracy: 0.8384 Epoch 13/20 55000/55000 [==============================] - 2s 30us/sample - loss: 0.4792 - accuracy: 0.8322 - val_loss: 0.4793 - val_accuracy: 0.8398 Epoch 14/20 55000/55000 [==============================] - 2s 30us/sample - loss: 0.4702 - accuracy: 0.8355 - val_loss: 0.4713 - val_accuracy: 0.8438 Epoch 15/20 55000/55000 [==============================] - 2s 29us/sample - loss: 0.4622 - accuracy: 0.8381 - val_loss: 0.4644 - val_accuracy: 0.8470 Epoch 16/20 55000/55000 [==============================] - 2s 29us/sample - loss: 0.4549 - accuracy: 0.8397 - val_loss: 0.4583 - val_accuracy: 0.8494 Epoch 17/20 55000/55000 [==============================] - 2s 30us/sample - loss: 0.4482 - accuracy: 0.8425 - val_loss: 0.4527 - val_accuracy: 0.8512 Epoch 18/20 55000/55000 [==============================] - 2s 29us/sample - loss: 0.4421 - accuracy: 0.8443 - val_loss: 0.4476 - val_accuracy: 0.8516 Epoch 19/20 55000/55000 [==============================] - 2s 29us/sample - loss: 0.4365 - accuracy: 0.8468 - val_loss: 0.4428 - val_accuracy: 0.8532 Epoch 20/20 55000/55000 [==============================] - 2s 28us/sample - loss: 0.4313 - accuracy: 0.8479 - val_loss: 0.4386 - val_accuracy: 0.8540 model . evaluate(X_test_scaled, y_test) 10000/10000 [==============================] - 0s 39us/sample - loss: 0.4680 - accuracy: 0.8349 [0.4679875907897949, 0.8349] def plot_learning_curves (history): pd . DataFrame(history . history) . plot(figsize = ( 8 , 5 )) plt . grid( True ) plt . gca() . set_ylim( 0 , 1 ) plt . show() plot_learning_curves(history)","title":"Train!"},{"location":"feature/feature/#performing-inference","text":"model . evaluate(X_test, y_test) 10000/10000 [==============================] - 0s 33us/sample - loss: 6.5878 - accuracy: 0.5762 [6.58776294708252, 0.5762] X_valid . shape (5000, 28, 28) n_new = 10 X_new = X_test[:n_new] y_proba = model . predict(X_new) y_proba . round( 2 ) array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]], dtype=float32) y_pred = model . predict_classes(X_new) y_pred array([9, 2, 1, 1, 2, 4, 4, 4, 8, 8]) for item in y_pred: print (class_names[item]) Ankle boot Pullover Trouser Trouser Pullover Coat Coat Coat Bag Bag plt . imshow(valid_0 . reshape( 28 , 28 )) <matplotlib.image.AxesImage at 0x1a38b67550>","title":"Performing Inference"},{"location":"intro/intro/","text":"Neural Net : Introduction For classifying MNIST digits. Intall Tensorflow #pip install tensorflow==2.0.0-beta0 #pip install --upgrade tensorflow==2.0.0-beta0 import tensorflow as tf from tensorflow import keras # tf.keras import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import os import pandas as pd import sklearn import sys import time sns . set() % matplotlib inline % load_ext tensorboard The tensorboard extension is already loaded. To reload it, use: %reload_ext tensorboard print ( \"python\" , sys . version) for module in mpl, np, pd, sklearn, tf, keras: print (module . __name__ , module . __version__) python 3.7.1 (default, Dec 14 2018, 13:28:58) [Clang 4.0.1 (tags/RELEASE_401/final)] matplotlib 3.0.2 numpy 1.15.4 pandas 0.23.4 sklearn 0.20.1 tensorflow 2.0.0-beta0 tensorflow.python.keras.api._v2.keras 2.2.4-tf Load data (X_train, y_train), (X_valid, y_valid) = keras . datasets . mnist . load_data() What are the shape of the data? print (X_train . shape, y_train . shape,X_valid . shape, y_valid . shape) (60000, 28, 28) (60000,) (10000, 28, 28) (10000,) There are 60,000 Train data which are 28 by 28 pixcel images and There are 10,000 Train data which are also 28 by 28 pixcel images Data Visualization import matplotlib.pyplot as plt % matplotlib inline plt . figure(figsize = [ 10 , 8 ]) for i in range ( 1 , 51 ): plt . subplot( 5 , 10 ,i) plt . imshow(X_train[i] . reshape( 28 , 28 )) plt . show() y_train[ 1 : 26 ] array([0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2], dtype=uint8) Preprocess data We flaten the data (from 28 by 28 to 784) before it is feed into the model. We can also create a layer as a front layer to perform this flattening step. X_train = X_train . reshape( 60000 , 784 ) . astype( 'float32' ) X_valid = X_valid . reshape( 10000 , 784 ) . astype( 'float32' ) Data Normalization This step converts the pixcel value ranging from 0 to 255 into 0 to 1. X_train /= 255 X_valid /= 255 Prepare Labels We convert label 1,2,3,4,5,6,7,8,9 in label vectors n_classes = 10 y_train = keras . utils . to_categorical(y_train, n_classes) y_valid = keras . utils . to_categorical(y_valid, n_classes) y_train[ 1 : 16 ] array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], dtype=float32) Design neural network architecture Demo: https://datafiction.github.io/docs/ml/perceptron/dlnd-your-first-neural-network/ After the pixels are flattened, the network consists of a sequence of two tf.keras.layers.Dense layers. These are densely connected, or fully connected, neural layers. The first Dense layer has 64 nodes (or neurons). The second (and last) layer is a 10-node softmax layer that returns an array of 10 probability scores that sum to 1. Each node contains a score that indicates the probability that the current image belongs to one of the 10 classes. model = keras.models.sequential([ keras.layer.layerType(parameters...), keras.layer.layerType(parameters...), keras.layer.layerType(parameters...), keras.layer.layerType(parameters...) ]) model = keras . models . Sequential([ keras . layers . Dense( 64 ,\\ activation = 'sigmoid' ,\\ input_shape = ( 784 ,)), keras . layers . Dense( 10 ,\\ activation = 'softmax' ) ]) model . summary() Model: \"sequential_1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_2 (Dense) (None, 64) 50240 _________________________________________________________________ dense_3 (Dense) (None, 10) 650 ================================================================= Total params: 50,890 Trainable params: 50,890 Non-trainable params: 0 _________________________________________________________________ ( 784 * 64 ) + 64 , ( 64 * 10 ) + 10 (50240, 650) Configure model Before the model is ready for training, it needs a few more settings. These are added during the model's compile step: Loss function \u2014This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction. Optimizer \u2014This is how the model is updated based on the data it sees and its loss function. Metrics \u2014Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified. model . compile(loss = 'categorical_crossentropy' ,\\ optimizer = 'adam' ,\\ metrics = [ 'accuracy' ]) Train! Now your model is ready to be trained. Call its fit() method, passing it the input features ( X_train ) and the target classes ( y_train ). Set epochs=10 (or else it will just run for a single epoch). You can also (optionally) pass the validation data by setting validation_data=(X_valid, y_valid) . If you do, Keras will compute the loss and the additional metrics (the accuracy in this case) on the validation set at the end of each epoch. If the performance on the training set is much better than on the validation set, your model is probably overfitting the training set (or there is a bug, such as a mismatch between the training set and the validation set). Note : the fit() method will return a History object containing training stats. Make sure to preserve it ( history = model.fit(...) ). history = model . fit(X_train, y_train,\\ batch_size = 128 ,\\ epochs = 10 ,\\ verbose = 1 ,\\ validation_data = (X_valid, y_valid)) Train on 60000 samples, validate on 10000 samples Epoch 1/10 60000/60000 [==============================] - 1s 15us/sample - loss: 0.1051 - accuracy: 0.9707 - val_loss: 0.1200 - val_accuracy: 0.9641 Epoch 2/10 60000/60000 [==============================] - 1s 15us/sample - loss: 0.0973 - accuracy: 0.9732 - val_loss: 0.1133 - val_accuracy: 0.9649 Epoch 3/10 60000/60000 [==============================] - 1s 15us/sample - loss: 0.0904 - accuracy: 0.9752 - val_loss: 0.1092 - val_accuracy: 0.9672 Epoch 4/10 60000/60000 [==============================] - 1s 15us/sample - loss: 0.0843 - accuracy: 0.9773 - val_loss: 0.1047 - val_accuracy: 0.9689 Epoch 5/10 60000/60000 [==============================] - 1s 15us/sample - loss: 0.0786 - accuracy: 0.9789 - val_loss: 0.1019 - val_accuracy: 0.9680 Epoch 6/10 60000/60000 [==============================] - 1s 15us/sample - loss: 0.0737 - accuracy: 0.9800 - val_loss: 0.0997 - val_accuracy: 0.9697 Epoch 7/10 60000/60000 [==============================] - 1s 16us/sample - loss: 0.0689 - accuracy: 0.9821 - val_loss: 0.0976 - val_accuracy: 0.9702 Epoch 8/10 60000/60000 [==============================] - 1s 14us/sample - loss: 0.0648 - accuracy: 0.9831 - val_loss: 0.0957 - val_accuracy: 0.9706 Epoch 9/10 60000/60000 [==============================] - 1s 14us/sample - loss: 0.0609 - accuracy: 0.9845 - val_loss: 0.0930 - val_accuracy: 0.9713 Epoch 10/10 60000/60000 [==============================] - 1s 14us/sample - loss: 0.0574 - accuracy: 0.9849 - val_loss: 0.0922 - val_accuracy: 0.9723 Plot History Try running pd.DataFrame(history.history).plot() to plot the learning curves. To make the graph more readable, you can also set figsize=(8, 5) , call plt.grid(True) and plt.gca().set_ylim(0, 1) . def plot_learning_curves (history): pd . DataFrame(history . history) . plot(figsize = ( 8 , 5 )) plt . grid( True ) plt . gca() . set_ylim( 0 , 1 ) plt . show() plot_learning_curves(history) Performing Inference Call the model's evaluate() method, passing it the test set ( X_test and y_test ). This will compute the loss (cross-entropy) on the test set, as well as all the additional metrics (in this case, the accuracy). Your model should achieve over 80% accuracy on the test set. model . evaluate(X_train, y_train) 60000/60000 [==============================] - 1s 18us/sample - loss: 0.0519 - accuracy: 0.9872 [0.051851865920548634, 0.98723334] valid_0 = X_valid[ 0 ] . reshape( 1 , 784 ) model . predict(valid_0) array([[2.8813774e-09, 3.8345047e-10, 1.1818415e-04, 2.0551306e-06, 2.2122413e-11, 1.7986340e-09, 1.0651025e-13, 9.9987960e-01, 1.7287803e-08, 1.0190855e-07]], dtype=float32) model . predict_classes(valid_0) array([7]) plt . imshow(valid_0 . reshape( 28 , 28 )) <matplotlib.image.AxesImage at 0x1a478e6278> Define X_new as the first 10 instances of the test set. Call the model's predict() method to estimate the probability of each class for each instance (for better readability, you may use the output array's round() method): n_new = 10 X_new = X_train[:n_new] y_proba = model . predict(X_new) y_proba . round( 2 ) array([[0. , 0. , 0. , 0.01, 0. , 0.99, 0. , 0. , 0. , 0. ], [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0. , 0.01, 0. , 0.99, 0. , 0. , 0. , 0. , 0. ], [0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.99], [0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. ]], dtype=float32) Often, you may only be interested in the most likely class. Use np.argmax() to get the class ID of the most likely class for each instance. Tip : you want to set axis=1 . y_pred = y_proba . argmax(axis = 1 ) y_pred array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4]) Call the model's predict_classes() method for X_new . You should get the same result as above. y_pred = model . predict_classes(X_new) y_pred array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4]) (Optional) It is often useful to know how confident the model is for each prediction. Try finding the estimated probability for each predicted class using np.max() . y_proba . max(axis = 1 ) . round( 2 ) array([0.99, 1. , 0.99, 1. , 0.99, 1. , 1. , 1. , 1. , 1. ], dtype=float32) (Optional) It is frequent to want the top k classes and their estimated probabilities rather just the most likely class. You can use np.argsort() for this. k = 3 top_k = np . argsort( - y_proba, axis = 1 )[:, :k] top_k array([[5, 3, 1], [0, 5, 2], [4, 2, 7], [1, 2, 7], [9, 4, 7], [2, 8, 9], [1, 8, 3], [3, 9, 8], [1, 9, 7], [4, 5, 6]]) row_indices = np . tile(np . arange( len (top_k)), [k, 1 ]) . T y_proba[row_indices, top_k] . round( 2 ) array([[0.99, 0.01, 0. ], [1. , 0. , 0. ], [0.99, 0.01, 0. ], [1. , 0. , 0. ], [0.99, 0. , 0. ], [1. , 0. , 0. ], [1. , 0. , 0. ], [1. , 0. , 0. ], [1. , 0. , 0. ], [1. , 0. , 0. ]], dtype=float32)","title":"Neural Network Introduction"},{"location":"intro/intro/#neural-net-introduction","text":"For classifying MNIST digits.","title":"Neural Net : Introduction"},{"location":"intro/intro/#intall-tensorflow","text":"#pip install tensorflow==2.0.0-beta0 #pip install --upgrade tensorflow==2.0.0-beta0 import tensorflow as tf from tensorflow import keras # tf.keras import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import os import pandas as pd import sklearn import sys import time sns . set() % matplotlib inline % load_ext tensorboard The tensorboard extension is already loaded. To reload it, use: %reload_ext tensorboard print ( \"python\" , sys . version) for module in mpl, np, pd, sklearn, tf, keras: print (module . __name__ , module . __version__) python 3.7.1 (default, Dec 14 2018, 13:28:58) [Clang 4.0.1 (tags/RELEASE_401/final)] matplotlib 3.0.2 numpy 1.15.4 pandas 0.23.4 sklearn 0.20.1 tensorflow 2.0.0-beta0 tensorflow.python.keras.api._v2.keras 2.2.4-tf","title":"Intall Tensorflow"},{"location":"intro/intro/#load-data","text":"(X_train, y_train), (X_valid, y_valid) = keras . datasets . mnist . load_data()","title":"Load data"},{"location":"intro/intro/#what-are-the-shape-of-the-data","text":"print (X_train . shape, y_train . shape,X_valid . shape, y_valid . shape) (60000, 28, 28) (60000,) (10000, 28, 28) (10000,) There are 60,000 Train data which are 28 by 28 pixcel images and There are 10,000 Train data which are also 28 by 28 pixcel images","title":"What are the shape of the data?"},{"location":"intro/intro/#data-visualization","text":"import matplotlib.pyplot as plt % matplotlib inline plt . figure(figsize = [ 10 , 8 ]) for i in range ( 1 , 51 ): plt . subplot( 5 , 10 ,i) plt . imshow(X_train[i] . reshape( 28 , 28 )) plt . show() y_train[ 1 : 26 ] array([0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2], dtype=uint8)","title":"Data Visualization"},{"location":"intro/intro/#preprocess-data","text":"We flaten the data (from 28 by 28 to 784) before it is feed into the model. We can also create a layer as a front layer to perform this flattening step. X_train = X_train . reshape( 60000 , 784 ) . astype( 'float32' ) X_valid = X_valid . reshape( 10000 , 784 ) . astype( 'float32' )","title":"Preprocess data"},{"location":"intro/intro/#data-normalization","text":"This step converts the pixcel value ranging from 0 to 255 into 0 to 1. X_train /= 255 X_valid /= 255","title":"Data Normalization"},{"location":"intro/intro/#prepare-labels","text":"We convert label 1,2,3,4,5,6,7,8,9 in label vectors n_classes = 10 y_train = keras . utils . to_categorical(y_train, n_classes) y_valid = keras . utils . to_categorical(y_valid, n_classes) y_train[ 1 : 16 ] array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], dtype=float32)","title":"Prepare Labels"},{"location":"intro/intro/#design-neural-network-architecture","text":"Demo: https://datafiction.github.io/docs/ml/perceptron/dlnd-your-first-neural-network/ After the pixels are flattened, the network consists of a sequence of two tf.keras.layers.Dense layers. These are densely connected, or fully connected, neural layers. The first Dense layer has 64 nodes (or neurons). The second (and last) layer is a 10-node softmax layer that returns an array of 10 probability scores that sum to 1. Each node contains a score that indicates the probability that the current image belongs to one of the 10 classes. model = keras.models.sequential([ keras.layer.layerType(parameters...), keras.layer.layerType(parameters...), keras.layer.layerType(parameters...), keras.layer.layerType(parameters...) ]) model = keras . models . Sequential([ keras . layers . Dense( 64 ,\\ activation = 'sigmoid' ,\\ input_shape = ( 784 ,)), keras . layers . Dense( 10 ,\\ activation = 'softmax' ) ]) model . summary() Model: \"sequential_1\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_2 (Dense) (None, 64) 50240 _________________________________________________________________ dense_3 (Dense) (None, 10) 650 ================================================================= Total params: 50,890 Trainable params: 50,890 Non-trainable params: 0 _________________________________________________________________ ( 784 * 64 ) + 64 , ( 64 * 10 ) + 10 (50240, 650)","title":"Design neural network architecture"},{"location":"intro/intro/#configure-model","text":"Before the model is ready for training, it needs a few more settings. These are added during the model's compile step: Loss function \u2014This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction. Optimizer \u2014This is how the model is updated based on the data it sees and its loss function. Metrics \u2014Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified. model . compile(loss = 'categorical_crossentropy' ,\\ optimizer = 'adam' ,\\ metrics = [ 'accuracy' ])","title":"Configure model"},{"location":"intro/intro/#train","text":"Now your model is ready to be trained. Call its fit() method, passing it the input features ( X_train ) and the target classes ( y_train ). Set epochs=10 (or else it will just run for a single epoch). You can also (optionally) pass the validation data by setting validation_data=(X_valid, y_valid) . If you do, Keras will compute the loss and the additional metrics (the accuracy in this case) on the validation set at the end of each epoch. If the performance on the training set is much better than on the validation set, your model is probably overfitting the training set (or there is a bug, such as a mismatch between the training set and the validation set). Note : the fit() method will return a History object containing training stats. Make sure to preserve it ( history = model.fit(...) ). history = model . fit(X_train, y_train,\\ batch_size = 128 ,\\ epochs = 10 ,\\ verbose = 1 ,\\ validation_data = (X_valid, y_valid)) Train on 60000 samples, validate on 10000 samples Epoch 1/10 60000/60000 [==============================] - 1s 15us/sample - loss: 0.1051 - accuracy: 0.9707 - val_loss: 0.1200 - val_accuracy: 0.9641 Epoch 2/10 60000/60000 [==============================] - 1s 15us/sample - loss: 0.0973 - accuracy: 0.9732 - val_loss: 0.1133 - val_accuracy: 0.9649 Epoch 3/10 60000/60000 [==============================] - 1s 15us/sample - loss: 0.0904 - accuracy: 0.9752 - val_loss: 0.1092 - val_accuracy: 0.9672 Epoch 4/10 60000/60000 [==============================] - 1s 15us/sample - loss: 0.0843 - accuracy: 0.9773 - val_loss: 0.1047 - val_accuracy: 0.9689 Epoch 5/10 60000/60000 [==============================] - 1s 15us/sample - loss: 0.0786 - accuracy: 0.9789 - val_loss: 0.1019 - val_accuracy: 0.9680 Epoch 6/10 60000/60000 [==============================] - 1s 15us/sample - loss: 0.0737 - accuracy: 0.9800 - val_loss: 0.0997 - val_accuracy: 0.9697 Epoch 7/10 60000/60000 [==============================] - 1s 16us/sample - loss: 0.0689 - accuracy: 0.9821 - val_loss: 0.0976 - val_accuracy: 0.9702 Epoch 8/10 60000/60000 [==============================] - 1s 14us/sample - loss: 0.0648 - accuracy: 0.9831 - val_loss: 0.0957 - val_accuracy: 0.9706 Epoch 9/10 60000/60000 [==============================] - 1s 14us/sample - loss: 0.0609 - accuracy: 0.9845 - val_loss: 0.0930 - val_accuracy: 0.9713 Epoch 10/10 60000/60000 [==============================] - 1s 14us/sample - loss: 0.0574 - accuracy: 0.9849 - val_loss: 0.0922 - val_accuracy: 0.9723","title":"Train!"},{"location":"intro/intro/#plot-history","text":"Try running pd.DataFrame(history.history).plot() to plot the learning curves. To make the graph more readable, you can also set figsize=(8, 5) , call plt.grid(True) and plt.gca().set_ylim(0, 1) . def plot_learning_curves (history): pd . DataFrame(history . history) . plot(figsize = ( 8 , 5 )) plt . grid( True ) plt . gca() . set_ylim( 0 , 1 ) plt . show() plot_learning_curves(history)","title":"Plot History"},{"location":"intro/intro/#performing-inference","text":"Call the model's evaluate() method, passing it the test set ( X_test and y_test ). This will compute the loss (cross-entropy) on the test set, as well as all the additional metrics (in this case, the accuracy). Your model should achieve over 80% accuracy on the test set. model . evaluate(X_train, y_train) 60000/60000 [==============================] - 1s 18us/sample - loss: 0.0519 - accuracy: 0.9872 [0.051851865920548634, 0.98723334] valid_0 = X_valid[ 0 ] . reshape( 1 , 784 ) model . predict(valid_0) array([[2.8813774e-09, 3.8345047e-10, 1.1818415e-04, 2.0551306e-06, 2.2122413e-11, 1.7986340e-09, 1.0651025e-13, 9.9987960e-01, 1.7287803e-08, 1.0190855e-07]], dtype=float32) model . predict_classes(valid_0) array([7]) plt . imshow(valid_0 . reshape( 28 , 28 )) <matplotlib.image.AxesImage at 0x1a478e6278> Define X_new as the first 10 instances of the test set. Call the model's predict() method to estimate the probability of each class for each instance (for better readability, you may use the output array's round() method): n_new = 10 X_new = X_train[:n_new] y_proba = model . predict(X_new) y_proba . round( 2 ) array([[0. , 0. , 0. , 0.01, 0. , 0.99, 0. , 0. , 0. , 0. ], [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0. , 0.01, 0. , 0.99, 0. , 0. , 0. , 0. , 0. ], [0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.99], [0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. ]], dtype=float32) Often, you may only be interested in the most likely class. Use np.argmax() to get the class ID of the most likely class for each instance. Tip : you want to set axis=1 . y_pred = y_proba . argmax(axis = 1 ) y_pred array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4]) Call the model's predict_classes() method for X_new . You should get the same result as above. y_pred = model . predict_classes(X_new) y_pred array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4]) (Optional) It is often useful to know how confident the model is for each prediction. Try finding the estimated probability for each predicted class using np.max() . y_proba . max(axis = 1 ) . round( 2 ) array([0.99, 1. , 0.99, 1. , 0.99, 1. , 1. , 1. , 1. , 1. ], dtype=float32) (Optional) It is frequent to want the top k classes and their estimated probabilities rather just the most likely class. You can use np.argsort() for this. k = 3 top_k = np . argsort( - y_proba, axis = 1 )[:, :k] top_k array([[5, 3, 1], [0, 5, 2], [4, 2, 7], [1, 2, 7], [9, 4, 7], [2, 8, 9], [1, 8, 3], [3, 9, 8], [1, 9, 7], [4, 5, 6]]) row_indices = np . tile(np . arange( len (top_k)), [k, 1 ]) . T y_proba[row_indices, top_k] . round( 2 ) array([[0.99, 0.01, 0. ], [1. , 0. , 0. ], [0.99, 0.01, 0. ], [1. , 0. , 0. ], [0.99, 0. , 0. ], [1. , 0. , 0. ], [1. , 0. , 0. ], [1. , 0. , 0. ], [1. , 0. , 0. ], [1. , 0. , 0. ]], dtype=float32)","title":"Performing Inference"},{"location":"lstm/lstm/","text":"LSTM-Introduction For classifying MNIST digits. Intall Tensorflow #pip install tensorflow==2.0.0-beta0 #pip install --upgrade tensorflow==2.0.0-beta0 import tensorflow as tf from tensorflow import keras # tf.keras import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import os import pandas as pd import sklearn import sys import time sns . set() % matplotlib inline % load_ext tensorboard The tensorboard extension is already loaded. To reload it, use: %reload_ext tensorboard print ( \"python\" , sys . version) for module in mpl, np, pd, sklearn, tf, keras: print (module . __name__ , module . __version__) python 3.7.1 (default, Dec 14 2018, 13:28:58) [Clang 4.0.1 (tags/RELEASE_401/final)] matplotlib 3.0.2 numpy 1.15.4 pandas 0.23.4 sklearn 0.20.1 tensorflow 2.0.0-beta0 tensorflow.python.keras.api._v2.keras 2.2.4-tf Load data time = np . arange( 0 , 100 , 0.1 ) sin = np . sin(time) + np . random . normal(scale = 0.5 , size = len (time)) plt . figure(figsize = [ 14 , 8 ]) plt . plot(time, sin, label = 'sine (with noise)' ); plt . legend(); df = pd . DataFrame( dict (sine = sin), index = time, columns = [ 'sine' ]) df . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sine 0.0 0.699236 0.1 -0.377399 0.2 -0.200619 0.3 0.851897 0.4 0.487359 train_size = int ( len (df) * 0.8 ) test_size = len (df) - train_size train, test = df . iloc[ 0 :train_size], df . iloc[train_size: len (df)] print ( len (train), len (test)) 800 200 def create_dataset (X, y, time_steps = 1 ): Xs, ys = [], [] for i in range ( len (X) - time_steps): v = X . iloc[i:(i + time_steps)] . values Xs . append(v) ys . append(y . iloc[i + time_steps]) return np . array(Xs), np . array(ys) time_steps = 10 # reshape to [samples, time_steps, n_features] X_train, y_train = create_dataset(train, train . sine, time_steps) X_test, y_test = create_dataset(test, test . sine, time_steps) print (X_train . shape, y_train . shape) (790, 10, 1) (790,) Model model = keras . models . Sequential([ keras . layers . LSTM(units = 128 ,\\ input_shape = (X_train . shape[ 1 ],X_train . shape[ 2 ])), keras . layers . Dense(units = 1 ) ]) model . compile( loss = 'mean_squared_error' , optimizer = keras . optimizers . Adam( 0.001 ) ) history = model . fit( X_train, y_train, epochs = 30 , batch_size = 16 , validation_split = 0.1 , verbose = 1 , shuffle = False ) Train on 711 samples, validate on 79 samples Epoch 1/30 711/711 [==============================] - 1s 2ms/sample - loss: 0.4392 - val_loss: 0.3535 Epoch 2/30 711/711 [==============================] - 0s 478us/sample - loss: 0.3619 - val_loss: 0.3273 Epoch 3/30 711/711 [==============================] - 0s 482us/sample - loss: 0.3554 - val_loss: 0.3271 Epoch 4/30 711/711 [==============================] - 0s 497us/sample - loss: 0.3540 - val_loss: 0.3273 Epoch 5/30 711/711 [==============================] - 0s 462us/sample - loss: 0.3530 - val_loss: 0.3276 Epoch 6/30 711/711 [==============================] - 0s 460us/sample - loss: 0.3522 - val_loss: 0.3278 Epoch 7/30 711/711 [==============================] - 0s 459us/sample - loss: 0.3516 - val_loss: 0.3280 Epoch 8/30 711/711 [==============================] - 0s 474us/sample - loss: 0.3511 - val_loss: 0.3280 Epoch 9/30 711/711 [==============================] - 0s 470us/sample - loss: 0.3505 - val_loss: 0.3277 Epoch 10/30 711/711 [==============================] - 0s 470us/sample - loss: 0.3498 - val_loss: 0.3271 Epoch 11/30 711/711 [==============================] - 0s 481us/sample - loss: 0.3491 - val_loss: 0.3261 Epoch 12/30 711/711 [==============================] - 0s 468us/sample - loss: 0.3482 - val_loss: 0.3248 Epoch 13/30 711/711 [==============================] - 0s 463us/sample - loss: 0.3473 - val_loss: 0.3233 Epoch 14/30 711/711 [==============================] - 0s 452us/sample - loss: 0.3463 - val_loss: 0.3217 Epoch 15/30 711/711 [==============================] - 0s 464us/sample - loss: 0.3454 - val_loss: 0.3203 Epoch 16/30 711/711 [==============================] - 0s 468us/sample - loss: 0.3447 - val_loss: 0.3192 Epoch 17/30 711/711 [==============================] - 0s 473us/sample - loss: 0.3441 - val_loss: 0.3185 Epoch 18/30 711/711 [==============================] - 0s 472us/sample - loss: 0.3435 - val_loss: 0.3180 Epoch 19/30 711/711 [==============================] - 0s 468us/sample - loss: 0.3431 - val_loss: 0.3177 Epoch 20/30 711/711 [==============================] - 0s 478us/sample - loss: 0.3426 - val_loss: 0.3175 Epoch 21/30 711/711 [==============================] - 0s 488us/sample - loss: 0.3422 - val_loss: 0.3173 Epoch 22/30 711/711 [==============================] - 0s 489us/sample - loss: 0.3419 - val_loss: 0.3172 Epoch 23/30 711/711 [==============================] - 0s 483us/sample - loss: 0.3415 - val_loss: 0.3172 Epoch 24/30 711/711 [==============================] - 0s 477us/sample - loss: 0.3411 - val_loss: 0.3172 Epoch 25/30 711/711 [==============================] - 0s 476us/sample - loss: 0.3407 - val_loss: 0.3172 Epoch 26/30 711/711 [==============================] - 0s 470us/sample - loss: 0.3403 - val_loss: 0.3173 Epoch 27/30 711/711 [==============================] - 0s 481us/sample - loss: 0.3398 - val_loss: 0.3175 Epoch 28/30 711/711 [==============================] - 0s 477us/sample - loss: 0.3392 - val_loss: 0.3179 Epoch 29/30 711/711 [==============================] - 0s 488us/sample - loss: 0.3386 - val_loss: 0.3186 Epoch 30/30 711/711 [==============================] - 0s 478us/sample - loss: 0.3378 - val_loss: 0.3197 Evaluation plt . figure(figsize = [ 14 , 10 ]) plt . plot(history . history[ 'loss' ], label = 'train' ) plt . plot(history . history[ 'val_loss' ], label = 'test' ) plt . legend(); y_pred = model . predict(X_test) plt . figure(figsize = [ 14 , 10 ]) plt . plot(np . arange( 0 , len (y_train)), y_train, 'g' , label = \"history\" ) plt . plot(np . arange( len (y_train), len (y_train) + len (y_test)), y_test, marker = '.' , label = \"true\" ) plt . plot(np . arange( len (y_train), len (y_train) + len (y_test)), y_pred, 'r' , label = \"prediction\" ) plt . ylabel( 'Value' ) plt . xlabel( 'Time Step' ) plt . legend() plt . show(); plt . figure(figsize = [ 14 , 10 ]) plt . plot(y_test, marker = '.' , label = \"true\" ) plt . plot(y_pred, 'r' , label = \"prediction\" ) plt . ylabel( 'Value' ) plt . xlabel( 'Time Step' ) plt . legend() plt . show();","title":"LSTM-Introduction"},{"location":"lstm/lstm/#lstm-introduction","text":"For classifying MNIST digits.","title":"LSTM-Introduction"},{"location":"lstm/lstm/#intall-tensorflow","text":"#pip install tensorflow==2.0.0-beta0 #pip install --upgrade tensorflow==2.0.0-beta0 import tensorflow as tf from tensorflow import keras # tf.keras import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import os import pandas as pd import sklearn import sys import time sns . set() % matplotlib inline % load_ext tensorboard The tensorboard extension is already loaded. To reload it, use: %reload_ext tensorboard print ( \"python\" , sys . version) for module in mpl, np, pd, sklearn, tf, keras: print (module . __name__ , module . __version__) python 3.7.1 (default, Dec 14 2018, 13:28:58) [Clang 4.0.1 (tags/RELEASE_401/final)] matplotlib 3.0.2 numpy 1.15.4 pandas 0.23.4 sklearn 0.20.1 tensorflow 2.0.0-beta0 tensorflow.python.keras.api._v2.keras 2.2.4-tf","title":"Intall Tensorflow"},{"location":"lstm/lstm/#load-data","text":"time = np . arange( 0 , 100 , 0.1 ) sin = np . sin(time) + np . random . normal(scale = 0.5 , size = len (time)) plt . figure(figsize = [ 14 , 8 ]) plt . plot(time, sin, label = 'sine (with noise)' ); plt . legend(); df = pd . DataFrame( dict (sine = sin), index = time, columns = [ 'sine' ]) df . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sine 0.0 0.699236 0.1 -0.377399 0.2 -0.200619 0.3 0.851897 0.4 0.487359 train_size = int ( len (df) * 0.8 ) test_size = len (df) - train_size train, test = df . iloc[ 0 :train_size], df . iloc[train_size: len (df)] print ( len (train), len (test)) 800 200 def create_dataset (X, y, time_steps = 1 ): Xs, ys = [], [] for i in range ( len (X) - time_steps): v = X . iloc[i:(i + time_steps)] . values Xs . append(v) ys . append(y . iloc[i + time_steps]) return np . array(Xs), np . array(ys) time_steps = 10 # reshape to [samples, time_steps, n_features] X_train, y_train = create_dataset(train, train . sine, time_steps) X_test, y_test = create_dataset(test, test . sine, time_steps) print (X_train . shape, y_train . shape) (790, 10, 1) (790,)","title":"Load data"},{"location":"lstm/lstm/#model","text":"model = keras . models . Sequential([ keras . layers . LSTM(units = 128 ,\\ input_shape = (X_train . shape[ 1 ],X_train . shape[ 2 ])), keras . layers . Dense(units = 1 ) ]) model . compile( loss = 'mean_squared_error' , optimizer = keras . optimizers . Adam( 0.001 ) ) history = model . fit( X_train, y_train, epochs = 30 , batch_size = 16 , validation_split = 0.1 , verbose = 1 , shuffle = False ) Train on 711 samples, validate on 79 samples Epoch 1/30 711/711 [==============================] - 1s 2ms/sample - loss: 0.4392 - val_loss: 0.3535 Epoch 2/30 711/711 [==============================] - 0s 478us/sample - loss: 0.3619 - val_loss: 0.3273 Epoch 3/30 711/711 [==============================] - 0s 482us/sample - loss: 0.3554 - val_loss: 0.3271 Epoch 4/30 711/711 [==============================] - 0s 497us/sample - loss: 0.3540 - val_loss: 0.3273 Epoch 5/30 711/711 [==============================] - 0s 462us/sample - loss: 0.3530 - val_loss: 0.3276 Epoch 6/30 711/711 [==============================] - 0s 460us/sample - loss: 0.3522 - val_loss: 0.3278 Epoch 7/30 711/711 [==============================] - 0s 459us/sample - loss: 0.3516 - val_loss: 0.3280 Epoch 8/30 711/711 [==============================] - 0s 474us/sample - loss: 0.3511 - val_loss: 0.3280 Epoch 9/30 711/711 [==============================] - 0s 470us/sample - loss: 0.3505 - val_loss: 0.3277 Epoch 10/30 711/711 [==============================] - 0s 470us/sample - loss: 0.3498 - val_loss: 0.3271 Epoch 11/30 711/711 [==============================] - 0s 481us/sample - loss: 0.3491 - val_loss: 0.3261 Epoch 12/30 711/711 [==============================] - 0s 468us/sample - loss: 0.3482 - val_loss: 0.3248 Epoch 13/30 711/711 [==============================] - 0s 463us/sample - loss: 0.3473 - val_loss: 0.3233 Epoch 14/30 711/711 [==============================] - 0s 452us/sample - loss: 0.3463 - val_loss: 0.3217 Epoch 15/30 711/711 [==============================] - 0s 464us/sample - loss: 0.3454 - val_loss: 0.3203 Epoch 16/30 711/711 [==============================] - 0s 468us/sample - loss: 0.3447 - val_loss: 0.3192 Epoch 17/30 711/711 [==============================] - 0s 473us/sample - loss: 0.3441 - val_loss: 0.3185 Epoch 18/30 711/711 [==============================] - 0s 472us/sample - loss: 0.3435 - val_loss: 0.3180 Epoch 19/30 711/711 [==============================] - 0s 468us/sample - loss: 0.3431 - val_loss: 0.3177 Epoch 20/30 711/711 [==============================] - 0s 478us/sample - loss: 0.3426 - val_loss: 0.3175 Epoch 21/30 711/711 [==============================] - 0s 488us/sample - loss: 0.3422 - val_loss: 0.3173 Epoch 22/30 711/711 [==============================] - 0s 489us/sample - loss: 0.3419 - val_loss: 0.3172 Epoch 23/30 711/711 [==============================] - 0s 483us/sample - loss: 0.3415 - val_loss: 0.3172 Epoch 24/30 711/711 [==============================] - 0s 477us/sample - loss: 0.3411 - val_loss: 0.3172 Epoch 25/30 711/711 [==============================] - 0s 476us/sample - loss: 0.3407 - val_loss: 0.3172 Epoch 26/30 711/711 [==============================] - 0s 470us/sample - loss: 0.3403 - val_loss: 0.3173 Epoch 27/30 711/711 [==============================] - 0s 481us/sample - loss: 0.3398 - val_loss: 0.3175 Epoch 28/30 711/711 [==============================] - 0s 477us/sample - loss: 0.3392 - val_loss: 0.3179 Epoch 29/30 711/711 [==============================] - 0s 488us/sample - loss: 0.3386 - val_loss: 0.3186 Epoch 30/30 711/711 [==============================] - 0s 478us/sample - loss: 0.3378 - val_loss: 0.3197","title":"Model"},{"location":"lstm/lstm/#evaluation","text":"plt . figure(figsize = [ 14 , 10 ]) plt . plot(history . history[ 'loss' ], label = 'train' ) plt . plot(history . history[ 'val_loss' ], label = 'test' ) plt . legend(); y_pred = model . predict(X_test) plt . figure(figsize = [ 14 , 10 ]) plt . plot(np . arange( 0 , len (y_train)), y_train, 'g' , label = \"history\" ) plt . plot(np . arange( len (y_train), len (y_train) + len (y_test)), y_test, marker = '.' , label = \"true\" ) plt . plot(np . arange( len (y_train), len (y_train) + len (y_test)), y_pred, 'r' , label = \"prediction\" ) plt . ylabel( 'Value' ) plt . xlabel( 'Time Step' ) plt . legend() plt . show(); plt . figure(figsize = [ 14 , 10 ]) plt . plot(y_test, marker = '.' , label = \"true\" ) plt . plot(y_pred, 'r' , label = \"prediction\" ) plt . ylabel( 'Value' ) plt . xlabel( 'Time Step' ) plt . legend() plt . show();","title":"Evaluation"},{"location":"tuning/tuning/","text":"Neural Nets :Tuning % matplotlib inline % load_ext tensorboard import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import os import pandas as pd import sklearn import sys import tensorflow as tf from tensorflow import keras # tf.keras import time import seaborn as sns sns . set() print ( \"python\" , sys . version) for module in mpl, np, pd, sklearn, tf, keras: print (module . __name__ , module . __version__) python 3.7.1 (default, Dec 14 2018, 13:28:58) [Clang 4.0.1 (tags/RELEASE_401/final)] matplotlib 3.0.2 numpy 1.15.4 pandas 0.23.4 sklearn 0.20.1 tensorflow 2.0.0-beta0 tensorflow.python.keras.api._v2.keras 2.2.4-tf assert sys . version_info >= ( 3 , 5 ) # Python \u22653.5 required assert tf . __version__ >= \"2.0\" # TensorFlow \u22652.0 required A neural net for regression Load the California housing dataset using sklearn.datasets.fetch_california_housing . This returns an object with a DESCR attribute describing the dataset, a data attribute with the input features, and a target attribute with the labels. The goal is to predict the price of houses in a district (a census block) given some stats about that district. This is a regression task (predicting values). from sklearn.datasets import fetch_california_housing housing = fetch_california_housing() print (housing . DESCR) .. _california_housing_dataset: California Housing dataset -------------------------- **Data Set Characteristics:** :Number of Instances: 20640 :Number of Attributes: 8 numeric, predictive attributes and the target :Attribute Information: - MedInc median income in block - HouseAge median house age in block - AveRooms average number of rooms - AveBedrms average number of bedrooms - Population block population - AveOccup average house occupancy - Latitude house block latitude - Longitude house block longitude :Missing Attribute Values: None This dataset was obtained from the StatLib repository. http://lib.stat.cmu.edu/datasets/ The target variable is the median house value for California districts. This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). It can be downloaded/loaded using the :func:`sklearn.datasets.fetch_california_housing` function. .. topic:: References - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions, Statistics and Probability Letters, 33 (1997) 291-297 housing . data . shape (20640, 8) housing . target . shape (20640,) Split the dataset into a training set, a validation set and a test set using Scikit-Learn's sklearn.model_selection.train_test_split() function. from sklearn.model_selection import train_test_split X_train_full, X_test, y_train_full, y_test = train_test_split(housing . data, housing . target, random_state = 42 ) X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state = 42 ) len (X_train), len (X_valid), len (X_test) (11610, 3870, 5160) Scale the input features (e.g., using a sklearn.preprocessing.StandardScaler ). Once again, don't forget that you should not fit the validation set or the test set, only the training set. from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train_scaled = scaler . fit_transform(X_train) X_valid_scaled = scaler . transform(X_valid) X_test_scaled = scaler . transform(X_test) Now build, train and evaluate a neural network to tackle this problem. Then use it to make predictions on the test set. Tips : * Since you are predicting a single value per district (the median house price), there should only be one neuron in the output layer. * Usually for regression tasks you don't want to use any activation function in the output layer (in some cases you may want to use \"relu\" or \"softplus\" if you want to constrain the predicted values to be positive, or \"sigmoid\" or \"tanh\" if you want to constrain the predicted values to 0-1 or -1-1). * A good loss function for regression is generally the \"mean_squared_error\" (aka \"mse\" ). When there are many outliers in your dataset, you may prefer to use the \"mean_absolute_error\" (aka \"mae\" ), which is a bit less precise but less sensitive to outliers. model = keras . models . Sequential([ keras . layers . Dense( 30 , activation = \"relu\" ,\\ input_shape = X_train . shape[ 1 :]), keras . layers . Dense( 1 ) ]) model . compile(loss = \"mean_squared_error\" ,\\ optimizer = keras . optimizers . SGD( 1e-3 )) callbacks = [keras . callbacks . EarlyStopping(patience = 10 )] history = model . fit(X_train_scaled,\\ y_train, validation_data = (X_valid_scaled, y_valid),\\ epochs = 100 , callbacks = callbacks) Train on 11610 samples, validate on 3870 samples Epoch 1/100 11610/11610 [==============================] - 0s 41us/sample - loss: 1.4373 - val_loss: 4.9912 Epoch 2/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.8371 - val_loss: 0.8625 Epoch 3/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.7555 - val_loss: 0.7340 Epoch 4/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.7094 - val_loss: 0.7095 Epoch 5/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6720 - val_loss: 0.6471 Epoch 6/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6392 - val_loss: 0.6335 Epoch 7/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6105 - val_loss: 0.5858 Epoch 8/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5852 - val_loss: 0.5641 Epoch 9/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5626 - val_loss: 0.5456 Epoch 10/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5425 - val_loss: 0.5173 Epoch 11/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5253 - val_loss: 0.5070 Epoch 12/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5096 - val_loss: 0.4918 Epoch 13/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4964 - val_loss: 0.4793 Epoch 14/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4848 - val_loss: 0.4752 Epoch 15/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4743 - val_loss: 0.4597 Epoch 16/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4658 - val_loss: 0.4685 Epoch 17/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4586 - val_loss: 0.4433 Epoch 18/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4520 - val_loss: 0.4364 Epoch 19/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4464 - val_loss: 0.4511 Epoch 20/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4418 - val_loss: 0.4285 Epoch 21/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4377 - val_loss: 0.4347 Epoch 22/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4341 - val_loss: 0.4288 Epoch 23/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4309 - val_loss: 0.4265 Epoch 24/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4281 - val_loss: 0.4219 Epoch 25/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4256 - val_loss: 0.4368 Epoch 26/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4233 - val_loss: 0.4404 Epoch 27/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4210 - val_loss: 0.4438 Epoch 28/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4192 - val_loss: 0.4358 Epoch 29/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4172 - val_loss: 0.4398 Epoch 30/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4154 - val_loss: 0.4463 Epoch 31/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4141 - val_loss: 0.4247 Epoch 32/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4126 - val_loss: 0.4308 Epoch 33/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4112 - val_loss: 0.4330 Epoch 34/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4099 - val_loss: 0.4273 model . evaluate(X_test_scaled, y_test) 5160/5160 [==============================] - 0s 15us/sample - loss: 0.4072 0.40716409780258356 model . predict(X_test_scaled) array([[0.6690421], [1.6140628], [3.5537164], ..., [1.504305 ], [2.528902 ], [3.6941016]], dtype=float32) def plot_learning_curves (history): pd . DataFrame(history . history) . plot(figsize = ( 8 , 5 )) plt . grid( True ) plt . gca() . set_ylim( 0 , 1 ) plt . show() plot_learning_curves(history) Hyperparameter search Try training your model multiple times, with different a learning rate each time (e.g., 1e-4, 3e-4, 1e-3, 3e-3, 3e-2), and compare the learning curves. For this, you need to create a keras.optimizers.SGD optimizer and specify the learning_rate in its constructor, then pass this SGD instance to the compile() method using the optimizer argument. learning_rates = [ 1e-4 , 3e-4 , 1e-3 , 3e-3 , 1e-2 , 3e-2 ] histories = [] for learning_rate in learning_rates: model = keras . models . Sequential([ keras . layers . Dense( 30 , activation = \"relu\" , input_shape = X_train . shape[ 1 :]), keras . layers . Dense( 1 ) ]) optimizer = keras . optimizers . SGD(learning_rate) model . compile(loss = \"mean_squared_error\" , optimizer = optimizer) callbacks = [keras . callbacks . EarlyStopping(patience = 10 )] history = model . fit(X_train_scaled, y_train, validation_data = (X_valid_scaled, y_valid), epochs = 100 , callbacks = callbacks) histories . append(history) Train on 11610 samples, validate on 3870 samples Epoch 1/100 11610/11610 [==============================] - 1s 46us/sample - loss: 5.4083 - val_loss: 4.2292 Epoch 2/100 11610/11610 [==============================] - 0s 32us/sample - loss: 3.7043 - val_loss: 3.6743 Epoch 3/100 11610/11610 [==============================] - 0s 33us/sample - loss: 2.6746 - val_loss: 4.1190 Epoch 4/100 11610/11610 [==============================] - 0s 32us/sample - loss: 2.0397 - val_loss: 4.4212 Epoch 5/100 11610/11610 [==============================] - 0s 31us/sample - loss: 1.6337 - val_loss: 4.5097 Epoch 6/100 11610/11610 [==============================] - 0s 31us/sample - loss: 1.3677 - val_loss: 4.2815 Epoch 7/100 11610/11610 [==============================] - 0s 31us/sample - loss: 1.1891 - val_loss: 3.9109 Epoch 8/100 11610/11610 [==============================] - 0s 32us/sample - loss: 1.0654 - val_loss: 3.4672 Epoch 9/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.9781 - val_loss: 2.9750 Epoch 10/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.9146 - val_loss: 2.5285 Epoch 11/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.8676 - val_loss: 2.1245 Epoch 12/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.8319 - val_loss: 1.7892 Epoch 13/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.8042 - val_loss: 1.5134 Epoch 14/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.7823 - val_loss: 1.2938 Epoch 15/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.7646 - val_loss: 1.1226 Epoch 16/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.7500 - val_loss: 0.9907 Epoch 17/100 11610/11610 [==============================] - 0s 36us/sample - loss: 0.7377 - val_loss: 0.8902 Epoch 18/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.7272 - val_loss: 0.8149 Epoch 19/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.7180 - val_loss: 0.7574 Epoch 20/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.7098 - val_loss: 0.7162 Epoch 21/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.7025 - val_loss: 0.6860 Epoch 22/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.6958 - val_loss: 0.6657 Epoch 23/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6897 - val_loss: 0.6500 Epoch 24/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.6839 - val_loss: 0.6394 Epoch 25/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.6786 - val_loss: 0.6314 Epoch 26/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6734 - val_loss: 0.6258 Epoch 27/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.6686 - val_loss: 0.6221 Epoch 28/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.6640 - val_loss: 0.6193 Epoch 29/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.6595 - val_loss: 0.6173 Epoch 30/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6553 - val_loss: 0.6154 Epoch 31/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6512 - val_loss: 0.6134 Epoch 32/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6472 - val_loss: 0.6118 Epoch 33/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6433 - val_loss: 0.6101 Epoch 34/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6396 - val_loss: 0.6085 Epoch 35/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6360 - val_loss: 0.6073 Epoch 36/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6325 - val_loss: 0.6056 Epoch 37/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6290 - val_loss: 0.6041 Epoch 38/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6256 - val_loss: 0.6026 Epoch 39/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6224 - val_loss: 0.6003 Epoch 40/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6192 - val_loss: 0.5986 Epoch 41/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6160 - val_loss: 0.5954 Epoch 42/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6130 - val_loss: 0.5924 Epoch 43/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6100 - val_loss: 0.5901 Epoch 44/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6070 - val_loss: 0.5885 Epoch 45/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6042 - val_loss: 0.5857 Epoch 46/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6013 - val_loss: 0.5834 Epoch 47/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.5986 - val_loss: 0.5801 Epoch 48/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5959 - val_loss: 0.5777 Epoch 49/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5932 - val_loss: 0.5749 Epoch 50/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5906 - val_loss: 0.5728 Epoch 51/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5881 - val_loss: 0.5694 Epoch 52/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5856 - val_loss: 0.5663 Epoch 53/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5832 - val_loss: 0.5633 Epoch 54/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5807 - val_loss: 0.5614 Epoch 55/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5784 - val_loss: 0.5595 Epoch 56/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5761 - val_loss: 0.5578 Epoch 57/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5738 - val_loss: 0.5548 Epoch 58/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5716 - val_loss: 0.5518 Epoch 59/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5694 - val_loss: 0.5495 Epoch 60/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5672 - val_loss: 0.5472 Epoch 61/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5651 - val_loss: 0.5446 Epoch 62/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5630 - val_loss: 0.5427 Epoch 63/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5609 - val_loss: 0.5390 Epoch 64/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5589 - val_loss: 0.5361 Epoch 65/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5569 - val_loss: 0.5333 Epoch 66/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5549 - val_loss: 0.5312 Epoch 67/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5530 - val_loss: 0.5285 Epoch 68/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5511 - val_loss: 0.5271 Epoch 69/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5492 - val_loss: 0.5245 Epoch 70/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5474 - val_loss: 0.5217 Epoch 71/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5456 - val_loss: 0.5199 Epoch 72/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5438 - val_loss: 0.5180 Epoch 73/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5420 - val_loss: 0.5161 Epoch 74/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5403 - val_loss: 0.5140 Epoch 75/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5386 - val_loss: 0.5117 Epoch 76/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5369 - val_loss: 0.5099 Epoch 77/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5353 - val_loss: 0.5079 Epoch 78/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5336 - val_loss: 0.5062 Epoch 79/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5320 - val_loss: 0.5043 Epoch 80/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5305 - val_loss: 0.5026 Epoch 81/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5289 - val_loss: 0.5010 Epoch 82/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5274 - val_loss: 0.4988 Epoch 83/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5259 - val_loss: 0.4967 Epoch 84/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5244 - val_loss: 0.4952 Epoch 85/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5230 - val_loss: 0.4931 Epoch 86/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5215 - val_loss: 0.4911 Epoch 87/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5201 - val_loss: 0.4896 Epoch 88/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5187 - val_loss: 0.4880 Epoch 89/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5173 - val_loss: 0.4864 Epoch 90/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5160 - val_loss: 0.4846 Epoch 91/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5146 - val_loss: 0.4829 Epoch 92/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5133 - val_loss: 0.4816 Epoch 93/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5120 - val_loss: 0.4802 Epoch 94/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5107 - val_loss: 0.4786 Epoch 95/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5095 - val_loss: 0.4772 Epoch 96/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5082 - val_loss: 0.4759 Epoch 97/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5070 - val_loss: 0.4743 Epoch 98/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5058 - val_loss: 0.4728 Epoch 99/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5046 - val_loss: 0.4715 Epoch 100/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5034 - val_loss: 0.4702 Train on 11610 samples, validate on 3870 samples Epoch 1/100 11610/11610 [==============================] - 0s 39us/sample - loss: 4.2273 - val_loss: 2.7904 Epoch 2/100 11610/11610 [==============================] - 0s 30us/sample - loss: 2.0515 - val_loss: 2.1264 Epoch 3/100 11610/11610 [==============================] - 0s 31us/sample - loss: 1.3341 - val_loss: 1.4705 Epoch 4/100 11610/11610 [==============================] - 0s 31us/sample - loss: 1.0088 - val_loss: 1.0015 Epoch 5/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.8489 - val_loss: 0.7896 Epoch 6/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.7662 - val_loss: 0.7104 Epoch 7/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.7200 - val_loss: 0.6770 Epoch 8/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6912 - val_loss: 0.6601 Epoch 9/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6708 - val_loss: 0.6464 Epoch 10/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.6548 - val_loss: 0.6345 Epoch 11/100 11610/11610 [==============================] - 0s 39us/sample - loss: 0.6411 - val_loss: 0.6242 Epoch 12/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6291 - val_loss: 0.6284 Epoch 13/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6183 - val_loss: 0.6130 Epoch 14/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6082 - val_loss: 0.5974 Epoch 15/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5988 - val_loss: 0.6021 Epoch 16/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5903 - val_loss: 0.5907 Epoch 17/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5823 - val_loss: 0.5736 Epoch 18/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5747 - val_loss: 0.5723 Epoch 19/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5676 - val_loss: 0.5652 Epoch 20/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5610 - val_loss: 0.5589 Epoch 21/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5547 - val_loss: 0.5514 Epoch 22/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5487 - val_loss: 0.5474 Epoch 23/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5432 - val_loss: 0.5348 Epoch 24/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5379 - val_loss: 0.5247 Epoch 25/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5329 - val_loss: 0.5162 Epoch 26/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5282 - val_loss: 0.5123 Epoch 27/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5236 - val_loss: 0.5157 Epoch 28/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5194 - val_loss: 0.5115 Epoch 29/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5154 - val_loss: 0.5064 Epoch 30/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5116 - val_loss: 0.4988 Epoch 31/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5079 - val_loss: 0.5025 Epoch 32/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5045 - val_loss: 0.4979 Epoch 33/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5012 - val_loss: 0.4884 Epoch 34/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4981 - val_loss: 0.4813 Epoch 35/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4951 - val_loss: 0.4756 Epoch 36/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4922 - val_loss: 0.4775 Epoch 37/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4895 - val_loss: 0.4719 Epoch 38/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4869 - val_loss: 0.4719 Epoch 39/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4844 - val_loss: 0.4638 Epoch 40/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4821 - val_loss: 0.4610 Epoch 41/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4798 - val_loss: 0.4614 Epoch 42/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4777 - val_loss: 0.4602 Epoch 43/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4756 - val_loss: 0.4555 Epoch 44/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4735 - val_loss: 0.4536 Epoch 45/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4716 - val_loss: 0.4511 Epoch 46/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4697 - val_loss: 0.4465 Epoch 47/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4679 - val_loss: 0.4472 Epoch 48/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.4662 - val_loss: 0.4427 Epoch 49/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4645 - val_loss: 0.4379 Epoch 50/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4629 - val_loss: 0.4380 Epoch 51/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4613 - val_loss: 0.4363 Epoch 52/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4598 - val_loss: 0.4352 Epoch 53/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4583 - val_loss: 0.4353 Epoch 54/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4569 - val_loss: 0.4346 Epoch 55/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4555 - val_loss: 0.4300 Epoch 56/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4541 - val_loss: 0.4291 Epoch 57/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4529 - val_loss: 0.4283 Epoch 58/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4516 - val_loss: 0.4252 Epoch 59/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4503 - val_loss: 0.4226 Epoch 60/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4492 - val_loss: 0.4231 Epoch 61/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4479 - val_loss: 0.4219 Epoch 62/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4468 - val_loss: 0.4204 Epoch 63/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4456 - val_loss: 0.4184 Epoch 64/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4445 - val_loss: 0.4180 Epoch 65/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4434 - val_loss: 0.4167 Epoch 66/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4424 - val_loss: 0.4151 Epoch 67/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4413 - val_loss: 0.4144 Epoch 68/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4403 - val_loss: 0.4138 Epoch 69/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4393 - val_loss: 0.4112 Epoch 70/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4383 - val_loss: 0.4103 Epoch 71/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4373 - val_loss: 0.4097 Epoch 72/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4364 - val_loss: 0.4079 Epoch 73/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4355 - val_loss: 0.4084 Epoch 74/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4345 - val_loss: 0.4061 Epoch 75/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4337 - val_loss: 0.4057 Epoch 76/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4328 - val_loss: 0.4049 Epoch 77/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.4320 - val_loss: 0.4044 Epoch 78/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4311 - val_loss: 0.4028 Epoch 79/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4304 - val_loss: 0.4026 Epoch 80/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4296 - val_loss: 0.4016 Epoch 81/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4288 - val_loss: 0.4008 Epoch 82/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4280 - val_loss: 0.3999 Epoch 83/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4273 - val_loss: 0.3992 Epoch 84/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4265 - val_loss: 0.3986 Epoch 85/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4258 - val_loss: 0.3975 Epoch 86/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4251 - val_loss: 0.3971 Epoch 87/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4244 - val_loss: 0.3967 Epoch 88/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4237 - val_loss: 0.3961 Epoch 89/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4230 - val_loss: 0.3950 Epoch 90/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4224 - val_loss: 0.3948 Epoch 91/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4217 - val_loss: 0.3940 Epoch 92/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4211 - val_loss: 0.3933 Epoch 93/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4204 - val_loss: 0.3941 Epoch 94/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4198 - val_loss: 0.3928 Epoch 95/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4192 - val_loss: 0.3917 Epoch 96/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4186 - val_loss: 0.3913 Epoch 97/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4180 - val_loss: 0.3908 Epoch 98/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4174 - val_loss: 0.3902 Epoch 99/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4169 - val_loss: 0.3894 Epoch 100/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4163 - val_loss: 0.3892 Train on 11610 samples, validate on 3870 samples Epoch 1/100 11610/11610 [==============================] - 0s 38us/sample - loss: 2.4134 - val_loss: 1.3718 Epoch 2/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.7545 - val_loss: 0.6598 Epoch 3/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6376 - val_loss: 0.5935 Epoch 4/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6023 - val_loss: 0.5817 Epoch 5/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5779 - val_loss: 0.5530 Epoch 6/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5579 - val_loss: 0.5253 Epoch 7/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5411 - val_loss: 0.5445 Epoch 8/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5265 - val_loss: 0.5110 Epoch 9/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5136 - val_loss: 0.5042 Epoch 10/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5024 - val_loss: 0.4798 Epoch 11/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4927 - val_loss: 0.4648 Epoch 12/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4840 - val_loss: 0.4726 Epoch 13/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4764 - val_loss: 0.4456 Epoch 14/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4700 - val_loss: 0.4380 Epoch 15/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4644 - val_loss: 0.4392 Epoch 16/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4593 - val_loss: 0.4401 Epoch 17/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4548 - val_loss: 0.4267 Epoch 18/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4508 - val_loss: 0.4354 Epoch 19/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4472 - val_loss: 0.4276 Epoch 20/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4437 - val_loss: 0.4317 Epoch 21/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4407 - val_loss: 0.4150 Epoch 22/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4379 - val_loss: 0.4173 Epoch 23/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4351 - val_loss: 0.4187 Epoch 24/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4328 - val_loss: 0.4153 Epoch 25/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4304 - val_loss: 0.4099 Epoch 26/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4283 - val_loss: 0.4065 Epoch 27/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4261 - val_loss: 0.4083 Epoch 28/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4242 - val_loss: 0.4039 Epoch 29/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4224 - val_loss: 0.3960 Epoch 30/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4207 - val_loss: 0.3992 Epoch 31/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4188 - val_loss: 0.3920 Epoch 32/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4174 - val_loss: 0.4008 Epoch 33/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4159 - val_loss: 0.3919 Epoch 34/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.4143 - val_loss: 0.4006 Epoch 35/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4129 - val_loss: 0.3990 Epoch 36/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4116 - val_loss: 0.3882 Epoch 37/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4104 - val_loss: 0.3860 Epoch 38/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4090 - val_loss: 0.3875 Epoch 39/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4079 - val_loss: 0.3806 Epoch 40/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4068 - val_loss: 0.3814 Epoch 41/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4056 - val_loss: 0.3827 Epoch 42/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4045 - val_loss: 0.3786 Epoch 43/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4035 - val_loss: 0.3812 Epoch 44/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4024 - val_loss: 0.3789 Epoch 45/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4015 - val_loss: 0.3787 Epoch 46/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4005 - val_loss: 0.3805 Epoch 47/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3995 - val_loss: 0.3754 Epoch 48/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3987 - val_loss: 0.3753 Epoch 49/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3977 - val_loss: 0.3772 Epoch 50/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3967 - val_loss: 0.3706 Epoch 51/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3958 - val_loss: 0.3808 Epoch 52/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3950 - val_loss: 0.3865 Epoch 53/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3942 - val_loss: 0.3874 Epoch 54/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3933 - val_loss: 0.3936 Epoch 55/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3927 - val_loss: 0.3897 Epoch 56/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3919 - val_loss: 0.3940 Epoch 57/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3912 - val_loss: 0.3777 Epoch 58/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3905 - val_loss: 0.3831 Epoch 59/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3898 - val_loss: 0.3797 Epoch 60/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3892 - val_loss: 0.3834 Train on 11610 samples, validate on 3870 samples Epoch 1/100 11610/11610 [==============================] - 0s 39us/sample - loss: 1.3072 - val_loss: 5.4533 Epoch 2/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6424 - val_loss: 2.8452 Epoch 3/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5206 - val_loss: 2.5899 Epoch 4/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4927 - val_loss: 6.1177 Epoch 5/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4958 - val_loss: 7.5540 Epoch 6/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4943 - val_loss: 3.3703 Epoch 7/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4485 - val_loss: 0.7394 Epoch 8/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4171 - val_loss: 0.4349 Epoch 9/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4102 - val_loss: 0.4125 Epoch 10/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4055 - val_loss: 0.3867 Epoch 11/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4017 - val_loss: 0.3774 Epoch 12/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3982 - val_loss: 0.3820 Epoch 13/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3944 - val_loss: 0.4447 Epoch 14/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3926 - val_loss: 0.3701 Epoch 15/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3900 - val_loss: 0.3717 Epoch 16/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3876 - val_loss: 0.3667 Epoch 17/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3851 - val_loss: 0.3589 Epoch 18/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3829 - val_loss: 0.3570 Epoch 19/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3807 - val_loss: 0.3977 Epoch 20/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3802 - val_loss: 0.4129 Epoch 21/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3783 - val_loss: 0.4629 Epoch 22/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3772 - val_loss: 0.3535 Epoch 23/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3749 - val_loss: 0.3909 Epoch 24/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3742 - val_loss: 0.3567 Epoch 25/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3726 - val_loss: 0.3504 Epoch 26/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3710 - val_loss: 0.4323 Epoch 27/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3716 - val_loss: 0.3593 Epoch 28/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3697 - val_loss: 0.3478 Epoch 29/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3688 - val_loss: 0.3642 Epoch 30/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3681 - val_loss: 0.3518 Epoch 31/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3669 - val_loss: 0.3702 Epoch 32/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3662 - val_loss: 0.3608 Epoch 33/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3656 - val_loss: 0.3444 Epoch 34/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3649 - val_loss: 0.3676 Epoch 35/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3645 - val_loss: 0.4251 Epoch 36/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3644 - val_loss: 0.6289 Epoch 37/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3664 - val_loss: 0.4964 Epoch 38/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3626 - val_loss: 0.9082 Epoch 39/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3679 - val_loss: 0.7426 Epoch 40/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3645 - val_loss: 0.8972 Epoch 41/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3665 - val_loss: 0.6627 Epoch 42/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3618 - val_loss: 0.8439 Epoch 43/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3624 - val_loss: 0.8183 Train on 11610 samples, validate on 3870 samples Epoch 1/100 11610/11610 [==============================] - 0s 38us/sample - loss: 0.7428 - val_loss: 0.5310 Epoch 2/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4772 - val_loss: 0.7309 Epoch 3/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4394 - val_loss: 0.4341 Epoch 4/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4377 - val_loss: 0.9035 Epoch 5/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4206 - val_loss: 2.1735 Epoch 6/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4003 - val_loss: 3.6124 Epoch 7/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4402 - val_loss: 5.5876 Epoch 8/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4307 - val_loss: 1.7575 Epoch 9/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3921 - val_loss: 0.3940 Epoch 10/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3885 - val_loss: 0.4234 Epoch 11/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3743 - val_loss: 0.4203 Epoch 12/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3730 - val_loss: 0.4551 Epoch 13/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3709 - val_loss: 0.3796 Epoch 14/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3668 - val_loss: 0.3914 Epoch 15/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3675 - val_loss: 0.3904 Epoch 16/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3647 - val_loss: 0.4041 Epoch 17/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3670 - val_loss: 0.4419 Epoch 18/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3622 - val_loss: 0.3666 Epoch 19/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3594 - val_loss: 0.3740 Epoch 20/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3613 - val_loss: 0.3728 Epoch 21/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3586 - val_loss: 0.4143 Epoch 22/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3552 - val_loss: 0.3474 Epoch 23/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3556 - val_loss: 0.3595 Epoch 24/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3694 - val_loss: 0.3725 Epoch 25/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3554 - val_loss: 0.3636 Epoch 26/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3517 - val_loss: 0.3741 Epoch 27/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3532 - val_loss: 0.3486 Epoch 28/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3495 - val_loss: 0.3586 Epoch 29/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3587 - val_loss: 0.3759 Epoch 30/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3484 - val_loss: 0.3318 Epoch 31/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3463 - val_loss: 0.3511 Epoch 32/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3460 - val_loss: 0.3682 Epoch 33/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3453 - val_loss: 0.5032 Epoch 34/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3471 - val_loss: 0.3380 Epoch 35/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3419 - val_loss: 0.3651 Epoch 36/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3407 - val_loss: 0.3318 Epoch 37/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3433 - val_loss: 0.3625 Epoch 38/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3430 - val_loss: 0.3544 Epoch 39/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3370 - val_loss: 0.3249 Epoch 40/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3412 - val_loss: 0.3564 Epoch 41/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3378 - val_loss: 0.3569 Epoch 42/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3343 - val_loss: 0.3182 Epoch 43/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3381 - val_loss: 0.3800 Epoch 44/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3325 - val_loss: 0.3294 Epoch 45/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3336 - val_loss: 0.3423 Epoch 46/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3324 - val_loss: 0.3299 Epoch 47/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3344 - val_loss: 0.3148 Epoch 48/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3298 - val_loss: 0.3498 Epoch 49/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3413 - val_loss: 0.3286 Epoch 50/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3271 - val_loss: 0.5932 Epoch 51/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3313 - val_loss: 0.3845 Epoch 52/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3261 - val_loss: 0.6172 Epoch 53/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3282 - val_loss: 0.6108 Epoch 54/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3302 - val_loss: 0.3378 Epoch 55/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3258 - val_loss: 0.4014 Epoch 56/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3244 - val_loss: 0.3276 Epoch 57/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3222 - val_loss: 0.4102 Train on 11610 samples, validate on 3870 samples Epoch 1/100 11610/11610 [==============================] - 0s 38us/sample - loss: 0.6626 - val_loss: 1.9500 Epoch 2/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4895 - val_loss: 0.4488 Epoch 3/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4463 - val_loss: 4.8228 Epoch 4/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3891 - val_loss: 8.5697 Epoch 5/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4137 - val_loss: 0.3434 Epoch 6/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3676 - val_loss: 0.3476 Epoch 7/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3674 - val_loss: 0.4051 Epoch 8/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4190 - val_loss: 0.3482 Epoch 9/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3666 - val_loss: 0.3420 Epoch 10/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3602 - val_loss: 0.3352 Epoch 11/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3579 - val_loss: 0.3361 Epoch 12/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3557 - val_loss: 0.3494 Epoch 13/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3565 - val_loss: 0.3343 Epoch 14/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3527 - val_loss: 0.3270 Epoch 15/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3491 - val_loss: 0.3259 Epoch 16/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3452 - val_loss: 0.3255 Epoch 17/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3875 - val_loss: 0.3663 Epoch 18/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3572 - val_loss: 0.3324 Epoch 19/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3551 - val_loss: 0.3254 Epoch 20/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3532 - val_loss: 0.3350 Epoch 21/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3404 - val_loss: 0.3183 Epoch 22/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3382 - val_loss: 0.4119 Epoch 23/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3356 - val_loss: 0.3146 Epoch 24/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3334 - val_loss: 0.3138 Epoch 25/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3315 - val_loss: 0.3144 Epoch 26/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3301 - val_loss: 0.3158 Epoch 27/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3263 - val_loss: 0.3053 Epoch 28/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3251 - val_loss: 0.3817 Epoch 29/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3245 - val_loss: 0.3011 Epoch 30/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3224 - val_loss: 0.3064 Epoch 31/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3211 - val_loss: 0.3338 Epoch 32/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3206 - val_loss: 0.2942 Epoch 33/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3185 - val_loss: 0.3047 Epoch 34/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3182 - val_loss: 0.2964 Epoch 35/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3240 - val_loss: 0.2974 Epoch 36/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3160 - val_loss: 0.2945 Epoch 37/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3143 - val_loss: 0.2956 Epoch 38/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3138 - val_loss: 0.2969 Epoch 39/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3122 - val_loss: 0.2938 Epoch 40/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3106 - val_loss: 0.2912 Epoch 41/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3121 - val_loss: 0.3569 Epoch 42/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3137 - val_loss: 0.2975 Epoch 43/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3093 - val_loss: 0.2939 Epoch 44/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3078 - val_loss: 0.2913 Epoch 45/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3082 - val_loss: 0.2980 Epoch 46/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3084 - val_loss: 0.2925 Epoch 47/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3104 - val_loss: 0.2883 Epoch 48/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3075 - val_loss: 0.2887 Epoch 49/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3050 - val_loss: 0.3013 Epoch 50/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3027 - val_loss: 0.3024 Epoch 51/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3049 - val_loss: 0.3493 Epoch 52/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3023 - val_loss: 0.3955 Epoch 53/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3038 - val_loss: 0.2910 Epoch 54/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3037 - val_loss: 0.3976 Epoch 55/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3027 - val_loss: 0.2860 Epoch 56/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3010 - val_loss: 0.2943 Epoch 57/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3011 - val_loss: 0.2864 Epoch 58/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3035 - val_loss: 0.3168 Epoch 59/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3008 - val_loss: 0.2943 Epoch 60/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3017 - val_loss: 0.2994 Epoch 61/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3009 - val_loss: 0.3375 Epoch 62/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3075 - val_loss: 0.3327 Epoch 63/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3022 - val_loss: 0.3012 Epoch 64/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3009 - val_loss: 0.3123 Epoch 65/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3008 - val_loss: 0.2923 for learning_rate, history in zip (learning_rates, histories): print ( \"Learning rate:\" , learning_rate) plot_learning_curves(history) Learning rate: 0.0001 Learning rate: 0.0003 Learning rate: 0.001 Learning rate: 0.003 Learning rate: 0.01 Learning rate: 0.03 Let's look at a more sophisticated way to tune hyperparameters. Create a build_model() function that takes three arguments, n_hidden , n_neurons , learning_rate , and builds, compiles and returns a model with the given number of hidden layers, the given number of neurons and the given learning rate. It is good practice to give a reasonable default value to each argument. def build_model (n_hidden = 1 , n_neurons = 30 , learning_rate = 3e-3 ): model = keras . models . Sequential() options = { \"input_shape\" : X_train . shape[ 1 :]} for layer in range (n_hidden + 1 ): model . add(keras . layers . Dense(n_neurons, activation = \"relu\" , ** options)) options = {} model . add(keras . layers . Dense( 1 , ** options)) optimizer = keras . optimizers . SGD(learning_rate) model . compile(loss = \"mse\" , optimizer = optimizer) return model Create a keras.wrappers.scikit_learn.KerasRegressor and pass the build_model function to the constructor. This gives you a Scikit-Learn compatible predictor. Try training it and using it to make predictions. Note that you can pass the n_epochs , callbacks and validation_data to the fit() method. keras_reg = keras . wrappers . scikit_learn . KerasRegressor(build_model) keras_reg . fit(X_train_scaled, y_train, epochs = 100 , validation_data = (X_valid_scaled, y_valid), callbacks = [keras . callbacks . EarlyStopping(patience = 10 )]) Train on 11610 samples, validate on 3870 samples Epoch 1/100 11610/11610 [==============================] - 0s 41us/sample - loss: 0.9440 - val_loss: 9.4997 Epoch 2/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.6070 - val_loss: 34.8291 Epoch 3/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.6444 - val_loss: 2.0556 Epoch 4/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.4426 - val_loss: 0.4424 Epoch 5/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4072 - val_loss: 0.3828 Epoch 6/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3978 - val_loss: 0.3810 Epoch 7/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3908 - val_loss: 0.3813 Epoch 8/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3859 - val_loss: 0.3877 Epoch 9/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3821 - val_loss: 0.3704 Epoch 10/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3788 - val_loss: 0.3779 Epoch 11/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3761 - val_loss: 0.3823 Epoch 12/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3742 - val_loss: 0.3783 Epoch 13/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3715 - val_loss: 0.3787 Epoch 14/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3689 - val_loss: 0.3870 Epoch 15/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3677 - val_loss: 0.3929 Epoch 16/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.3653 - val_loss: 0.3907 Epoch 17/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3632 - val_loss: 0.3841 Epoch 18/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.3628 - val_loss: 0.3691 Epoch 19/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3606 - val_loss: 0.3888 Epoch 20/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3593 - val_loss: 0.3602 Epoch 21/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3577 - val_loss: 0.3654 Epoch 22/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3569 - val_loss: 0.3606 Epoch 23/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3551 - val_loss: 0.4164 Epoch 24/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3544 - val_loss: 0.3564 Epoch 25/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.3533 - val_loss: 0.3717 Epoch 26/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3522 - val_loss: 0.3663 Epoch 27/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3516 - val_loss: 0.3929 Epoch 28/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3508 - val_loss: 0.3429 Epoch 29/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3493 - val_loss: 0.3521 Epoch 30/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.3488 - val_loss: 0.3937 Epoch 31/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3478 - val_loss: 0.3611 Epoch 32/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3467 - val_loss: 0.3792 Epoch 33/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.3455 - val_loss: 0.3740 Epoch 34/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.3452 - val_loss: 0.3629 Epoch 35/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.3443 - val_loss: 0.3792 Epoch 36/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3432 - val_loss: 0.3710 Epoch 37/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3428 - val_loss: 0.3737 Epoch 38/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3420 - val_loss: 0.3575 <tensorflow.python.keras.callbacks.History at 0x1a3b91d9b0> keras_reg . predict(X_test_scaled) array([0.76681733, 1.8267894 , 4.295369 , ..., 1.3182094 , 2.648156 , 4.0545692 ], dtype=float32) Use a sklearn.model_selection.RandomizedSearchCV to search the hyperparameter space of your KerasRegressor . from scipy.stats import reciprocal param_distribs = { \"n_hidden\" : [ 0 , 1 , 2 , 3 ], \"n_neurons\" : np . arange( 1 , 100 ), \"learning_rate\" : reciprocal( 3e-4 , 3e-2 ), } from sklearn.model_selection import RandomizedSearchCV rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter = 10 , cv = 3 , verbose = 2 ) rnd_search_cv . fit(X_train_scaled, y_train, epochs = 100 , validation_data = (X_valid_scaled, y_valid), callbacks = [keras . callbacks . EarlyStopping(patience = 10 )]) Fitting 3 folds for each of 10 candidates, totalling 30 fits [CV] learning_rate=0.0023302047292153563, n_hidden=2, n_neurons=41 ... [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 61us/sample - loss: 1.6032 - val_loss: 1.7070 Epoch 2/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.6348 - val_loss: 0.5833 Epoch 3/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.5404 - val_loss: 0.4916 Epoch 4/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4899 - val_loss: 0.4363 Epoch 5/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4539 - val_loss: 0.4108 Epoch 6/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4299 - val_loss: 0.4061 Epoch 7/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4147 - val_loss: 0.4094 Epoch 8/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4044 - val_loss: 0.4162 Epoch 9/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3966 - val_loss: 0.4136 Epoch 10/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3907 - val_loss: 0.4069 Epoch 11/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3860 - val_loss: 0.4050 Epoch 12/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3811 - val_loss: 0.4054 Epoch 13/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3777 - val_loss: 0.4089 Epoch 14/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3745 - val_loss: 0.4118 Epoch 15/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3712 - val_loss: 0.4270 Epoch 16/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3682 - val_loss: 0.3820 Epoch 17/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3652 - val_loss: 0.3883 Epoch 18/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3628 - val_loss: 0.4042 Epoch 19/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3605 - val_loss: 0.3741 Epoch 20/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3581 - val_loss: 0.3736 Epoch 21/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3557 - val_loss: 0.3662 Epoch 22/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3535 - val_loss: 0.3517 Epoch 23/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3511 - val_loss: 0.3583 Epoch 24/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3493 - val_loss: 0.3570 Epoch 25/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3477 - val_loss: 0.3631 Epoch 26/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3463 - val_loss: 0.3423 Epoch 27/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3446 - val_loss: 0.3695 Epoch 28/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3420 - val_loss: 0.3397 Epoch 29/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3414 - val_loss: 0.3492 Epoch 30/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3404 - val_loss: 0.3500 Epoch 31/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3385 - val_loss: 0.3407 Epoch 32/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3379 - val_loss: 0.3365 Epoch 33/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3365 - val_loss: 0.3482 Epoch 34/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3353 - val_loss: 0.3435 Epoch 35/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3334 - val_loss: 0.3367 Epoch 36/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3329 - val_loss: 0.3609 Epoch 37/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3318 - val_loss: 0.3485 Epoch 38/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3304 - val_loss: 0.3455 Epoch 39/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3301 - val_loss: 0.3264 Epoch 40/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3291 - val_loss: 0.3259 Epoch 41/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3274 - val_loss: 0.3431 Epoch 42/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3265 - val_loss: 0.3371 Epoch 43/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3258 - val_loss: 0.3332 Epoch 44/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3249 - val_loss: 0.3310 Epoch 45/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3240 - val_loss: 0.3318 Epoch 46/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3236 - val_loss: 0.3268 Epoch 47/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3222 - val_loss: 0.3270 Epoch 48/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3215 - val_loss: 0.3296 Epoch 49/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3204 - val_loss: 0.3274 Epoch 50/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3198 - val_loss: 0.3180 Epoch 51/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3185 - val_loss: 0.3408 Epoch 52/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3177 - val_loss: 0.3154 Epoch 53/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3173 - val_loss: 0.3288 Epoch 54/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3154 - val_loss: 0.3332 Epoch 55/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3148 - val_loss: 0.3334 Epoch 56/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3145 - val_loss: 0.3171 Epoch 57/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3140 - val_loss: 0.3272 Epoch 58/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3129 - val_loss: 0.3203 Epoch 59/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3125 - val_loss: 0.3195 Epoch 60/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3119 - val_loss: 0.3197 Epoch 61/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3101 - val_loss: 0.3084 Epoch 62/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3102 - val_loss: 0.3236 Epoch 63/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3092 - val_loss: 0.3236 Epoch 64/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3089 - val_loss: 0.3137 Epoch 65/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3076 - val_loss: 0.3309 Epoch 66/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3071 - val_loss: 0.3164 Epoch 67/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3066 - val_loss: 0.3116 Epoch 68/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3053 - val_loss: 0.3275 Epoch 69/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3058 - val_loss: 0.3333 Epoch 70/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3058 - val_loss: 0.3057 Epoch 71/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3042 - val_loss: 0.3118 Epoch 72/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3034 - val_loss: 0.3156 Epoch 73/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3023 - val_loss: 0.3021 Epoch 74/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3018 - val_loss: 0.3117 Epoch 75/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3012 - val_loss: 0.3137 Epoch 76/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3009 - val_loss: 0.3042 Epoch 77/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3001 - val_loss: 0.3010 Epoch 78/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2996 - val_loss: 0.2999 Epoch 79/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2981 - val_loss: 0.3223 Epoch 80/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2989 - val_loss: 0.3033 Epoch 81/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.2978 - val_loss: 0.3024 Epoch 82/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2972 - val_loss: 0.2972 Epoch 83/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.2968 - val_loss: 0.3024 Epoch 84/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2964 - val_loss: 0.2988 Epoch 85/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.2953 - val_loss: 0.3088 Epoch 86/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.2944 - val_loss: 0.3030 Epoch 87/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2944 - val_loss: 0.2996 Epoch 88/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.2941 - val_loss: 0.2984 Epoch 89/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.2934 - val_loss: 0.3058 Epoch 90/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.2928 - val_loss: 0.3072 Epoch 91/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.2923 - val_loss: 0.3268 Epoch 92/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.2918 - val_loss: 0.3025 3870/3870 [==============================] - 0s 14us/sample - loss: 0.3291 7740/7740 [==============================] - 0s 16us/sample - loss: 0.2901 [CV] learning_rate=0.0023302047292153563, n_hidden=2, n_neurons=41, total= 27.1s [CV] learning_rate=0.0023302047292153563, n_hidden=2, n_neurons=41 ... [Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 27.3s remaining: 0.0s Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 55us/sample - loss: 1.3620 - val_loss: 1.9705 Epoch 2/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5921 - val_loss: 0.6401 Epoch 3/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.5145 - val_loss: 0.4716 Epoch 4/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4709 - val_loss: 0.4411 Epoch 5/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4432 - val_loss: 0.4216 Epoch 6/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4248 - val_loss: 0.3981 Epoch 7/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4126 - val_loss: 0.3831 Epoch 8/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4020 - val_loss: 0.3851 Epoch 9/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3949 - val_loss: 0.4085 Epoch 10/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3881 - val_loss: 0.4479 Epoch 11/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3839 - val_loss: 0.4876 Epoch 12/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3786 - val_loss: 0.5219 Epoch 13/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3754 - val_loss: 0.6181 Epoch 14/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3716 - val_loss: 0.6809 Epoch 15/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3688 - val_loss: 0.7110 Epoch 16/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3663 - val_loss: 0.7682 Epoch 17/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3642 - val_loss: 0.8752 3870/3870 [==============================] - 0s 14us/sample - loss: 0.3741 7740/7740 [==============================] - 0s 16us/sample - loss: 0.3604 [CV] learning_rate=0.0023302047292153563, n_hidden=2, n_neurons=41, total= 5.5s [CV] learning_rate=0.0023302047292153563, n_hidden=2, n_neurons=41 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 55us/sample - loss: 1.5622 - val_loss: 0.6830 Epoch 2/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.6203 - val_loss: 0.5885 Epoch 3/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5482 - val_loss: 0.5091 Epoch 4/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4988 - val_loss: 0.4652 Epoch 5/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4643 - val_loss: 0.4598 Epoch 6/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4398 - val_loss: 0.4594 Epoch 7/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4229 - val_loss: 0.3934 Epoch 8/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4119 - val_loss: 0.5014 Epoch 9/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4040 - val_loss: 0.4281 Epoch 10/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3975 - val_loss: 0.3707 Epoch 11/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3922 - val_loss: 0.4708 Epoch 12/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3880 - val_loss: 0.4576 Epoch 13/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3850 - val_loss: 0.3639 Epoch 14/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3817 - val_loss: 0.4434 Epoch 15/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3793 - val_loss: 0.3625 Epoch 16/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3756 - val_loss: 0.4428 Epoch 17/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3742 - val_loss: 0.3508 Epoch 18/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3716 - val_loss: 0.3632 Epoch 19/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3692 - val_loss: 0.4190 Epoch 20/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3664 - val_loss: 0.4335 Epoch 21/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3653 - val_loss: 0.3813 Epoch 22/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3628 - val_loss: 0.4380 Epoch 23/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3617 - val_loss: 0.3523 Epoch 24/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3600 - val_loss: 0.3579 Epoch 25/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3582 - val_loss: 0.4265 Epoch 26/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3569 - val_loss: 0.4404 Epoch 27/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3556 - val_loss: 0.3602 3870/3870 [==============================] - 0s 15us/sample - loss: 0.3564 7740/7740 [==============================] - 0s 16us/sample - loss: 0.3544 [CV] learning_rate=0.0023302047292153563, n_hidden=2, n_neurons=41, total= 8.3s [CV] learning_rate=0.015956195942385693, n_hidden=0, n_neurons=97 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 47us/sample - loss: 0.7604 - val_loss: 5.4232 Epoch 2/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.6441 - val_loss: 14.1695 Epoch 3/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.6713 - val_loss: 3.7025 Epoch 4/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.5019 - val_loss: 4.6624 Epoch 5/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.8346 - val_loss: 49.8473 Epoch 6/100 7740/7740 [==============================] - 0s 34us/sample - loss: 3.0755 - val_loss: 0.3787 Epoch 7/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4175 - val_loss: 0.3511 Epoch 8/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3728 - val_loss: 0.3492 Epoch 9/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5112 - val_loss: 0.3382 Epoch 10/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3575 - val_loss: 0.3333 Epoch 11/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4000 - val_loss: 0.3305 Epoch 12/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5751 - val_loss: 1.1609 Epoch 13/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4492 - val_loss: 0.3487 Epoch 14/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3481 - val_loss: 0.3288 Epoch 15/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3440 - val_loss: 0.3263 Epoch 16/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3395 - val_loss: 0.3361 Epoch 17/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3359 - val_loss: 0.3362 Epoch 18/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3359 - val_loss: 0.3213 Epoch 19/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3335 - val_loss: 0.3174 Epoch 20/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3284 - val_loss: 0.3236 Epoch 21/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3275 - val_loss: 0.3134 Epoch 22/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3257 - val_loss: 0.3121 Epoch 23/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3243 - val_loss: 0.3127 Epoch 24/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3218 - val_loss: 0.3214 Epoch 25/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3217 - val_loss: 0.3085 Epoch 26/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3191 - val_loss: 0.3113 Epoch 27/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3189 - val_loss: 0.3051 Epoch 28/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3172 - val_loss: 0.3059 Epoch 29/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3154 - val_loss: 0.3059 Epoch 30/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3146 - val_loss: 0.3134 Epoch 31/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3111 - val_loss: 0.3168 Epoch 32/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3134 - val_loss: 0.3356 Epoch 33/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3130 - val_loss: 0.3226 Epoch 34/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3100 - val_loss: 0.3243 Epoch 35/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3097 - val_loss: 0.3224 Epoch 36/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3088 - val_loss: 0.3283 Epoch 37/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3113 - val_loss: 0.3471 3870/3870 [==============================] - 0s 13us/sample - loss: 0.4386 7740/7740 [==============================] - 0s 14us/sample - loss: 0.3676 [CV] learning_rate=0.015956195942385693, n_hidden=0, n_neurons=97, total= 10.0s [CV] learning_rate=0.015956195942385693, n_hidden=0, n_neurons=97 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 46us/sample - loss: 0.7714 - val_loss: 0.7641 Epoch 2/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4561 - val_loss: 1.2963 Epoch 3/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4122 - val_loss: 0.3778 Epoch 4/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4151 - val_loss: 0.4568 Epoch 5/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3952 - val_loss: 0.7350 Epoch 6/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3759 - val_loss: 0.7615 Epoch 7/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3707 - val_loss: 0.5305 Epoch 8/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3696 - val_loss: 0.3510 Epoch 9/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3634 - val_loss: 0.3410 Epoch 10/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3602 - val_loss: 0.3868 Epoch 11/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3531 - val_loss: 0.8745 Epoch 12/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3574 - val_loss: 0.3859 Epoch 13/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3542 - val_loss: 0.6109 Epoch 14/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3512 - val_loss: 0.4101 Epoch 15/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3496 - val_loss: 0.3618 Epoch 16/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3458 - val_loss: 0.3277 Epoch 17/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3454 - val_loss: 0.4292 Epoch 18/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3431 - val_loss: 0.3737 Epoch 19/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3434 - val_loss: 0.6402 Epoch 20/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3444 - val_loss: 0.3269 Epoch 21/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3388 - val_loss: 0.6092 Epoch 22/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3431 - val_loss: 0.3750 Epoch 23/100 7740/7740 [==============================] - 0s 43us/sample - loss: 0.3417 - val_loss: 0.3615 Epoch 24/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3409 - val_loss: 0.6085 Epoch 25/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3331 - val_loss: 0.4557 Epoch 26/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3308 - val_loss: 0.4441 Epoch 27/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3303 - val_loss: 0.3698 Epoch 28/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3282 - val_loss: 0.3496 Epoch 29/100 7740/7740 [==============================] - 0s 45us/sample - loss: 0.3314 - val_loss: 0.3630 Epoch 30/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3258 - val_loss: 0.3160 Epoch 31/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3255 - val_loss: 0.3887 Epoch 32/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3229 - val_loss: 0.7654 Epoch 33/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3269 - val_loss: 0.3289 Epoch 34/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3192 - val_loss: 0.3337 Epoch 35/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3304 - val_loss: 0.4950 Epoch 36/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3192 - val_loss: 1.9769 Epoch 37/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3214 - val_loss: 0.6508 Epoch 38/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3190 - val_loss: 0.3250 Epoch 39/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3191 - val_loss: 0.7641 Epoch 40/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3161 - val_loss: 0.4083 3870/3870 [==============================] - 0s 13us/sample - loss: 0.3752 7740/7740 [==============================] - 0s 14us/sample - loss: 0.3483 [CV] learning_rate=0.015956195942385693, n_hidden=0, n_neurons=97, total= 11.3s [CV] learning_rate=0.015956195942385693, n_hidden=0, n_neurons=97 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 46us/sample - loss: 0.7449 - val_loss: 1.8796 Epoch 2/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4798 - val_loss: 0.5772 Epoch 3/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4285 - val_loss: 0.3831 Epoch 4/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4161 - val_loss: 1.4244 Epoch 5/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4231 - val_loss: 18.9345 Epoch 6/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3970 - val_loss: 6.8569 Epoch 7/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.6343 - val_loss: 3.4721 Epoch 8/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4484 - val_loss: 103.4562 Epoch 9/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5382 - val_loss: 0.3930 Epoch 10/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3965 - val_loss: 0.3660 Epoch 11/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4080 - val_loss: 0.3647 Epoch 12/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3822 - val_loss: 0.3534 Epoch 13/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3735 - val_loss: 0.3537 Epoch 14/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3672 - val_loss: 0.3453 Epoch 15/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3628 - val_loss: 0.3444 Epoch 16/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3766 - val_loss: 0.3401 Epoch 17/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3711 - val_loss: 0.3460 Epoch 18/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3623 - val_loss: 0.3325 Epoch 19/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3570 - val_loss: 0.3644 Epoch 20/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3536 - val_loss: 0.3517 Epoch 21/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3496 - val_loss: 0.3682 Epoch 22/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3599 - val_loss: 0.3256 Epoch 23/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3469 - val_loss: 0.3334 Epoch 24/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3439 - val_loss: 0.3432 Epoch 25/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3469 - val_loss: 0.3219 Epoch 26/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3416 - val_loss: 0.3368 Epoch 27/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3394 - val_loss: 0.3201 Epoch 28/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3370 - val_loss: 0.3207 Epoch 29/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3346 - val_loss: 0.3202 Epoch 30/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3315 - val_loss: 0.3141 Epoch 31/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3335 - val_loss: 0.3171 Epoch 32/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3306 - val_loss: 0.3172 Epoch 33/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3389 - val_loss: 0.3142 Epoch 34/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3332 - val_loss: 0.3638 Epoch 35/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3315 - val_loss: 0.3186 Epoch 36/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3256 - val_loss: 0.3102 Epoch 37/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3233 - val_loss: 0.3238 Epoch 38/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3291 - val_loss: 0.3299 Epoch 39/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3339 - val_loss: 0.3433 Epoch 40/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3250 - val_loss: 0.3063 Epoch 41/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3228 - val_loss: 0.3480 Epoch 42/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3186 - val_loss: 0.3669 Epoch 43/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3251 - val_loss: 0.3058 Epoch 44/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3247 - val_loss: 0.3364 Epoch 45/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3180 - val_loss: 0.3104 Epoch 46/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3232 - val_loss: 0.3273 Epoch 47/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3166 - val_loss: 0.3027 Epoch 48/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3136 - val_loss: 0.3203 Epoch 49/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3131 - val_loss: 0.3446 Epoch 50/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3147 - val_loss: 0.3076 Epoch 51/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3226 - val_loss: 0.3362 Epoch 52/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3121 - val_loss: 0.3097 Epoch 53/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3116 - val_loss: 0.4066 Epoch 54/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3176 - val_loss: 0.3047 Epoch 55/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3096 - val_loss: 0.3028 Epoch 56/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3083 - val_loss: 0.3614 Epoch 57/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3074 - val_loss: 0.3032 3870/3870 [==============================] - 0s 13us/sample - loss: 0.3190 7740/7740 [==============================] - 0s 14us/sample - loss: 0.3051 [CV] learning_rate=0.015956195942385693, n_hidden=0, n_neurons=97, total= 15.3s [CV] learning_rate=0.0017196854145436866, n_hidden=2, n_neurons=77 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 56us/sample - loss: 1.5431 - val_loss: 2.1386 Epoch 2/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.6880 - val_loss: 0.5965 Epoch 3/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.5931 - val_loss: 0.5344 Epoch 4/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.5405 - val_loss: 0.5002 Epoch 5/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4996 - val_loss: 0.5025 Epoch 6/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4679 - val_loss: 0.4393 Epoch 7/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4435 - val_loss: 0.4388 Epoch 8/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4256 - val_loss: 0.3981 Epoch 9/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4115 - val_loss: 0.4108 Epoch 10/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4016 - val_loss: 0.4239 Epoch 11/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3936 - val_loss: 0.4047 Epoch 12/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3870 - val_loss: 0.4046 Epoch 13/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3808 - val_loss: 0.3774 Epoch 14/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3757 - val_loss: 0.4053 Epoch 15/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3706 - val_loss: 0.3574 Epoch 16/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3669 - val_loss: 0.3605 Epoch 17/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3628 - val_loss: 0.4119 Epoch 18/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3604 - val_loss: 0.4028 Epoch 19/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3572 - val_loss: 0.3772 Epoch 20/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3547 - val_loss: 0.3834 Epoch 21/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3522 - val_loss: 0.3464 Epoch 22/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3494 - val_loss: 0.3959 Epoch 23/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3477 - val_loss: 0.3378 Epoch 24/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3456 - val_loss: 0.4059 Epoch 25/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3446 - val_loss: 0.3477 Epoch 26/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3420 - val_loss: 0.3920 Epoch 27/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3410 - val_loss: 0.3325 Epoch 28/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3389 - val_loss: 0.3782 Epoch 29/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3373 - val_loss: 0.3309 Epoch 30/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3366 - val_loss: 0.3642 Epoch 31/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3350 - val_loss: 0.3590 Epoch 32/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3333 - val_loss: 0.3305 Epoch 33/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3319 - val_loss: 0.3564 Epoch 34/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3305 - val_loss: 0.3445 Epoch 35/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3300 - val_loss: 0.3723 Epoch 36/100 7740/7740 [==============================] - 0s 42us/sample - loss: 0.3287 - val_loss: 0.3593 Epoch 37/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3278 - val_loss: 0.3276 Epoch 38/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3272 - val_loss: 0.4318 Epoch 39/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3264 - val_loss: 0.3511 Epoch 40/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3252 - val_loss: 0.3221 Epoch 41/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3240 - val_loss: 0.3791 Epoch 42/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3236 - val_loss: 0.3305 Epoch 43/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3225 - val_loss: 0.3361 Epoch 44/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3210 - val_loss: 0.3781 Epoch 45/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3205 - val_loss: 0.3291 Epoch 46/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3197 - val_loss: 0.3419 Epoch 47/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3189 - val_loss: 0.3624 Epoch 48/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3179 - val_loss: 0.3237 Epoch 49/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3168 - val_loss: 0.3165 Epoch 50/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3160 - val_loss: 0.3233 Epoch 51/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3148 - val_loss: 0.4150 Epoch 52/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3152 - val_loss: 0.3100 Epoch 53/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3138 - val_loss: 0.4137 Epoch 54/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3140 - val_loss: 0.3866 Epoch 55/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3125 - val_loss: 0.3258 Epoch 56/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3112 - val_loss: 0.3279 Epoch 57/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3109 - val_loss: 0.4006 Epoch 58/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3111 - val_loss: 0.3077 Epoch 59/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3094 - val_loss: 0.3585 Epoch 60/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3097 - val_loss: 0.3600 Epoch 61/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3089 - val_loss: 0.3933 Epoch 62/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3080 - val_loss: 0.3050 Epoch 63/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3066 - val_loss: 0.3039 Epoch 64/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3057 - val_loss: 0.3046 Epoch 65/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3053 - val_loss: 0.3040 Epoch 66/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3047 - val_loss: 0.3039 Epoch 67/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3036 - val_loss: 0.4107 Epoch 68/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3036 - val_loss: 0.3624 Epoch 69/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3031 - val_loss: 0.4021 Epoch 70/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3026 - val_loss: 0.3481 Epoch 71/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3021 - val_loss: 0.5111 Epoch 72/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3034 - val_loss: 0.3270 Epoch 73/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2998 - val_loss: 0.4186 3870/3870 [==============================] - 0s 15us/sample - loss: 0.3326 7740/7740 [==============================] - 0s 17us/sample - loss: 0.2985 [CV] learning_rate=0.0017196854145436866, n_hidden=2, n_neurons=77, total= 22.5s [CV] learning_rate=0.0017196854145436866, n_hidden=2, n_neurons=77 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 56us/sample - loss: 1.6301 - val_loss: 4.2039 Epoch 2/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.6970 - val_loss: 0.6768 Epoch 3/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.6090 - val_loss: 0.8804 Epoch 4/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.5529 - val_loss: 1.6374 Epoch 5/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.5127 - val_loss: 2.1626 Epoch 6/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4821 - val_loss: 2.8482 Epoch 7/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4595 - val_loss: 2.3233 Epoch 8/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4420 - val_loss: 2.1937 Epoch 9/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4277 - val_loss: 2.1441 Epoch 10/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.4166 - val_loss: 1.2412 Epoch 11/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4067 - val_loss: 1.0166 Epoch 12/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3987 - val_loss: 0.7848 3870/3870 [==============================] - 0s 15us/sample - loss: 0.4227 7740/7740 [==============================] - 0s 17us/sample - loss: 0.3932 [CV] learning_rate=0.0017196854145436866, n_hidden=2, n_neurons=77, total= 4.1s [CV] learning_rate=0.0017196854145436866, n_hidden=2, n_neurons=77 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 56us/sample - loss: 1.5410 - val_loss: 1.7610 Epoch 2/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.6859 - val_loss: 0.6030 Epoch 3/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.6079 - val_loss: 0.5475 Epoch 4/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.5578 - val_loss: 0.5058 Epoch 5/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.5189 - val_loss: 0.4733 Epoch 6/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4871 - val_loss: 0.4425 Epoch 7/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4617 - val_loss: 0.4225 Epoch 8/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4422 - val_loss: 0.4064 Epoch 9/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4270 - val_loss: 0.4141 Epoch 10/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4156 - val_loss: 0.4001 Epoch 11/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4056 - val_loss: 0.4111 Epoch 12/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3978 - val_loss: 0.3833 Epoch 13/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3928 - val_loss: 0.4107 Epoch 14/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3881 - val_loss: 0.3695 Epoch 15/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3824 - val_loss: 0.4339 Epoch 16/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3784 - val_loss: 0.4015 Epoch 17/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3751 - val_loss: 0.3944 Epoch 18/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3715 - val_loss: 0.3888 Epoch 19/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3681 - val_loss: 0.4058 Epoch 20/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3654 - val_loss: 0.3471 Epoch 21/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3638 - val_loss: 0.3469 Epoch 22/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3600 - val_loss: 0.3725 Epoch 23/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3581 - val_loss: 0.4448 Epoch 24/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3571 - val_loss: 0.3868 Epoch 25/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3549 - val_loss: 0.3871 Epoch 26/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3532 - val_loss: 0.3699 Epoch 27/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3510 - val_loss: 0.3456 Epoch 28/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3492 - val_loss: 0.4108 Epoch 29/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3481 - val_loss: 0.3753 Epoch 30/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3461 - val_loss: 0.3960 Epoch 31/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3445 - val_loss: 0.3945 Epoch 32/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3434 - val_loss: 0.3682 Epoch 33/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3419 - val_loss: 0.3298 Epoch 34/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3411 - val_loss: 0.3609 Epoch 35/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3398 - val_loss: 0.3399 Epoch 36/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3385 - val_loss: 0.3297 Epoch 37/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3368 - val_loss: 0.3665 Epoch 38/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3364 - val_loss: 0.3196 Epoch 39/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3353 - val_loss: 0.4043 Epoch 40/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3344 - val_loss: 0.3602 Epoch 41/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3329 - val_loss: 0.3659 Epoch 42/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3321 - val_loss: 0.3848 Epoch 43/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3310 - val_loss: 0.3145 Epoch 44/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3296 - val_loss: 0.3836 Epoch 45/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3294 - val_loss: 0.3203 Epoch 46/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3272 - val_loss: 0.3455 Epoch 47/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3271 - val_loss: 0.3557 Epoch 48/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3262 - val_loss: 0.3176 Epoch 49/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3252 - val_loss: 0.3626 Epoch 50/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3246 - val_loss: 0.3258 Epoch 51/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3235 - val_loss: 0.3191 Epoch 52/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3231 - val_loss: 0.3160 Epoch 53/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3216 - val_loss: 0.3293 3870/3870 [==============================] - 0s 15us/sample - loss: 0.3243 7740/7740 [==============================] - 0s 16us/sample - loss: 0.3189 [CV] learning_rate=0.0017196854145436866, n_hidden=2, n_neurons=77, total= 16.3s [CV] learning_rate=0.003284284607688981, n_hidden=2, n_neurons=75 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 56us/sample - loss: 1.4842 - val_loss: 2.1657 Epoch 2/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.5991 - val_loss: 1.9354 Epoch 3/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.5146 - val_loss: 0.6279 Epoch 4/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4586 - val_loss: 0.5010 Epoch 5/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4262 - val_loss: 0.3873 Epoch 6/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4064 - val_loss: 0.3757 Epoch 7/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3914 - val_loss: 0.3797 Epoch 8/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3825 - val_loss: 0.3684 Epoch 9/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3742 - val_loss: 0.3503 Epoch 10/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3679 - val_loss: 0.3745 Epoch 11/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3622 - val_loss: 0.3999 Epoch 12/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3582 - val_loss: 0.3889 Epoch 13/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3541 - val_loss: 0.3470 Epoch 14/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3500 - val_loss: 0.4274 Epoch 15/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3486 - val_loss: 0.3568 Epoch 16/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3443 - val_loss: 0.4425 Epoch 17/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3429 - val_loss: 0.3352 Epoch 18/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3390 - val_loss: 0.3316 Epoch 19/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3365 - val_loss: 0.4038 Epoch 20/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3357 - val_loss: 0.3435 Epoch 21/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3330 - val_loss: 0.3226 Epoch 22/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3316 - val_loss: 0.3796 Epoch 23/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3293 - val_loss: 0.3240 Epoch 24/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3274 - val_loss: 0.3713 Epoch 25/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3268 - val_loss: 0.3210 Epoch 26/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3225 - val_loss: 0.3229 Epoch 27/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3219 - val_loss: 0.3197 Epoch 28/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3205 - val_loss: 0.3354 Epoch 29/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3195 - val_loss: 0.3137 Epoch 30/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3180 - val_loss: 0.3713 Epoch 31/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3170 - val_loss: 0.3105 Epoch 32/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3153 - val_loss: 0.3809 Epoch 33/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3137 - val_loss: 0.3351 Epoch 34/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3127 - val_loss: 0.3326 Epoch 35/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3109 - val_loss: 0.3667 Epoch 36/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3089 - val_loss: 0.3167 Epoch 37/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3073 - val_loss: 0.3544 Epoch 38/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3082 - val_loss: 0.3108 Epoch 39/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3065 - val_loss: 0.3324 Epoch 40/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3063 - val_loss: 0.3016 Epoch 41/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3038 - val_loss: 0.3123 Epoch 42/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3025 - val_loss: 0.3372 Epoch 43/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3027 - val_loss: 0.3443 Epoch 44/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3021 - val_loss: 0.3409 Epoch 45/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3002 - val_loss: 0.3001 Epoch 46/100 7740/7740 [==============================] - 0s 43us/sample - loss: 0.2992 - val_loss: 0.3370 Epoch 47/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2981 - val_loss: 0.3304 Epoch 48/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2979 - val_loss: 0.3872 Epoch 49/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2984 - val_loss: 0.3408 Epoch 50/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2955 - val_loss: 0.3122 Epoch 51/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2942 - val_loss: 0.3237 Epoch 52/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2932 - val_loss: 0.3119 Epoch 53/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2924 - val_loss: 0.3206 Epoch 54/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2910 - val_loss: 0.4156 Epoch 55/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2920 - val_loss: 0.3537 3870/3870 [==============================] - 0s 15us/sample - loss: 0.3294 7740/7740 [==============================] - 0s 17us/sample - loss: 0.2875 [CV] learning_rate=0.003284284607688981, n_hidden=2, n_neurons=75, total= 17.3s [CV] learning_rate=0.003284284607688981, n_hidden=2, n_neurons=75 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 56us/sample - loss: 1.0847 - val_loss: 5.8122 Epoch 2/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.6135 - val_loss: 0.7386 Epoch 3/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.5218 - val_loss: 0.6025 Epoch 4/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4632 - val_loss: 0.7112 Epoch 5/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4265 - val_loss: 0.5877 Epoch 6/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4039 - val_loss: 0.4176 Epoch 7/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3898 - val_loss: 0.3684 Epoch 8/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3788 - val_loss: 0.4108 Epoch 9/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3707 - val_loss: 0.4843 Epoch 10/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3641 - val_loss: 0.6541 Epoch 11/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3576 - val_loss: 0.7952 Epoch 12/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3545 - val_loss: 0.8378 Epoch 13/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3505 - val_loss: 0.8978 Epoch 14/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3467 - val_loss: 0.8281 Epoch 15/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3450 - val_loss: 0.9674 Epoch 16/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3422 - val_loss: 0.9459 Epoch 17/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3400 - val_loss: 0.8946 3870/3870 [==============================] - 0s 15us/sample - loss: 0.3573 7740/7740 [==============================] - 0s 16us/sample - loss: 0.3355 [CV] learning_rate=0.003284284607688981, n_hidden=2, n_neurons=75, total= 5.7s [CV] learning_rate=0.003284284607688981, n_hidden=2, n_neurons=75 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 56us/sample - loss: 1.3023 - val_loss: 2.6927 Epoch 2/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.5547 - val_loss: 0.5447 Epoch 3/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4631 - val_loss: 0.4596 Epoch 4/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4296 - val_loss: 0.4099 Epoch 5/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4112 - val_loss: 0.4182 Epoch 6/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3992 - val_loss: 0.4050 Epoch 7/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3907 - val_loss: 0.3833 Epoch 8/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3824 - val_loss: 0.4306 Epoch 9/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3773 - val_loss: 0.3806 Epoch 10/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3720 - val_loss: 0.3851 Epoch 11/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3677 - val_loss: 0.3504 Epoch 12/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3632 - val_loss: 0.3865 Epoch 13/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3596 - val_loss: 0.4215 Epoch 14/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3577 - val_loss: 0.3906 Epoch 15/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3538 - val_loss: 0.4588 Epoch 16/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3525 - val_loss: 0.3350 Epoch 17/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3488 - val_loss: 0.4020 Epoch 18/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3459 - val_loss: 0.4451 Epoch 19/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3445 - val_loss: 0.4082 Epoch 20/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3429 - val_loss: 0.3690 Epoch 21/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3413 - val_loss: 0.3326 Epoch 22/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3389 - val_loss: 0.3475 Epoch 23/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3376 - val_loss: 0.3381 Epoch 24/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3352 - val_loss: 0.3333 Epoch 25/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3340 - val_loss: 0.4147 Epoch 26/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3325 - val_loss: 0.4207 Epoch 27/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3319 - val_loss: 0.3328 Epoch 28/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3302 - val_loss: 0.3656 Epoch 29/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3288 - val_loss: 0.3180 Epoch 30/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3266 - val_loss: 0.3994 Epoch 31/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3269 - val_loss: 0.3344 Epoch 32/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3252 - val_loss: 0.3281 Epoch 33/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3226 - val_loss: 0.3339 Epoch 34/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3233 - val_loss: 0.3283 Epoch 35/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3208 - val_loss: 0.3834 Epoch 36/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3209 - val_loss: 0.3260 Epoch 37/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3187 - val_loss: 0.3149 Epoch 38/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3175 - val_loss: 0.3136 Epoch 39/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3159 - val_loss: 0.4119 Epoch 40/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3158 - val_loss: 0.3252 Epoch 41/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3144 - val_loss: 0.3174 Epoch 42/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3129 - val_loss: 0.3911 Epoch 43/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3131 - val_loss: 0.3167 Epoch 44/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3117 - val_loss: 0.4159 Epoch 45/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3113 - val_loss: 0.3414 Epoch 46/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3078 - val_loss: 0.3597 Epoch 47/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3089 - val_loss: 0.3062 Epoch 48/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3074 - val_loss: 0.3133 Epoch 49/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3052 - val_loss: 0.3587 Epoch 50/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3060 - val_loss: 0.3166 Epoch 51/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3038 - val_loss: 0.3144 Epoch 52/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3038 - val_loss: 0.3803 Epoch 53/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3024 - val_loss: 0.3023 Epoch 54/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3012 - val_loss: 0.3206 Epoch 55/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3006 - val_loss: 0.3453 Epoch 56/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3008 - val_loss: 0.3081 Epoch 57/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2989 - val_loss: 0.3795 Epoch 58/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2982 - val_loss: 0.3208 Epoch 59/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2973 - val_loss: 0.3284 Epoch 60/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2957 - val_loss: 0.3058 Epoch 61/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2966 - val_loss: 0.3155 Epoch 62/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2947 - val_loss: 0.3156 Epoch 63/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2948 - val_loss: 0.3417 3870/3870 [==============================] - 0s 15us/sample - loss: 0.3149 7740/7740 [==============================] - 0s 16us/sample - loss: 0.2935 [CV] learning_rate=0.003284284607688981, n_hidden=2, n_neurons=75, total= 19.2s [CV] learning_rate=0.004038022680351922, n_hidden=2, n_neurons=69 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 56us/sample - loss: 1.1144 - val_loss: 1.6181 Epoch 2/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.5747 - val_loss: 1.2753 Epoch 3/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4971 - val_loss: 0.5712 Epoch 4/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4514 - val_loss: 0.5843 Epoch 5/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4202 - val_loss: 0.3850 Epoch 6/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4007 - val_loss: 0.3774 Epoch 7/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3869 - val_loss: 0.4252 Epoch 8/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3780 - val_loss: 0.3768 Epoch 9/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3698 - val_loss: 0.3744 Epoch 10/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3627 - val_loss: 0.3604 Epoch 11/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3577 - val_loss: 0.4952 Epoch 12/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3563 - val_loss: 0.4150 Epoch 13/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3503 - val_loss: 0.3910 Epoch 14/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3469 - val_loss: 0.3373 Epoch 15/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3428 - val_loss: 0.3744 Epoch 16/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3414 - val_loss: 0.3325 Epoch 17/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3371 - val_loss: 0.4917 Epoch 18/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3410 - val_loss: 0.6252 Epoch 19/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3394 - val_loss: 1.0122 Epoch 20/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3407 - val_loss: 0.9887 Epoch 21/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3384 - val_loss: 0.7173 Epoch 22/100 7740/7740 [==============================] - 0s 43us/sample - loss: 0.3325 - val_loss: 0.3328 Epoch 23/100 7740/7740 [==============================] - 0s 45us/sample - loss: 0.3257 - val_loss: 0.3620 Epoch 24/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3255 - val_loss: 0.3204 Epoch 25/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3224 - val_loss: 0.3355 Epoch 26/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3205 - val_loss: 0.3574 Epoch 27/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3197 - val_loss: 0.3340 Epoch 28/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3172 - val_loss: 0.3134 Epoch 29/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3155 - val_loss: 0.3252 Epoch 30/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3151 - val_loss: 0.3228 Epoch 31/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3131 - val_loss: 0.3122 Epoch 32/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3109 - val_loss: 0.3072 Epoch 33/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3095 - val_loss: 0.4399 Epoch 34/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3111 - val_loss: 0.5012 Epoch 35/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3116 - val_loss: 0.4788 Epoch 36/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3081 - val_loss: 0.3058 Epoch 37/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3061 - val_loss: 0.3707 Epoch 38/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3038 - val_loss: 0.3291 Epoch 39/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3021 - val_loss: 0.3020 Epoch 40/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3014 - val_loss: 0.3682 Epoch 41/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3014 - val_loss: 0.3114 Epoch 42/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2999 - val_loss: 0.3104 Epoch 43/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2986 - val_loss: 0.3037 Epoch 44/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2966 - val_loss: 0.3074 Epoch 45/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2953 - val_loss: 0.3298 Epoch 46/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2956 - val_loss: 0.4312 Epoch 47/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2953 - val_loss: 0.4607 Epoch 48/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2959 - val_loss: 0.3917 Epoch 49/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2955 - val_loss: 0.3009 Epoch 50/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2912 - val_loss: 0.3113 Epoch 51/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2909 - val_loss: 0.3125 Epoch 52/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2903 - val_loss: 0.3306 Epoch 53/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2884 - val_loss: 0.2966 Epoch 54/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2876 - val_loss: 0.3536 Epoch 55/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2877 - val_loss: 0.4683 Epoch 56/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2902 - val_loss: 0.4054 Epoch 57/100 7740/7740 [==============================] - 0s 42us/sample - loss: 0.2866 - val_loss: 0.3190 Epoch 58/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.2854 - val_loss: 0.3752 Epoch 59/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2839 - val_loss: 0.2895 Epoch 60/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2828 - val_loss: 0.3034 Epoch 61/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2829 - val_loss: 0.3833 Epoch 62/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2839 - val_loss: 0.2852 Epoch 63/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2806 - val_loss: 0.2916 Epoch 64/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2787 - val_loss: 0.2897 Epoch 65/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2794 - val_loss: 0.2838 Epoch 66/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2785 - val_loss: 0.3772 Epoch 67/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2787 - val_loss: 0.3277 Epoch 68/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2771 - val_loss: 0.5036 Epoch 69/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2791 - val_loss: 0.3969 Epoch 70/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2765 - val_loss: 0.6491 Epoch 71/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2857 - val_loss: 0.6069 Epoch 72/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2822 - val_loss: 0.8506 Epoch 73/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2840 - val_loss: 0.4954 Epoch 74/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2785 - val_loss: 0.4551 Epoch 75/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2754 - val_loss: 0.2963 3870/3870 [==============================] - 0s 15us/sample - loss: 0.3199 7740/7740 [==============================] - 0s 16us/sample - loss: 0.2753 [CV] learning_rate=0.004038022680351922, n_hidden=2, n_neurons=69, total= 23.2s [CV] learning_rate=0.004038022680351922, n_hidden=2, n_neurons=69 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 55us/sample - loss: 1.1371 - val_loss: 1.3452 Epoch 2/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.5859 - val_loss: 0.5920 Epoch 3/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.5003 - val_loss: 0.4443 Epoch 4/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4476 - val_loss: 0.4072 Epoch 5/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4162 - val_loss: 0.3839 Epoch 6/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3974 - val_loss: 0.3671 Epoch 7/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3837 - val_loss: 0.3693 Epoch 8/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3740 - val_loss: 0.3850 Epoch 9/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3688 - val_loss: 0.4103 Epoch 10/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3634 - val_loss: 0.4479 Epoch 11/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3587 - val_loss: 0.5044 Epoch 12/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3554 - val_loss: 0.5236 Epoch 13/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3508 - val_loss: 0.5970 Epoch 14/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3474 - val_loss: 0.6589 Epoch 15/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3454 - val_loss: 0.6735 Epoch 16/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3421 - val_loss: 0.7284 3870/3870 [==============================] - 0s 15us/sample - loss: 0.3575 7740/7740 [==============================] - 0s 16us/sample - loss: 0.3373 [CV] learning_rate=0.004038022680351922, n_hidden=2, n_neurons=69, total= 5.2s [CV] learning_rate=0.004038022680351922, n_hidden=2, n_neurons=69 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 56us/sample - loss: 1.2970 - val_loss: 7.5366 Epoch 2/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.6151 - val_loss: 2.4204 Epoch 3/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.5010 - val_loss: 0.4373 Epoch 4/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4434 - val_loss: 0.4774 Epoch 5/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4185 - val_loss: 0.4510 Epoch 6/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4012 - val_loss: 0.4292 Epoch 7/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3888 - val_loss: 0.3930 Epoch 8/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3802 - val_loss: 0.4631 Epoch 9/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3740 - val_loss: 0.3844 Epoch 10/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3680 - val_loss: 0.4246 Epoch 11/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3648 - val_loss: 0.4520 Epoch 12/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3609 - val_loss: 0.4205 Epoch 13/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3570 - val_loss: 0.3746 Epoch 14/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3529 - val_loss: 0.3670 Epoch 15/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3511 - val_loss: 0.3522 Epoch 16/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3471 - val_loss: 0.4065 Epoch 17/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3453 - val_loss: 0.3991 Epoch 18/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3426 - val_loss: 0.3844 Epoch 19/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3402 - val_loss: 0.3834 Epoch 20/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3375 - val_loss: 0.4403 Epoch 21/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3362 - val_loss: 0.3458 Epoch 22/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3335 - val_loss: 0.3672 Epoch 23/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3317 - val_loss: 0.3860 Epoch 24/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3304 - val_loss: 0.3250 Epoch 25/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3278 - val_loss: 0.3844 Epoch 26/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3265 - val_loss: 0.4097 Epoch 27/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3250 - val_loss: 0.3139 Epoch 28/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3238 - val_loss: 0.3452 Epoch 29/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3215 - val_loss: 0.3300 Epoch 30/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3198 - val_loss: 0.3281 Epoch 31/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3173 - val_loss: 0.3994 Epoch 32/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3172 - val_loss: 0.3340 Epoch 33/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3153 - val_loss: 0.3991 Epoch 34/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3152 - val_loss: 0.3153 Epoch 35/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3128 - val_loss: 0.3425 Epoch 36/100 7740/7740 [==============================] - 0s 48us/sample - loss: 0.3115 - val_loss: 0.3047 Epoch 37/100 7740/7740 [==============================] - 0s 44us/sample - loss: 0.3098 - val_loss: 0.3154 Epoch 38/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3085 - val_loss: 0.3139 Epoch 39/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3074 - val_loss: 0.3774 Epoch 40/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3071 - val_loss: 0.3249 Epoch 41/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3071 - val_loss: 0.3420 Epoch 42/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3026 - val_loss: 0.3788 Epoch 43/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3027 - val_loss: 0.3213 Epoch 44/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3014 - val_loss: 0.3695 Epoch 45/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3001 - val_loss: 0.2987 Epoch 46/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2987 - val_loss: 0.3171 Epoch 47/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2981 - val_loss: 0.3631 Epoch 48/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2975 - val_loss: 0.2914 Epoch 49/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2959 - val_loss: 0.3695 Epoch 50/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.2942 - val_loss: 0.2942 Epoch 51/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2925 - val_loss: 0.3925 Epoch 52/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2939 - val_loss: 0.2917 Epoch 53/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2916 - val_loss: 0.3588 Epoch 54/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2899 - val_loss: 0.3232 Epoch 55/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2914 - val_loss: 0.2941 Epoch 56/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2878 - val_loss: 0.2984 Epoch 57/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2871 - val_loss: 0.3576 Epoch 58/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.2862 - val_loss: 0.3397 3870/3870 [==============================] - 0s 15us/sample - loss: 0.3052 7740/7740 [==============================] - 0s 17us/sample - loss: 0.2829 [CV] learning_rate=0.004038022680351922, n_hidden=2, n_neurons=69, total= 18.3s [CV] learning_rate=0.0006477779307312751, n_hidden=1, n_neurons=59 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 51us/sample - loss: 2.7840 - val_loss: 3.1844 Epoch 2/100 7740/7740 [==============================] - 0s 36us/sample - loss: 1.2596 - val_loss: 0.9812 Epoch 3/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.8758 - val_loss: 0.7441 Epoch 4/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.7312 - val_loss: 0.6454 Epoch 5/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.6688 - val_loss: 0.6013 Epoch 6/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6341 - val_loss: 0.5979 Epoch 7/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.6103 - val_loss: 0.5533 Epoch 8/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5899 - val_loss: 0.5331 Epoch 9/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5719 - val_loss: 0.5214 Epoch 10/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5560 - val_loss: 0.5139 Epoch 11/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5418 - val_loss: 0.4965 Epoch 12/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5286 - val_loss: 0.4943 Epoch 13/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5171 - val_loss: 0.4726 Epoch 14/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5061 - val_loss: 0.4608 Epoch 15/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4962 - val_loss: 0.4600 Epoch 16/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4876 - val_loss: 0.4459 Epoch 17/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4794 - val_loss: 0.4467 Epoch 18/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4722 - val_loss: 0.4322 Epoch 19/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4656 - val_loss: 0.4305 Epoch 20/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4597 - val_loss: 0.4219 Epoch 21/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4540 - val_loss: 0.4187 Epoch 22/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4492 - val_loss: 0.4126 Epoch 23/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4442 - val_loss: 0.4112 Epoch 24/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4402 - val_loss: 0.4057 Epoch 25/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4363 - val_loss: 0.4048 Epoch 26/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4327 - val_loss: 0.4011 Epoch 27/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4291 - val_loss: 0.3973 Epoch 28/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4259 - val_loss: 0.3977 Epoch 29/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4230 - val_loss: 0.3935 Epoch 30/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4201 - val_loss: 0.3995 Epoch 31/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4176 - val_loss: 0.3938 Epoch 32/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4149 - val_loss: 0.3924 Epoch 33/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4125 - val_loss: 0.3842 Epoch 34/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4102 - val_loss: 0.3922 Epoch 35/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4080 - val_loss: 0.3810 Epoch 36/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4059 - val_loss: 0.3883 Epoch 37/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4039 - val_loss: 0.3848 Epoch 38/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4018 - val_loss: 0.3784 Epoch 39/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4001 - val_loss: 0.3841 Epoch 40/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3984 - val_loss: 0.3793 Epoch 41/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3966 - val_loss: 0.3837 Epoch 42/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3951 - val_loss: 0.3736 Epoch 43/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3934 - val_loss: 0.3825 Epoch 44/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3921 - val_loss: 0.3693 Epoch 45/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3905 - val_loss: 0.3694 Epoch 46/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3891 - val_loss: 0.3673 Epoch 47/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3875 - val_loss: 0.3753 Epoch 48/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3862 - val_loss: 0.3868 Epoch 49/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3850 - val_loss: 0.3735 Epoch 50/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3837 - val_loss: 0.3745 Epoch 51/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3825 - val_loss: 0.3742 Epoch 52/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3814 - val_loss: 0.3672 Epoch 53/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3800 - val_loss: 0.3624 Epoch 54/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3787 - val_loss: 0.3769 Epoch 55/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3778 - val_loss: 0.3609 Epoch 56/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3766 - val_loss: 0.3719 Epoch 57/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3757 - val_loss: 0.3732 Epoch 58/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3748 - val_loss: 0.3588 Epoch 59/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3736 - val_loss: 0.3681 Epoch 60/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3726 - val_loss: 0.3557 Epoch 61/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3716 - val_loss: 0.3600 Epoch 62/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3706 - val_loss: 0.3524 Epoch 63/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3699 - val_loss: 0.3550 Epoch 64/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3689 - val_loss: 0.3609 Epoch 65/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3681 - val_loss: 0.3504 Epoch 66/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3670 - val_loss: 0.3500 Epoch 67/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3662 - val_loss: 0.3657 Epoch 68/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3655 - val_loss: 0.3561 Epoch 69/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3647 - val_loss: 0.3559 Epoch 70/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3638 - val_loss: 0.3566 Epoch 71/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3631 - val_loss: 0.3482 Epoch 72/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3623 - val_loss: 0.3550 Epoch 73/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3617 - val_loss: 0.3494 Epoch 74/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3609 - val_loss: 0.3559 Epoch 75/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3602 - val_loss: 0.3543 Epoch 76/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3595 - val_loss: 0.3439 Epoch 77/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3588 - val_loss: 0.3552 Epoch 78/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3582 - val_loss: 0.3423 Epoch 79/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3576 - val_loss: 0.3480 Epoch 80/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3569 - val_loss: 0.3429 Epoch 81/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3562 - val_loss: 0.3474 Epoch 82/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3555 - val_loss: 0.3482 Epoch 83/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3551 - val_loss: 0.3444 Epoch 84/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3545 - val_loss: 0.3417 Epoch 85/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3538 - val_loss: 0.3400 Epoch 86/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3533 - val_loss: 0.3438 Epoch 87/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3526 - val_loss: 0.3545 Epoch 88/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3523 - val_loss: 0.3430 Epoch 89/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3515 - val_loss: 0.3515 Epoch 90/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3511 - val_loss: 0.3432 Epoch 91/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3505 - val_loss: 0.3404 Epoch 92/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3499 - val_loss: 0.3428 Epoch 93/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3496 - val_loss: 0.3470 Epoch 94/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3490 - val_loss: 0.3530 Epoch 95/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3485 - val_loss: 0.3673 3870/3870 [==============================] - 0s 13us/sample - loss: 0.3662 7740/7740 [==============================] - 0s 15us/sample - loss: 0.3478 [CV] learning_rate=0.0006477779307312751, n_hidden=1, n_neurons=59, total= 27.3s [CV] learning_rate=0.0006477779307312751, n_hidden=1, n_neurons=59 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 51us/sample - loss: 2.9243 - val_loss: 18.8442 Epoch 2/100 7740/7740 [==============================] - 0s 36us/sample - loss: 1.2074 - val_loss: 16.0418 Epoch 3/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.8573 - val_loss: 11.0685 Epoch 4/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.7260 - val_loss: 7.3959 Epoch 5/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6690 - val_loss: 5.0236 Epoch 6/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6350 - val_loss: 3.4955 Epoch 7/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6096 - val_loss: 2.3247 Epoch 8/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5878 - val_loss: 1.5633 Epoch 9/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5687 - val_loss: 1.0776 Epoch 10/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5517 - val_loss: 0.7538 Epoch 11/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5365 - val_loss: 0.5741 Epoch 12/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5228 - val_loss: 0.5018 Epoch 13/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5106 - val_loss: 0.4949 Epoch 14/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4997 - val_loss: 0.5297 Epoch 15/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4899 - val_loss: 0.5825 Epoch 16/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4812 - val_loss: 0.6560 Epoch 17/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4736 - val_loss: 0.7104 Epoch 18/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4666 - val_loss: 0.7667 Epoch 19/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4604 - val_loss: 0.7889 Epoch 20/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4546 - val_loss: 0.8336 Epoch 21/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4495 - val_loss: 0.8446 Epoch 22/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4448 - val_loss: 0.8488 Epoch 23/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4403 - val_loss: 0.8735 3870/3870 [==============================] - 0s 14us/sample - loss: 0.4598 7740/7740 [==============================] - 0s 15us/sample - loss: 0.4382 [CV] learning_rate=0.0006477779307312751, n_hidden=1, n_neurons=59, total= 6.9s [CV] learning_rate=0.0006477779307312751, n_hidden=1, n_neurons=59 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 51us/sample - loss: 2.6866 - val_loss: 8.0515 Epoch 2/100 7740/7740 [==============================] - 0s 37us/sample - loss: 1.0829 - val_loss: 1.5668 Epoch 3/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.7425 - val_loss: 0.6838 Epoch 4/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6664 - val_loss: 0.6391 Epoch 5/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6360 - val_loss: 0.6036 Epoch 6/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6139 - val_loss: 0.5849 Epoch 7/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5957 - val_loss: 0.5632 Epoch 8/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5793 - val_loss: 0.5505 Epoch 9/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5648 - val_loss: 0.5358 Epoch 10/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5513 - val_loss: 0.5225 Epoch 11/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5389 - val_loss: 0.5139 Epoch 12/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5273 - val_loss: 0.4968 Epoch 13/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5164 - val_loss: 0.4871 Epoch 14/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.5063 - val_loss: 0.4761 Epoch 15/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4972 - val_loss: 0.4666 Epoch 16/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4882 - val_loss: 0.4606 Epoch 17/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4801 - val_loss: 0.4513 Epoch 18/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4728 - val_loss: 0.4440 Epoch 19/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4654 - val_loss: 0.4390 Epoch 20/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4594 - val_loss: 0.4303 Epoch 21/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4531 - val_loss: 0.4275 Epoch 22/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4476 - val_loss: 0.4231 Epoch 23/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4425 - val_loss: 0.4187 Epoch 24/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4376 - val_loss: 0.4136 Epoch 25/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4333 - val_loss: 0.4054 Epoch 26/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4293 - val_loss: 0.4118 Epoch 27/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4256 - val_loss: 0.4052 Epoch 28/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4221 - val_loss: 0.3981 Epoch 29/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4190 - val_loss: 0.3979 Epoch 30/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4159 - val_loss: 0.3940 Epoch 31/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4132 - val_loss: 0.3969 Epoch 32/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4106 - val_loss: 0.3897 Epoch 33/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4080 - val_loss: 0.3888 Epoch 34/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4058 - val_loss: 0.3832 Epoch 35/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4036 - val_loss: 0.3949 Epoch 36/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4017 - val_loss: 0.3838 Epoch 37/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3996 - val_loss: 0.3866 Epoch 38/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3978 - val_loss: 0.3872 Epoch 39/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3961 - val_loss: 0.3955 Epoch 40/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3944 - val_loss: 0.3855 Epoch 41/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3929 - val_loss: 0.3826 Epoch 42/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3914 - val_loss: 0.3714 Epoch 43/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3896 - val_loss: 0.3914 Epoch 44/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3885 - val_loss: 0.3794 Epoch 45/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3870 - val_loss: 0.3651 Epoch 46/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3854 - val_loss: 0.3952 Epoch 47/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3843 - val_loss: 0.3603 Epoch 48/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3831 - val_loss: 0.3862 Epoch 49/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3819 - val_loss: 0.3959 Epoch 50/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3807 - val_loss: 0.3726 Epoch 51/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3797 - val_loss: 0.3636 Epoch 52/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3783 - val_loss: 0.3939 Epoch 53/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3776 - val_loss: 0.3632 Epoch 54/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3760 - val_loss: 0.3598 Epoch 55/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3753 - val_loss: 0.3850 Epoch 56/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3745 - val_loss: 0.3665 Epoch 57/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3738 - val_loss: 0.3574 Epoch 58/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3724 - val_loss: 0.3859 Epoch 59/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3720 - val_loss: 0.3780 Epoch 60/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3710 - val_loss: 0.3745 Epoch 61/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3703 - val_loss: 0.3562 Epoch 62/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3694 - val_loss: 0.3705 Epoch 63/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3685 - val_loss: 0.3532 Epoch 64/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3675 - val_loss: 0.3531 Epoch 65/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3670 - val_loss: 0.3522 Epoch 66/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3660 - val_loss: 0.3824 Epoch 67/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3654 - val_loss: 0.3472 Epoch 68/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3649 - val_loss: 0.3528 Epoch 69/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3639 - val_loss: 0.3488 Epoch 70/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3634 - val_loss: 0.3521 Epoch 71/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3628 - val_loss: 0.3633 Epoch 72/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3621 - val_loss: 0.3724 Epoch 73/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3615 - val_loss: 0.3504 Epoch 74/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3606 - val_loss: 0.3757 Epoch 75/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3601 - val_loss: 0.3428 Epoch 76/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3597 - val_loss: 0.3600 Epoch 77/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3590 - val_loss: 0.3468 Epoch 78/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3584 - val_loss: 0.3391 Epoch 79/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3576 - val_loss: 0.3450 Epoch 80/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3571 - val_loss: 0.3658 Epoch 81/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3566 - val_loss: 0.3612 Epoch 82/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3561 - val_loss: 0.3525 Epoch 83/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3554 - val_loss: 0.3548 Epoch 84/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3550 - val_loss: 0.3449 Epoch 85/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3542 - val_loss: 0.3533 Epoch 86/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3542 - val_loss: 0.3527 Epoch 87/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3534 - val_loss: 0.3655 Epoch 88/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3530 - val_loss: 0.3569 3870/3870 [==============================] - 0s 14us/sample - loss: 0.3494 7740/7740 [==============================] - 0s 14us/sample - loss: 0.3521 [CV] learning_rate=0.0006477779307312751, n_hidden=1, n_neurons=59, total= 24.8s [CV] learning_rate=0.0032123503761513073, n_hidden=0, n_neurons=87 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 46us/sample - loss: 1.3665 - val_loss: 30.0074 Epoch 2/100 7740/7740 [==============================] - 0s 34us/sample - loss: 1.0455 - val_loss: 22.4981 Epoch 3/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.8411 - val_loss: 0.5801 Epoch 4/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.5291 - val_loss: 0.5369 Epoch 5/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4956 - val_loss: 0.5124 Epoch 6/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4722 - val_loss: 0.4677 Epoch 7/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4550 - val_loss: 0.4384 Epoch 8/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4421 - val_loss: 0.4157 Epoch 9/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4312 - val_loss: 0.3990 Epoch 10/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4227 - val_loss: 0.3904 Epoch 11/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4157 - val_loss: 0.3844 Epoch 12/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4095 - val_loss: 0.3846 Epoch 13/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4053 - val_loss: 0.3783 Epoch 14/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4016 - val_loss: 0.3811 Epoch 15/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3976 - val_loss: 0.3855 Epoch 16/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3946 - val_loss: 0.3793 Epoch 17/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3917 - val_loss: 0.3790 Epoch 18/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3892 - val_loss: 0.3795 Epoch 19/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3868 - val_loss: 0.3751 Epoch 20/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3844 - val_loss: 0.3905 Epoch 21/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3830 - val_loss: 0.3758 Epoch 22/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3806 - val_loss: 0.3810 Epoch 23/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3790 - val_loss: 0.3810 Epoch 24/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3773 - val_loss: 0.3796 Epoch 25/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3763 - val_loss: 0.3838 Epoch 26/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3739 - val_loss: 0.3877 Epoch 27/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3731 - val_loss: 0.3776 Epoch 28/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3716 - val_loss: 0.3778 Epoch 29/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3702 - val_loss: 0.3812 3870/3870 [==============================] - 0s 13us/sample - loss: 0.3857 7740/7740 [==============================] - 0s 13us/sample - loss: 0.3677 [CV] learning_rate=0.0032123503761513073, n_hidden=0, n_neurons=87, total= 7.9s [CV] learning_rate=0.0032123503761513073, n_hidden=0, n_neurons=87 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 46us/sample - loss: 1.3253 - val_loss: 1.7528 Epoch 2/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.6961 - val_loss: 0.6533 Epoch 3/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.6193 - val_loss: 1.3104 Epoch 4/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5629 - val_loss: 2.2476 Epoch 5/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5220 - val_loss: 2.6183 Epoch 6/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4918 - val_loss: 2.7712 Epoch 7/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4692 - val_loss: 2.4734 Epoch 8/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4537 - val_loss: 2.0569 Epoch 9/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4415 - val_loss: 1.6704 Epoch 10/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4329 - val_loss: 1.3330 Epoch 11/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4249 - val_loss: 0.9791 Epoch 12/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4199 - val_loss: 0.8599 3870/3870 [==============================] - 0s 13us/sample - loss: 0.4479 7740/7740 [==============================] - 0s 14us/sample - loss: 0.4163 [CV] learning_rate=0.0032123503761513073, n_hidden=0, n_neurons=87, total= 3.5s [CV] learning_rate=0.0032123503761513073, n_hidden=0, n_neurons=87 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 47us/sample - loss: 1.5445 - val_loss: 15.2564 Epoch 2/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.9030 - val_loss: 5.7446 Epoch 3/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.6692 - val_loss: 0.6443 Epoch 4/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5637 - val_loss: 0.5236 Epoch 5/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5269 - val_loss: 0.4840 Epoch 6/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4986 - val_loss: 0.4984 Epoch 7/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4779 - val_loss: 0.4490 Epoch 8/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4620 - val_loss: 0.4321 Epoch 9/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4504 - val_loss: 0.4292 Epoch 10/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4414 - val_loss: 0.4454 Epoch 11/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4341 - val_loss: 0.4018 Epoch 12/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4281 - val_loss: 0.4089 Epoch 13/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4226 - val_loss: 0.4120 Epoch 14/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4179 - val_loss: 0.4455 Epoch 15/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4138 - val_loss: 0.4541 Epoch 16/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4108 - val_loss: 0.4184 Epoch 17/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4075 - val_loss: 0.4369 Epoch 18/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4044 - val_loss: 0.3751 Epoch 19/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4026 - val_loss: 0.3771 Epoch 20/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3994 - val_loss: 0.4666 Epoch 21/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3988 - val_loss: 0.4131 Epoch 22/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3952 - val_loss: 0.3811 Epoch 23/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3930 - val_loss: 0.3736 Epoch 24/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3915 - val_loss: 0.3913 Epoch 25/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3897 - val_loss: 0.3709 Epoch 26/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3880 - val_loss: 0.3942 Epoch 27/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3863 - val_loss: 0.3592 Epoch 28/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3848 - val_loss: 0.3652 Epoch 29/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3830 - val_loss: 0.3596 Epoch 30/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3815 - val_loss: 0.4866 Epoch 31/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3811 - val_loss: 0.3524 Epoch 32/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3783 - val_loss: 0.5189 Epoch 33/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3787 - val_loss: 0.3859 Epoch 34/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3761 - val_loss: 0.3516 Epoch 35/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3747 - val_loss: 0.5188 Epoch 36/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3748 - val_loss: 0.3542 Epoch 37/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3734 - val_loss: 0.5031 Epoch 38/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3737 - val_loss: 0.3494 Epoch 39/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3713 - val_loss: 0.3880 Epoch 40/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3697 - val_loss: 0.3591 Epoch 41/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3686 - val_loss: 0.4488 Epoch 42/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3683 - val_loss: 0.3543 Epoch 43/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3671 - val_loss: 0.3664 Epoch 44/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3662 - val_loss: 0.3420 Epoch 45/100 7740/7740 [==============================] - 0s 32us/sample - loss: 0.3657 - val_loss: 0.3827 Epoch 46/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3648 - val_loss: 0.3587 Epoch 47/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3640 - val_loss: 0.3558 Epoch 48/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3629 - val_loss: 0.5029 Epoch 49/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3633 - val_loss: 0.3404 Epoch 50/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3614 - val_loss: 0.5297 Epoch 51/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3627 - val_loss: 0.3581 Epoch 52/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3605 - val_loss: 0.4724 Epoch 53/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3612 - val_loss: 0.3414 Epoch 54/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3593 - val_loss: 0.3880 Epoch 55/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3583 - val_loss: 0.3962 Epoch 56/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3576 - val_loss: 0.4286 Epoch 57/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3569 - val_loss: 0.3945 Epoch 58/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3561 - val_loss: 0.3448 Epoch 59/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3563 - val_loss: 0.4321 3870/3870 [==============================] - 0s 13us/sample - loss: 0.3558 7740/7740 [==============================] - 0s 14us/sample - loss: 0.3563 [CV] learning_rate=0.0032123503761513073, n_hidden=0, n_neurons=87, total= 15.7s [CV] learning_rate=0.004576909498448105, n_hidden=2, n_neurons=87 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 58us/sample - loss: 1.0841 - val_loss: 1.6728 Epoch 2/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.5127 - val_loss: 3.2044 Epoch 3/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.4594 - val_loss: 0.4278 Epoch 4/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.4003 - val_loss: 0.4229 Epoch 5/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3793 - val_loss: 0.4236 Epoch 6/100 7740/7740 [==============================] - 0s 42us/sample - loss: 0.3653 - val_loss: 0.4169 Epoch 7/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3580 - val_loss: 0.4311 Epoch 8/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3524 - val_loss: 0.3587 Epoch 9/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3437 - val_loss: 0.4140 Epoch 10/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3405 - val_loss: 0.3898 Epoch 11/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3361 - val_loss: 0.3298 Epoch 12/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3320 - val_loss: 0.3523 Epoch 13/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3285 - val_loss: 0.3785 Epoch 14/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3275 - val_loss: 0.3206 Epoch 15/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3239 - val_loss: 0.3414 Epoch 16/100 7740/7740 [==============================] - 0s 42us/sample - loss: 0.3224 - val_loss: 0.3518 Epoch 17/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3189 - val_loss: 0.3833 Epoch 18/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3168 - val_loss: 0.3239 Epoch 19/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3165 - val_loss: 0.4082 Epoch 20/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3133 - val_loss: 0.3160 Epoch 21/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3114 - val_loss: 0.3317 Epoch 22/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3098 - val_loss: 0.3171 Epoch 23/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3073 - val_loss: 0.3239 Epoch 24/100 7740/7740 [==============================] - 0s 44us/sample - loss: 0.3062 - val_loss: 0.3073 Epoch 25/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3044 - val_loss: 0.3243 Epoch 26/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3034 - val_loss: 0.3801 Epoch 27/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3025 - val_loss: 0.3579 Epoch 28/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3026 - val_loss: 0.3355 Epoch 29/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3003 - val_loss: 0.3455 Epoch 30/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.2983 - val_loss: 0.3128 Epoch 31/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.2955 - val_loss: 0.3165 Epoch 32/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.2940 - val_loss: 0.3149 Epoch 33/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.2909 - val_loss: 0.3912 Epoch 34/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.2930 - val_loss: 0.3209 3870/3870 [==============================] - 0s 16us/sample - loss: 0.3271 7740/7740 [==============================] - 0s 17us/sample - loss: 0.2891 [CV] learning_rate=0.004576909498448105, n_hidden=2, n_neurons=87, total= 11.3s [CV] learning_rate=0.004576909498448105, n_hidden=2, n_neurons=87 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 58us/sample - loss: 1.0489 - val_loss: 1.0151 Epoch 2/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.5291 - val_loss: 0.5506 Epoch 3/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.4577 - val_loss: 0.4543 Epoch 4/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.4166 - val_loss: 0.4211 Epoch 5/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3932 - val_loss: 0.4991 Epoch 6/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3774 - val_loss: 0.6786 Epoch 7/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3659 - val_loss: 0.9418 Epoch 8/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3593 - val_loss: 0.8760 Epoch 9/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3531 - val_loss: 0.9404 Epoch 10/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3499 - val_loss: 1.0715 Epoch 11/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3454 - val_loss: 0.9127 Epoch 12/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3412 - val_loss: 1.0588 Epoch 13/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3385 - val_loss: 0.9105 Epoch 14/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3345 - val_loss: 0.9548 3870/3870 [==============================] - 0s 16us/sample - loss: 0.3628 7740/7740 [==============================] - 0s 17us/sample - loss: 0.3311 [CV] learning_rate=0.004576909498448105, n_hidden=2, n_neurons=87, total= 4.9s [CV] learning_rate=0.004576909498448105, n_hidden=2, n_neurons=87 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 58us/sample - loss: 0.9595 - val_loss: 10.8814 Epoch 2/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.6306 - val_loss: 2.7405 Epoch 3/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.4783 - val_loss: 0.4214 Epoch 4/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.4204 - val_loss: 0.3964 Epoch 5/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3979 - val_loss: 0.4372 Epoch 6/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3852 - val_loss: 0.4389 Epoch 7/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3759 - val_loss: 0.4287 Epoch 8/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3691 - val_loss: 0.4075 Epoch 9/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3640 - val_loss: 0.4152 Epoch 10/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3600 - val_loss: 0.4284 Epoch 11/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3548 - val_loss: 0.3699 Epoch 12/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3526 - val_loss: 0.4052 Epoch 13/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3497 - val_loss: 0.3468 Epoch 14/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3462 - val_loss: 0.3865 Epoch 15/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3427 - val_loss: 0.3764 Epoch 16/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3404 - val_loss: 0.3697 Epoch 17/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3379 - val_loss: 0.3259 Epoch 18/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3358 - val_loss: 0.3488 Epoch 19/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3335 - val_loss: 0.3473 Epoch 20/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3308 - val_loss: 0.3886 Epoch 21/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3314 - val_loss: 0.3202 Epoch 22/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3279 - val_loss: 0.3620 Epoch 23/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3263 - val_loss: 0.3380 Epoch 24/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3246 - val_loss: 0.4124 Epoch 25/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3234 - val_loss: 0.3171 Epoch 26/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3224 - val_loss: 0.3410 Epoch 27/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3195 - val_loss: 0.3358 Epoch 28/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3169 - val_loss: 0.3893 Epoch 29/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3175 - val_loss: 0.3208 Epoch 30/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3168 - val_loss: 0.3512 Epoch 31/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3147 - val_loss: 0.3236 Epoch 32/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3111 - val_loss: 0.3870 Epoch 33/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3105 - val_loss: 0.4080 Epoch 34/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3108 - val_loss: 0.3167 Epoch 35/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3102 - val_loss: 0.3398 Epoch 36/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3071 - val_loss: 0.3612 Epoch 37/100 7740/7740 [==============================] - 0s 42us/sample - loss: 0.3062 - val_loss: 0.3460 Epoch 38/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3046 - val_loss: 0.4104 Epoch 39/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3079 - val_loss: 0.2997 Epoch 40/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3056 - val_loss: 0.3803 Epoch 41/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3041 - val_loss: 0.3085 Epoch 42/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3026 - val_loss: 0.3959 Epoch 43/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3006 - val_loss: 0.3498 Epoch 44/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2981 - val_loss: 0.3024 Epoch 45/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.2986 - val_loss: 0.3475 Epoch 46/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.2949 - val_loss: 0.3191 Epoch 47/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.2946 - val_loss: 0.3201 Epoch 48/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.2937 - val_loss: 0.3001 Epoch 49/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.2929 - val_loss: 0.3431 3870/3870 [==============================] - 0s 15us/sample - loss: 0.3356 7740/7740 [==============================] - 0s 16us/sample - loss: 0.3133 [CV] learning_rate=0.004576909498448105, n_hidden=2, n_neurons=87, total= 15.6s [CV] learning_rate=0.0011714615165291644, n_hidden=0, n_neurons=56 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 46us/sample - loss: 2.4510 - val_loss: 5.1025 Epoch 2/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.9274 - val_loss: 0.8434 Epoch 3/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.7575 - val_loss: 0.7224 Epoch 4/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.7072 - val_loss: 0.6691 Epoch 5/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.6692 - val_loss: 0.6309 Epoch 6/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.6383 - val_loss: 0.6009 Epoch 7/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.6099 - val_loss: 0.5716 Epoch 8/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.5841 - val_loss: 0.5599 Epoch 9/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.5598 - val_loss: 0.5321 Epoch 10/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.5414 - val_loss: 0.5070 Epoch 11/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.5225 - val_loss: 0.5337 Epoch 12/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.5066 - val_loss: 0.4755 Epoch 13/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4927 - val_loss: 0.4675 Epoch 14/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4805 - val_loss: 0.4664 Epoch 15/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4693 - val_loss: 0.4675 Epoch 16/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4607 - val_loss: 0.4314 Epoch 17/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4532 - val_loss: 0.4594 Epoch 18/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4460 - val_loss: 0.4296 Epoch 19/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4400 - val_loss: 0.4329 Epoch 20/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4350 - val_loss: 0.4289 Epoch 21/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4304 - val_loss: 0.4344 Epoch 22/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4264 - val_loss: 0.4139 Epoch 23/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4227 - val_loss: 0.4219 Epoch 24/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4191 - val_loss: 0.4107 Epoch 25/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4165 - val_loss: 0.4236 Epoch 26/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4137 - val_loss: 0.4210 Epoch 27/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4110 - val_loss: 0.4112 Epoch 28/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4091 - val_loss: 0.4074 Epoch 29/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4068 - val_loss: 0.4051 Epoch 30/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4047 - val_loss: 0.3898 Epoch 31/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4025 - val_loss: 0.4200 Epoch 32/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4012 - val_loss: 0.3944 Epoch 33/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3991 - val_loss: 0.3930 Epoch 34/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3976 - val_loss: 0.4333 Epoch 35/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3957 - val_loss: 0.3851 Epoch 36/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3948 - val_loss: 0.3853 Epoch 37/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3930 - val_loss: 0.3914 Epoch 38/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3915 - val_loss: 0.4464 Epoch 39/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3907 - val_loss: 0.3995 Epoch 40/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3896 - val_loss: 0.4028 Epoch 41/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3882 - val_loss: 0.3802 Epoch 42/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3859 - val_loss: 0.3964 Epoch 43/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3854 - val_loss: 0.4256 Epoch 44/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3844 - val_loss: 0.3978 Epoch 45/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3836 - val_loss: 0.3725 Epoch 46/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3819 - val_loss: 0.4094 Epoch 47/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3822 - val_loss: 0.3933 Epoch 48/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3808 - val_loss: 0.3953 Epoch 49/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3797 - val_loss: 0.4138 Epoch 50/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3788 - val_loss: 0.3889 Epoch 51/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3782 - val_loss: 0.3666 Epoch 52/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3773 - val_loss: 0.4196 Epoch 53/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3768 - val_loss: 0.3661 Epoch 54/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3759 - val_loss: 0.3684 Epoch 55/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3753 - val_loss: 0.3827 Epoch 56/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3739 - val_loss: 0.3700 Epoch 57/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3738 - val_loss: 0.4040 Epoch 58/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3734 - val_loss: 0.4116 Epoch 59/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3726 - val_loss: 0.3979 Epoch 60/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3716 - val_loss: 0.3631 Epoch 61/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3714 - val_loss: 0.4177 Epoch 62/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3709 - val_loss: 0.3661 Epoch 63/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3700 - val_loss: 0.4091 Epoch 64/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3696 - val_loss: 0.3683 Epoch 65/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3692 - val_loss: 0.3736 Epoch 66/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3687 - val_loss: 0.3813 Epoch 67/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3679 - val_loss: 0.3978 Epoch 68/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3676 - val_loss: 0.3629 Epoch 69/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3667 - val_loss: 0.3627 Epoch 70/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3669 - val_loss: 0.3584 Epoch 71/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3658 - val_loss: 0.3676 Epoch 72/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3658 - val_loss: 0.3675 Epoch 73/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3650 - val_loss: 0.4163 Epoch 74/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3647 - val_loss: 0.3869 Epoch 75/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3644 - val_loss: 0.3578 Epoch 76/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3634 - val_loss: 0.4146 Epoch 77/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3635 - val_loss: 0.3891 Epoch 78/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3629 - val_loss: 0.3620 Epoch 79/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3623 - val_loss: 0.3643 Epoch 80/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3620 - val_loss: 0.3540 Epoch 81/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3616 - val_loss: 0.4140 Epoch 82/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3612 - val_loss: 0.3953 Epoch 83/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3608 - val_loss: 0.4081 Epoch 84/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3603 - val_loss: 0.3990 Epoch 85/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3602 - val_loss: 0.3479 Epoch 86/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3590 - val_loss: 0.4146 Epoch 87/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3596 - val_loss: 0.3958 Epoch 88/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3588 - val_loss: 0.3643 Epoch 89/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3581 - val_loss: 0.3519 Epoch 90/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3577 - val_loss: 0.4248 Epoch 91/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3577 - val_loss: 0.3980 Epoch 92/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3572 - val_loss: 0.3431 Epoch 93/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3568 - val_loss: 0.4402 Epoch 94/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3570 - val_loss: 0.3865 Epoch 95/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3560 - val_loss: 0.3469 Epoch 96/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3558 - val_loss: 0.3514 Epoch 97/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3556 - val_loss: 0.4285 Epoch 98/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3553 - val_loss: 0.3403 Epoch 99/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3549 - val_loss: 0.3584 Epoch 100/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3539 - val_loss: 0.3579 3870/3870 [==============================] - 0s 12us/sample - loss: 0.3695 7740/7740 [==============================] - 0s 14us/sample - loss: 0.3534 [CV] learning_rate=0.0011714615165291644, n_hidden=0, n_neurons=56, total= 26.8s [CV] learning_rate=0.0011714615165291644, n_hidden=0, n_neurons=56 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 46us/sample - loss: 2.2833 - val_loss: 9.3171 Epoch 2/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.8871 - val_loss: 6.2420 Epoch 3/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.7421 - val_loss: 3.4939 Epoch 4/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.6852 - val_loss: 1.9102 Epoch 5/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.6445 - val_loss: 1.0602 Epoch 6/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.6116 - val_loss: 0.6847 Epoch 7/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5845 - val_loss: 0.5604 Epoch 8/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5618 - val_loss: 0.5700 Epoch 9/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5430 - val_loss: 0.6534 Epoch 10/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5268 - val_loss: 0.7650 Epoch 11/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5133 - val_loss: 0.8208 Epoch 12/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5014 - val_loss: 0.8494 Epoch 13/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4916 - val_loss: 0.8949 Epoch 14/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4826 - val_loss: 0.9046 Epoch 15/100 7740/7740 [==============================] - 0s 32us/sample - loss: 0.4747 - val_loss: 0.8815 Epoch 16/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4677 - val_loss: 0.8587 Epoch 17/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4616 - val_loss: 0.7927 3870/3870 [==============================] - 0s 12us/sample - loss: 0.4803 7740/7740 [==============================] - 0s 14us/sample - loss: 0.4579 [CV] learning_rate=0.0011714615165291644, n_hidden=0, n_neurons=56, total= 4.7s [CV] learning_rate=0.0011714615165291644, n_hidden=0, n_neurons=56 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 47us/sample - loss: 2.0724 - val_loss: 1.4705 Epoch 2/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.7958 - val_loss: 0.7538 Epoch 3/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.6896 - val_loss: 0.6508 Epoch 4/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.6477 - val_loss: 0.6159 Epoch 5/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.6167 - val_loss: 0.5844 Epoch 6/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5906 - val_loss: 0.5521 Epoch 7/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.5675 - val_loss: 0.5322 Epoch 8/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5478 - val_loss: 0.5150 Epoch 9/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5305 - val_loss: 0.5061 Epoch 10/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.5161 - val_loss: 0.4909 Epoch 11/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5025 - val_loss: 0.4726 Epoch 12/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4914 - val_loss: 0.4612 Epoch 13/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4815 - val_loss: 0.4534 Epoch 14/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4728 - val_loss: 0.4438 Epoch 15/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4650 - val_loss: 0.4357 Epoch 16/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4585 - val_loss: 0.4313 Epoch 17/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4522 - val_loss: 0.4274 Epoch 18/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4470 - val_loss: 0.4222 Epoch 19/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4419 - val_loss: 0.4129 Epoch 20/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4377 - val_loss: 0.4086 Epoch 21/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4337 - val_loss: 0.4057 Epoch 22/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4300 - val_loss: 0.4034 Epoch 23/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4266 - val_loss: 0.3982 Epoch 24/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4237 - val_loss: 0.3950 Epoch 25/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4206 - val_loss: 0.4003 Epoch 26/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4182 - val_loss: 0.3956 Epoch 27/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4158 - val_loss: 0.3966 Epoch 28/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4134 - val_loss: 0.3945 Epoch 29/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4114 - val_loss: 0.3836 Epoch 30/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4094 - val_loss: 0.3941 Epoch 31/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4074 - val_loss: 0.3814 Epoch 32/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4053 - val_loss: 0.3792 Epoch 33/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4041 - val_loss: 0.3928 Epoch 34/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4027 - val_loss: 0.3802 Epoch 35/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4010 - val_loss: 0.3835 Epoch 36/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3997 - val_loss: 0.3741 Epoch 37/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3981 - val_loss: 0.3885 Epoch 38/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3974 - val_loss: 0.3860 Epoch 39/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3962 - val_loss: 0.3697 Epoch 40/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3949 - val_loss: 0.3734 Epoch 41/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3940 - val_loss: 0.3706 Epoch 42/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3928 - val_loss: 0.3909 Epoch 43/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3920 - val_loss: 0.3772 Epoch 44/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3908 - val_loss: 0.3750 Epoch 45/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3900 - val_loss: 0.3648 Epoch 46/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3891 - val_loss: 0.3807 Epoch 47/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3887 - val_loss: 0.3625 Epoch 48/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3876 - val_loss: 0.3746 Epoch 49/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3867 - val_loss: 0.3754 Epoch 50/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3864 - val_loss: 0.3645 Epoch 51/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3851 - val_loss: 0.3951 Epoch 52/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3845 - val_loss: 0.3832 Epoch 53/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3841 - val_loss: 0.3613 Epoch 54/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3829 - val_loss: 0.3953 Epoch 55/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3825 - val_loss: 0.3757 Epoch 56/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3818 - val_loss: 0.3647 Epoch 57/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3810 - val_loss: 0.3959 Epoch 58/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3807 - val_loss: 0.3563 Epoch 59/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3802 - val_loss: 0.3882 Epoch 60/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3795 - val_loss: 0.3784 Epoch 61/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3786 - val_loss: 0.3908 Epoch 62/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3785 - val_loss: 0.3719 Epoch 63/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3778 - val_loss: 0.3788 Epoch 64/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3774 - val_loss: 0.3566 Epoch 65/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3766 - val_loss: 0.3630 Epoch 66/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3764 - val_loss: 0.3537 Epoch 67/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3756 - val_loss: 0.3850 Epoch 68/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3755 - val_loss: 0.3769 Epoch 69/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3749 - val_loss: 0.3656 Epoch 70/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3744 - val_loss: 0.3584 Epoch 71/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3739 - val_loss: 0.3706 Epoch 72/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3735 - val_loss: 0.3533 Epoch 73/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3731 - val_loss: 0.3556 Epoch 74/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3724 - val_loss: 0.3960 Epoch 75/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3724 - val_loss: 0.3755 Epoch 76/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3718 - val_loss: 0.3667 Epoch 77/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3715 - val_loss: 0.3575 Epoch 78/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3708 - val_loss: 0.3851 Epoch 79/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3707 - val_loss: 0.3694 Epoch 80/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3702 - val_loss: 0.3824 Epoch 81/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3700 - val_loss: 0.3673 Epoch 82/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3696 - val_loss: 0.3491 Epoch 83/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3691 - val_loss: 0.3479 Epoch 84/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3683 - val_loss: 0.3513 Epoch 85/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3683 - val_loss: 0.3882 Epoch 86/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3679 - val_loss: 0.3776 Epoch 87/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3677 - val_loss: 0.3607 Epoch 88/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3670 - val_loss: 0.3911 Epoch 89/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3670 - val_loss: 0.3468 Epoch 90/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3667 - val_loss: 0.3470 Epoch 91/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3661 - val_loss: 0.3466 Epoch 92/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3658 - val_loss: 0.3778 Epoch 93/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3654 - val_loss: 0.3447 Epoch 94/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3649 - val_loss: 0.3542 Epoch 95/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3647 - val_loss: 0.3634 Epoch 96/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3646 - val_loss: 0.3474 Epoch 97/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3640 - val_loss: 0.3834 Epoch 98/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3640 - val_loss: 0.3535 Epoch 99/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3636 - val_loss: 0.3502 Epoch 100/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3630 - val_loss: 0.3472 3870/3870 [==============================] - 0s 13us/sample - loss: 0.3650 7740/7740 [==============================] - 0s 14us/sample - loss: 0.3623 [CV] learning_rate=0.0011714615165291644, n_hidden=0, n_neurons=56, total= 26.5s [CV] learning_rate=0.0012578066689117673, n_hidden=1, n_neurons=60 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 52us/sample - loss: 2.0678 - val_loss: 1.4867 Epoch 2/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.7582 - val_loss: 0.7377 Epoch 3/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.6402 - val_loss: 0.5802 Epoch 4/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5891 - val_loss: 0.5485 Epoch 5/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.5528 - val_loss: 0.5201 Epoch 6/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.5260 - val_loss: 0.4890 Epoch 7/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5025 - val_loss: 0.4603 Epoch 8/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4839 - val_loss: 0.4542 Epoch 9/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4686 - val_loss: 0.4417 Epoch 10/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4565 - val_loss: 0.4411 Epoch 11/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4466 - val_loss: 0.4201 Epoch 12/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4381 - val_loss: 0.4113 Epoch 13/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4314 - val_loss: 0.4170 Epoch 14/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4250 - val_loss: 0.4002 Epoch 15/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4195 - val_loss: 0.4316 Epoch 16/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4149 - val_loss: 0.3887 Epoch 17/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4103 - val_loss: 0.3882 Epoch 18/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4060 - val_loss: 0.4235 Epoch 19/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4030 - val_loss: 0.4081 Epoch 20/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3990 - val_loss: 0.4274 Epoch 21/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3962 - val_loss: 0.3974 Epoch 22/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3929 - val_loss: 0.3794 Epoch 23/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3899 - val_loss: 0.4050 Epoch 24/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3879 - val_loss: 0.3742 Epoch 25/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3849 - val_loss: 0.3693 Epoch 26/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3824 - val_loss: 0.3617 Epoch 27/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3800 - val_loss: 0.3715 Epoch 28/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3780 - val_loss: 0.3569 Epoch 29/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3758 - val_loss: 0.3695 Epoch 30/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3735 - val_loss: 0.3967 Epoch 31/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3723 - val_loss: 0.3551 Epoch 32/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3700 - val_loss: 0.3600 Epoch 33/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3680 - val_loss: 0.3780 Epoch 34/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3665 - val_loss: 0.3552 Epoch 35/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3650 - val_loss: 0.3626 Epoch 36/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3631 - val_loss: 0.3454 Epoch 37/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3617 - val_loss: 0.3514 Epoch 38/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3603 - val_loss: 0.3436 Epoch 39/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3589 - val_loss: 0.3621 Epoch 40/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3575 - val_loss: 0.3398 Epoch 41/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3559 - val_loss: 0.3806 Epoch 42/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3551 - val_loss: 0.3377 Epoch 43/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3535 - val_loss: 0.3823 Epoch 44/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3525 - val_loss: 0.3376 Epoch 45/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3513 - val_loss: 0.3599 Epoch 46/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3497 - val_loss: 0.3378 Epoch 47/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3489 - val_loss: 0.3848 Epoch 48/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3482 - val_loss: 0.3528 Epoch 49/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3468 - val_loss: 0.3474 Epoch 50/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3459 - val_loss: 0.3603 Epoch 51/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3451 - val_loss: 0.3538 Epoch 52/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3440 - val_loss: 0.3315 Epoch 53/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3433 - val_loss: 0.3428 Epoch 54/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3425 - val_loss: 0.3466 Epoch 55/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3414 - val_loss: 0.3303 Epoch 56/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3408 - val_loss: 0.3706 Epoch 57/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3401 - val_loss: 0.3334 Epoch 58/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3386 - val_loss: 0.3698 Epoch 59/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3383 - val_loss: 0.3272 Epoch 60/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3373 - val_loss: 0.4134 Epoch 61/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3370 - val_loss: 0.3250 Epoch 62/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3361 - val_loss: 0.4060 Epoch 63/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3357 - val_loss: 0.3332 Epoch 64/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3347 - val_loss: 0.3328 Epoch 65/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3341 - val_loss: 0.3264 Epoch 66/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3335 - val_loss: 0.3295 Epoch 67/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3329 - val_loss: 0.3599 Epoch 68/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3321 - val_loss: 0.3459 Epoch 69/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3313 - val_loss: 0.3226 Epoch 70/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3308 - val_loss: 0.3513 Epoch 71/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3302 - val_loss: 0.3653 Epoch 72/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3300 - val_loss: 0.3384 Epoch 73/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3289 - val_loss: 0.3897 Epoch 74/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3286 - val_loss: 0.3191 Epoch 75/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3282 - val_loss: 0.3525 Epoch 76/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3274 - val_loss: 0.3197 Epoch 77/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3268 - val_loss: 0.4065 Epoch 78/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3270 - val_loss: 0.3257 Epoch 79/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3256 - val_loss: 0.3260 Epoch 80/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3253 - val_loss: 0.3684 Epoch 81/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3251 - val_loss: 0.3202 Epoch 82/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3245 - val_loss: 0.4489 Epoch 83/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3246 - val_loss: 0.3344 Epoch 84/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3239 - val_loss: 0.4869 3870/3870 [==============================] - 0s 13us/sample - loss: 0.3442 7740/7740 [==============================] - 0s 15us/sample - loss: 0.3227 [CV] learning_rate=0.0012578066689117673, n_hidden=1, n_neurons=60, total= 23.7s [CV] learning_rate=0.0012578066689117673, n_hidden=1, n_neurons=60 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 50us/sample - loss: 1.7817 - val_loss: 9.1129 Epoch 2/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.7881 - val_loss: 4.1277 Epoch 3/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6598 - val_loss: 1.6439 Epoch 4/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6022 - val_loss: 0.8057 Epoch 5/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5604 - val_loss: 0.5135 Epoch 6/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5278 - val_loss: 0.5437 Epoch 7/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5023 - val_loss: 0.7848 Epoch 8/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4825 - val_loss: 0.9492 Epoch 9/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4660 - val_loss: 1.0810 Epoch 10/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4530 - val_loss: 1.1596 Epoch 11/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4423 - val_loss: 1.1965 Epoch 12/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4338 - val_loss: 1.2383 Epoch 13/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4256 - val_loss: 1.1621 Epoch 14/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4193 - val_loss: 1.0986 Epoch 15/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4133 - val_loss: 1.0196 3870/3870 [==============================] - 0s 14us/sample - loss: 0.4393 7740/7740 [==============================] - 0s 15us/sample - loss: 0.4098 [CV] learning_rate=0.0012578066689117673, n_hidden=1, n_neurons=60, total= 4.6s [CV] learning_rate=0.0012578066689117673, n_hidden=1, n_neurons=60 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 51us/sample - loss: 2.1548 - val_loss: 2.2374 Epoch 2/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.8591 - val_loss: 0.7902 Epoch 3/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.7066 - val_loss: 0.6506 Epoch 4/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6480 - val_loss: 0.6075 Epoch 5/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6086 - val_loss: 0.5687 Epoch 6/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5752 - val_loss: 0.5391 Epoch 7/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5474 - val_loss: 0.5183 Epoch 8/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5239 - val_loss: 0.4882 Epoch 9/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5039 - val_loss: 0.4673 Epoch 10/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4862 - val_loss: 0.4519 Epoch 11/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4721 - val_loss: 0.4395 Epoch 12/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4603 - val_loss: 0.4273 Epoch 13/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4498 - val_loss: 0.4201 Epoch 14/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4415 - val_loss: 0.4119 Epoch 15/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4346 - val_loss: 0.4132 Epoch 16/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4276 - val_loss: 0.3996 Epoch 17/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4230 - val_loss: 0.4010 Epoch 18/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4180 - val_loss: 0.3988 Epoch 19/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4138 - val_loss: 0.3940 Epoch 20/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4100 - val_loss: 0.3974 Epoch 21/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4067 - val_loss: 0.4028 Epoch 22/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4044 - val_loss: 0.4049 Epoch 23/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4006 - val_loss: 0.3939 Epoch 24/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3971 - val_loss: 0.3815 Epoch 25/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3951 - val_loss: 0.3838 Epoch 26/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3922 - val_loss: 0.3768 Epoch 27/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3902 - val_loss: 0.3716 Epoch 28/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3872 - val_loss: 0.3778 Epoch 29/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3859 - val_loss: 0.3884 Epoch 30/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3841 - val_loss: 0.3782 Epoch 31/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3822 - val_loss: 0.3698 Epoch 32/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3807 - val_loss: 0.3677 Epoch 33/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3786 - val_loss: 0.3782 Epoch 34/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3770 - val_loss: 0.3646 Epoch 35/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3757 - val_loss: 0.3699 Epoch 36/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3742 - val_loss: 0.3663 Epoch 37/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3728 - val_loss: 0.3685 Epoch 38/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3710 - val_loss: 0.3635 Epoch 39/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3699 - val_loss: 0.3762 Epoch 40/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3693 - val_loss: 0.3657 Epoch 41/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3679 - val_loss: 0.3567 Epoch 42/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3662 - val_loss: 0.3659 Epoch 43/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3652 - val_loss: 0.3720 Epoch 44/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3645 - val_loss: 0.3576 Epoch 45/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3633 - val_loss: 0.3676 Epoch 46/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3623 - val_loss: 0.3505 Epoch 47/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3613 - val_loss: 0.3488 Epoch 48/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3604 - val_loss: 0.3592 Epoch 49/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3598 - val_loss: 0.3445 Epoch 50/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3590 - val_loss: 0.3681 Epoch 51/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3581 - val_loss: 0.3576 Epoch 52/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3569 - val_loss: 0.3442 Epoch 53/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3563 - val_loss: 0.3443 Epoch 54/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3558 - val_loss: 0.3566 Epoch 55/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3552 - val_loss: 0.3392 Epoch 56/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3546 - val_loss: 0.3462 Epoch 57/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3537 - val_loss: 0.3421 Epoch 58/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3527 - val_loss: 0.3472 Epoch 59/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3526 - val_loss: 0.3521 Epoch 60/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3519 - val_loss: 0.3422 Epoch 61/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3509 - val_loss: 0.3555 Epoch 62/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3506 - val_loss: 0.3409 Epoch 63/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3500 - val_loss: 0.3394 Epoch 64/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3493 - val_loss: 0.3418 Epoch 65/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3486 - val_loss: 0.3506 3870/3870 [==============================] - 0s 14us/sample - loss: 0.3470 7740/7740 [==============================] - 0s 15us/sample - loss: 0.3468 [CV] learning_rate=0.0012578066689117673, n_hidden=1, n_neurons=60, total= 18.6s [Parallel(n_jobs=1)]: Done 30 out of 30 | elapsed: 7.3min finished Train on 11610 samples, validate on 3870 samples Epoch 1/100 11610/11610 [==============================] - 1s 46us/sample - loss: 0.8992 - val_loss: 2.7901 Epoch 2/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.4828 - val_loss: 0.4578 Epoch 3/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.4082 - val_loss: 0.3782 Epoch 4/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3842 - val_loss: 0.4179 Epoch 5/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3707 - val_loss: 0.3564 Epoch 6/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3624 - val_loss: 0.3539 Epoch 7/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3541 - val_loss: 0.3745 Epoch 8/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3493 - val_loss: 0.4061 Epoch 9/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3448 - val_loss: 0.3380 Epoch 10/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3405 - val_loss: 0.3352 Epoch 11/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.3361 - val_loss: 0.4455 Epoch 12/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3349 - val_loss: 0.6610 Epoch 13/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3335 - val_loss: 0.4351 Epoch 14/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3284 - val_loss: 0.3318 Epoch 15/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3254 - val_loss: 0.3144 Epoch 16/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.3223 - val_loss: 0.4413 Epoch 17/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3226 - val_loss: 0.3295 Epoch 18/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.3182 - val_loss: 0.4551 Epoch 19/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.3182 - val_loss: 0.7770 Epoch 20/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3176 - val_loss: 0.8286 Epoch 21/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.3168 - val_loss: 0.4178 Epoch 22/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3118 - val_loss: 0.3867 Epoch 23/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.3082 - val_loss: 0.3094 Epoch 24/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.3063 - val_loss: 0.3018 Epoch 25/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3046 - val_loss: 0.3086 Epoch 26/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3035 - val_loss: 0.2955 Epoch 27/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3016 - val_loss: 0.2874 Epoch 28/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2998 - val_loss: 0.3067 Epoch 29/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2987 - val_loss: 0.2855 Epoch 30/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2985 - val_loss: 0.4065 Epoch 31/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2964 - val_loss: 0.4888 Epoch 32/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2971 - val_loss: 0.6767 Epoch 33/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.2964 - val_loss: 0.4657 Epoch 34/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.2943 - val_loss: 0.3589 Epoch 35/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2922 - val_loss: 0.3978 Epoch 36/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2909 - val_loss: 0.6603 Epoch 37/100 11610/11610 [==============================] - 0s 36us/sample - loss: 0.2923 - val_loss: 0.4158 Epoch 38/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2899 - val_loss: 0.3226 Epoch 39/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2873 - val_loss: 0.2782 Epoch 40/100 11610/11610 [==============================] - 0s 36us/sample - loss: 0.2866 - val_loss: 0.3285 Epoch 41/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2850 - val_loss: 0.4074 Epoch 42/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2858 - val_loss: 0.3352 Epoch 43/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2827 - val_loss: 0.2797 Epoch 44/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.2824 - val_loss: 0.7323 Epoch 45/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2838 - val_loss: 0.7834 Epoch 46/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.2888 - val_loss: 0.6376 Epoch 47/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2844 - val_loss: 0.5744 Epoch 48/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2819 - val_loss: 0.3071 Epoch 49/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2790 - val_loss: 0.3484 RandomizedSearchCV(cv=3, error_score='raise-deprecating', estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x11213ed68>, fit_params=None, iid='warn', n_iter=10, n_jobs=None, param_distributions={'n_hidden': [0, 1, 2, 3], 'n_neurons': array([ 1, 2, ..., 98, 99]), 'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a3bcea4a8>}, pre_dispatch='2*n_jobs', random_state=None, refit=True, return_train_score='warn', scoring=None, verbose=2) rnd_search_cv . best_params_ {'learning_rate': 0.004038022680351922, 'n_hidden': 2, 'n_neurons': 69} rnd_search_cv . best_score_ -0.3275334420977329 rnd_search_cv . best_estimator_ <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor at 0x1a3d960588> Evaluate the best model found on the test set. You can either use the best estimator's score() method, or get its underlying Keras model via its model attribute, and call this model's evaluate() method. Note that the estimator returns the negative mean square error (it's a score, not a loss, so higher is better). rnd_search_cv . score(X_test_scaled, y_test) 5160/5160 [==============================] - 0s 19us/sample - loss: 0.2968 -0.2968322378604911 model = rnd_search_cv . best_estimator_ . model model . evaluate(X_test_scaled, y_test) 5160/5160 [==============================] - 0s 15us/sample - loss: 0.2968 0.2968322378604911 Finally, save the best Keras model found. Tip : it is available via the best estimator's model attribute, and just need to call its save() method. model . save( \"my_fine_tuned_housing_model.h5\" )","title":"Neural Network-Tuning"},{"location":"tuning/tuning/#neural-nets-tuning","text":"% matplotlib inline % load_ext tensorboard import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import os import pandas as pd import sklearn import sys import tensorflow as tf from tensorflow import keras # tf.keras import time import seaborn as sns sns . set() print ( \"python\" , sys . version) for module in mpl, np, pd, sklearn, tf, keras: print (module . __name__ , module . __version__) python 3.7.1 (default, Dec 14 2018, 13:28:58) [Clang 4.0.1 (tags/RELEASE_401/final)] matplotlib 3.0.2 numpy 1.15.4 pandas 0.23.4 sklearn 0.20.1 tensorflow 2.0.0-beta0 tensorflow.python.keras.api._v2.keras 2.2.4-tf assert sys . version_info >= ( 3 , 5 ) # Python \u22653.5 required assert tf . __version__ >= \"2.0\" # TensorFlow \u22652.0 required","title":"Neural Nets :Tuning"},{"location":"tuning/tuning/#a-neural-net-for-regression","text":"Load the California housing dataset using sklearn.datasets.fetch_california_housing . This returns an object with a DESCR attribute describing the dataset, a data attribute with the input features, and a target attribute with the labels. The goal is to predict the price of houses in a district (a census block) given some stats about that district. This is a regression task (predicting values). from sklearn.datasets import fetch_california_housing housing = fetch_california_housing() print (housing . DESCR) .. _california_housing_dataset: California Housing dataset -------------------------- **Data Set Characteristics:** :Number of Instances: 20640 :Number of Attributes: 8 numeric, predictive attributes and the target :Attribute Information: - MedInc median income in block - HouseAge median house age in block - AveRooms average number of rooms - AveBedrms average number of bedrooms - Population block population - AveOccup average house occupancy - Latitude house block latitude - Longitude house block longitude :Missing Attribute Values: None This dataset was obtained from the StatLib repository. http://lib.stat.cmu.edu/datasets/ The target variable is the median house value for California districts. This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). It can be downloaded/loaded using the :func:`sklearn.datasets.fetch_california_housing` function. .. topic:: References - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions, Statistics and Probability Letters, 33 (1997) 291-297 housing . data . shape (20640, 8) housing . target . shape (20640,) Split the dataset into a training set, a validation set and a test set using Scikit-Learn's sklearn.model_selection.train_test_split() function. from sklearn.model_selection import train_test_split X_train_full, X_test, y_train_full, y_test = train_test_split(housing . data, housing . target, random_state = 42 ) X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state = 42 ) len (X_train), len (X_valid), len (X_test) (11610, 3870, 5160) Scale the input features (e.g., using a sklearn.preprocessing.StandardScaler ). Once again, don't forget that you should not fit the validation set or the test set, only the training set. from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train_scaled = scaler . fit_transform(X_train) X_valid_scaled = scaler . transform(X_valid) X_test_scaled = scaler . transform(X_test) Now build, train and evaluate a neural network to tackle this problem. Then use it to make predictions on the test set. Tips : * Since you are predicting a single value per district (the median house price), there should only be one neuron in the output layer. * Usually for regression tasks you don't want to use any activation function in the output layer (in some cases you may want to use \"relu\" or \"softplus\" if you want to constrain the predicted values to be positive, or \"sigmoid\" or \"tanh\" if you want to constrain the predicted values to 0-1 or -1-1). * A good loss function for regression is generally the \"mean_squared_error\" (aka \"mse\" ). When there are many outliers in your dataset, you may prefer to use the \"mean_absolute_error\" (aka \"mae\" ), which is a bit less precise but less sensitive to outliers. model = keras . models . Sequential([ keras . layers . Dense( 30 , activation = \"relu\" ,\\ input_shape = X_train . shape[ 1 :]), keras . layers . Dense( 1 ) ]) model . compile(loss = \"mean_squared_error\" ,\\ optimizer = keras . optimizers . SGD( 1e-3 )) callbacks = [keras . callbacks . EarlyStopping(patience = 10 )] history = model . fit(X_train_scaled,\\ y_train, validation_data = (X_valid_scaled, y_valid),\\ epochs = 100 , callbacks = callbacks) Train on 11610 samples, validate on 3870 samples Epoch 1/100 11610/11610 [==============================] - 0s 41us/sample - loss: 1.4373 - val_loss: 4.9912 Epoch 2/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.8371 - val_loss: 0.8625 Epoch 3/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.7555 - val_loss: 0.7340 Epoch 4/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.7094 - val_loss: 0.7095 Epoch 5/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6720 - val_loss: 0.6471 Epoch 6/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6392 - val_loss: 0.6335 Epoch 7/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6105 - val_loss: 0.5858 Epoch 8/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5852 - val_loss: 0.5641 Epoch 9/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5626 - val_loss: 0.5456 Epoch 10/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5425 - val_loss: 0.5173 Epoch 11/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5253 - val_loss: 0.5070 Epoch 12/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5096 - val_loss: 0.4918 Epoch 13/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4964 - val_loss: 0.4793 Epoch 14/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4848 - val_loss: 0.4752 Epoch 15/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4743 - val_loss: 0.4597 Epoch 16/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4658 - val_loss: 0.4685 Epoch 17/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4586 - val_loss: 0.4433 Epoch 18/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4520 - val_loss: 0.4364 Epoch 19/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4464 - val_loss: 0.4511 Epoch 20/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4418 - val_loss: 0.4285 Epoch 21/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4377 - val_loss: 0.4347 Epoch 22/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4341 - val_loss: 0.4288 Epoch 23/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4309 - val_loss: 0.4265 Epoch 24/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4281 - val_loss: 0.4219 Epoch 25/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4256 - val_loss: 0.4368 Epoch 26/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4233 - val_loss: 0.4404 Epoch 27/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4210 - val_loss: 0.4438 Epoch 28/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4192 - val_loss: 0.4358 Epoch 29/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4172 - val_loss: 0.4398 Epoch 30/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4154 - val_loss: 0.4463 Epoch 31/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4141 - val_loss: 0.4247 Epoch 32/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4126 - val_loss: 0.4308 Epoch 33/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4112 - val_loss: 0.4330 Epoch 34/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4099 - val_loss: 0.4273 model . evaluate(X_test_scaled, y_test) 5160/5160 [==============================] - 0s 15us/sample - loss: 0.4072 0.40716409780258356 model . predict(X_test_scaled) array([[0.6690421], [1.6140628], [3.5537164], ..., [1.504305 ], [2.528902 ], [3.6941016]], dtype=float32) def plot_learning_curves (history): pd . DataFrame(history . history) . plot(figsize = ( 8 , 5 )) plt . grid( True ) plt . gca() . set_ylim( 0 , 1 ) plt . show() plot_learning_curves(history)","title":"A neural net for regression"},{"location":"tuning/tuning/#hyperparameter-search","text":"Try training your model multiple times, with different a learning rate each time (e.g., 1e-4, 3e-4, 1e-3, 3e-3, 3e-2), and compare the learning curves. For this, you need to create a keras.optimizers.SGD optimizer and specify the learning_rate in its constructor, then pass this SGD instance to the compile() method using the optimizer argument. learning_rates = [ 1e-4 , 3e-4 , 1e-3 , 3e-3 , 1e-2 , 3e-2 ] histories = [] for learning_rate in learning_rates: model = keras . models . Sequential([ keras . layers . Dense( 30 , activation = \"relu\" , input_shape = X_train . shape[ 1 :]), keras . layers . Dense( 1 ) ]) optimizer = keras . optimizers . SGD(learning_rate) model . compile(loss = \"mean_squared_error\" , optimizer = optimizer) callbacks = [keras . callbacks . EarlyStopping(patience = 10 )] history = model . fit(X_train_scaled, y_train, validation_data = (X_valid_scaled, y_valid), epochs = 100 , callbacks = callbacks) histories . append(history) Train on 11610 samples, validate on 3870 samples Epoch 1/100 11610/11610 [==============================] - 1s 46us/sample - loss: 5.4083 - val_loss: 4.2292 Epoch 2/100 11610/11610 [==============================] - 0s 32us/sample - loss: 3.7043 - val_loss: 3.6743 Epoch 3/100 11610/11610 [==============================] - 0s 33us/sample - loss: 2.6746 - val_loss: 4.1190 Epoch 4/100 11610/11610 [==============================] - 0s 32us/sample - loss: 2.0397 - val_loss: 4.4212 Epoch 5/100 11610/11610 [==============================] - 0s 31us/sample - loss: 1.6337 - val_loss: 4.5097 Epoch 6/100 11610/11610 [==============================] - 0s 31us/sample - loss: 1.3677 - val_loss: 4.2815 Epoch 7/100 11610/11610 [==============================] - 0s 31us/sample - loss: 1.1891 - val_loss: 3.9109 Epoch 8/100 11610/11610 [==============================] - 0s 32us/sample - loss: 1.0654 - val_loss: 3.4672 Epoch 9/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.9781 - val_loss: 2.9750 Epoch 10/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.9146 - val_loss: 2.5285 Epoch 11/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.8676 - val_loss: 2.1245 Epoch 12/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.8319 - val_loss: 1.7892 Epoch 13/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.8042 - val_loss: 1.5134 Epoch 14/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.7823 - val_loss: 1.2938 Epoch 15/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.7646 - val_loss: 1.1226 Epoch 16/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.7500 - val_loss: 0.9907 Epoch 17/100 11610/11610 [==============================] - 0s 36us/sample - loss: 0.7377 - val_loss: 0.8902 Epoch 18/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.7272 - val_loss: 0.8149 Epoch 19/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.7180 - val_loss: 0.7574 Epoch 20/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.7098 - val_loss: 0.7162 Epoch 21/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.7025 - val_loss: 0.6860 Epoch 22/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.6958 - val_loss: 0.6657 Epoch 23/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6897 - val_loss: 0.6500 Epoch 24/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.6839 - val_loss: 0.6394 Epoch 25/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.6786 - val_loss: 0.6314 Epoch 26/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6734 - val_loss: 0.6258 Epoch 27/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.6686 - val_loss: 0.6221 Epoch 28/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.6640 - val_loss: 0.6193 Epoch 29/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.6595 - val_loss: 0.6173 Epoch 30/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6553 - val_loss: 0.6154 Epoch 31/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6512 - val_loss: 0.6134 Epoch 32/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6472 - val_loss: 0.6118 Epoch 33/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6433 - val_loss: 0.6101 Epoch 34/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6396 - val_loss: 0.6085 Epoch 35/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6360 - val_loss: 0.6073 Epoch 36/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6325 - val_loss: 0.6056 Epoch 37/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6290 - val_loss: 0.6041 Epoch 38/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6256 - val_loss: 0.6026 Epoch 39/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6224 - val_loss: 0.6003 Epoch 40/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6192 - val_loss: 0.5986 Epoch 41/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6160 - val_loss: 0.5954 Epoch 42/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6130 - val_loss: 0.5924 Epoch 43/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6100 - val_loss: 0.5901 Epoch 44/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6070 - val_loss: 0.5885 Epoch 45/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6042 - val_loss: 0.5857 Epoch 46/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.6013 - val_loss: 0.5834 Epoch 47/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.5986 - val_loss: 0.5801 Epoch 48/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5959 - val_loss: 0.5777 Epoch 49/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5932 - val_loss: 0.5749 Epoch 50/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5906 - val_loss: 0.5728 Epoch 51/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5881 - val_loss: 0.5694 Epoch 52/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5856 - val_loss: 0.5663 Epoch 53/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5832 - val_loss: 0.5633 Epoch 54/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5807 - val_loss: 0.5614 Epoch 55/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5784 - val_loss: 0.5595 Epoch 56/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5761 - val_loss: 0.5578 Epoch 57/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5738 - val_loss: 0.5548 Epoch 58/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5716 - val_loss: 0.5518 Epoch 59/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5694 - val_loss: 0.5495 Epoch 60/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5672 - val_loss: 0.5472 Epoch 61/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5651 - val_loss: 0.5446 Epoch 62/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5630 - val_loss: 0.5427 Epoch 63/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5609 - val_loss: 0.5390 Epoch 64/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5589 - val_loss: 0.5361 Epoch 65/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5569 - val_loss: 0.5333 Epoch 66/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5549 - val_loss: 0.5312 Epoch 67/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5530 - val_loss: 0.5285 Epoch 68/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5511 - val_loss: 0.5271 Epoch 69/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5492 - val_loss: 0.5245 Epoch 70/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5474 - val_loss: 0.5217 Epoch 71/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5456 - val_loss: 0.5199 Epoch 72/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5438 - val_loss: 0.5180 Epoch 73/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5420 - val_loss: 0.5161 Epoch 74/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5403 - val_loss: 0.5140 Epoch 75/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5386 - val_loss: 0.5117 Epoch 76/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5369 - val_loss: 0.5099 Epoch 77/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5353 - val_loss: 0.5079 Epoch 78/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5336 - val_loss: 0.5062 Epoch 79/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5320 - val_loss: 0.5043 Epoch 80/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5305 - val_loss: 0.5026 Epoch 81/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5289 - val_loss: 0.5010 Epoch 82/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5274 - val_loss: 0.4988 Epoch 83/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5259 - val_loss: 0.4967 Epoch 84/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5244 - val_loss: 0.4952 Epoch 85/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5230 - val_loss: 0.4931 Epoch 86/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5215 - val_loss: 0.4911 Epoch 87/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5201 - val_loss: 0.4896 Epoch 88/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5187 - val_loss: 0.4880 Epoch 89/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5173 - val_loss: 0.4864 Epoch 90/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5160 - val_loss: 0.4846 Epoch 91/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5146 - val_loss: 0.4829 Epoch 92/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5133 - val_loss: 0.4816 Epoch 93/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5120 - val_loss: 0.4802 Epoch 94/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5107 - val_loss: 0.4786 Epoch 95/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5095 - val_loss: 0.4772 Epoch 96/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5082 - val_loss: 0.4759 Epoch 97/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5070 - val_loss: 0.4743 Epoch 98/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5058 - val_loss: 0.4728 Epoch 99/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5046 - val_loss: 0.4715 Epoch 100/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5034 - val_loss: 0.4702 Train on 11610 samples, validate on 3870 samples Epoch 1/100 11610/11610 [==============================] - 0s 39us/sample - loss: 4.2273 - val_loss: 2.7904 Epoch 2/100 11610/11610 [==============================] - 0s 30us/sample - loss: 2.0515 - val_loss: 2.1264 Epoch 3/100 11610/11610 [==============================] - 0s 31us/sample - loss: 1.3341 - val_loss: 1.4705 Epoch 4/100 11610/11610 [==============================] - 0s 31us/sample - loss: 1.0088 - val_loss: 1.0015 Epoch 5/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.8489 - val_loss: 0.7896 Epoch 6/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.7662 - val_loss: 0.7104 Epoch 7/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.7200 - val_loss: 0.6770 Epoch 8/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6912 - val_loss: 0.6601 Epoch 9/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6708 - val_loss: 0.6464 Epoch 10/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.6548 - val_loss: 0.6345 Epoch 11/100 11610/11610 [==============================] - 0s 39us/sample - loss: 0.6411 - val_loss: 0.6242 Epoch 12/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6291 - val_loss: 0.6284 Epoch 13/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6183 - val_loss: 0.6130 Epoch 14/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6082 - val_loss: 0.5974 Epoch 15/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5988 - val_loss: 0.6021 Epoch 16/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5903 - val_loss: 0.5907 Epoch 17/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5823 - val_loss: 0.5736 Epoch 18/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5747 - val_loss: 0.5723 Epoch 19/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5676 - val_loss: 0.5652 Epoch 20/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5610 - val_loss: 0.5589 Epoch 21/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5547 - val_loss: 0.5514 Epoch 22/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5487 - val_loss: 0.5474 Epoch 23/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5432 - val_loss: 0.5348 Epoch 24/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5379 - val_loss: 0.5247 Epoch 25/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5329 - val_loss: 0.5162 Epoch 26/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5282 - val_loss: 0.5123 Epoch 27/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5236 - val_loss: 0.5157 Epoch 28/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5194 - val_loss: 0.5115 Epoch 29/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5154 - val_loss: 0.5064 Epoch 30/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5116 - val_loss: 0.4988 Epoch 31/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5079 - val_loss: 0.5025 Epoch 32/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5045 - val_loss: 0.4979 Epoch 33/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5012 - val_loss: 0.4884 Epoch 34/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4981 - val_loss: 0.4813 Epoch 35/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4951 - val_loss: 0.4756 Epoch 36/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4922 - val_loss: 0.4775 Epoch 37/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4895 - val_loss: 0.4719 Epoch 38/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4869 - val_loss: 0.4719 Epoch 39/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4844 - val_loss: 0.4638 Epoch 40/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4821 - val_loss: 0.4610 Epoch 41/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4798 - val_loss: 0.4614 Epoch 42/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4777 - val_loss: 0.4602 Epoch 43/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4756 - val_loss: 0.4555 Epoch 44/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4735 - val_loss: 0.4536 Epoch 45/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4716 - val_loss: 0.4511 Epoch 46/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4697 - val_loss: 0.4465 Epoch 47/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4679 - val_loss: 0.4472 Epoch 48/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.4662 - val_loss: 0.4427 Epoch 49/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4645 - val_loss: 0.4379 Epoch 50/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4629 - val_loss: 0.4380 Epoch 51/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4613 - val_loss: 0.4363 Epoch 52/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4598 - val_loss: 0.4352 Epoch 53/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4583 - val_loss: 0.4353 Epoch 54/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4569 - val_loss: 0.4346 Epoch 55/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4555 - val_loss: 0.4300 Epoch 56/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4541 - val_loss: 0.4291 Epoch 57/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4529 - val_loss: 0.4283 Epoch 58/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4516 - val_loss: 0.4252 Epoch 59/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4503 - val_loss: 0.4226 Epoch 60/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4492 - val_loss: 0.4231 Epoch 61/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4479 - val_loss: 0.4219 Epoch 62/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4468 - val_loss: 0.4204 Epoch 63/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4456 - val_loss: 0.4184 Epoch 64/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4445 - val_loss: 0.4180 Epoch 65/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4434 - val_loss: 0.4167 Epoch 66/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4424 - val_loss: 0.4151 Epoch 67/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4413 - val_loss: 0.4144 Epoch 68/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4403 - val_loss: 0.4138 Epoch 69/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4393 - val_loss: 0.4112 Epoch 70/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4383 - val_loss: 0.4103 Epoch 71/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4373 - val_loss: 0.4097 Epoch 72/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4364 - val_loss: 0.4079 Epoch 73/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4355 - val_loss: 0.4084 Epoch 74/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4345 - val_loss: 0.4061 Epoch 75/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4337 - val_loss: 0.4057 Epoch 76/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4328 - val_loss: 0.4049 Epoch 77/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.4320 - val_loss: 0.4044 Epoch 78/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4311 - val_loss: 0.4028 Epoch 79/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4304 - val_loss: 0.4026 Epoch 80/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4296 - val_loss: 0.4016 Epoch 81/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4288 - val_loss: 0.4008 Epoch 82/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4280 - val_loss: 0.3999 Epoch 83/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4273 - val_loss: 0.3992 Epoch 84/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4265 - val_loss: 0.3986 Epoch 85/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4258 - val_loss: 0.3975 Epoch 86/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4251 - val_loss: 0.3971 Epoch 87/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4244 - val_loss: 0.3967 Epoch 88/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4237 - val_loss: 0.3961 Epoch 89/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4230 - val_loss: 0.3950 Epoch 90/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4224 - val_loss: 0.3948 Epoch 91/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4217 - val_loss: 0.3940 Epoch 92/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4211 - val_loss: 0.3933 Epoch 93/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4204 - val_loss: 0.3941 Epoch 94/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4198 - val_loss: 0.3928 Epoch 95/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4192 - val_loss: 0.3917 Epoch 96/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4186 - val_loss: 0.3913 Epoch 97/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4180 - val_loss: 0.3908 Epoch 98/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4174 - val_loss: 0.3902 Epoch 99/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4169 - val_loss: 0.3894 Epoch 100/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4163 - val_loss: 0.3892 Train on 11610 samples, validate on 3870 samples Epoch 1/100 11610/11610 [==============================] - 0s 38us/sample - loss: 2.4134 - val_loss: 1.3718 Epoch 2/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.7545 - val_loss: 0.6598 Epoch 3/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6376 - val_loss: 0.5935 Epoch 4/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6023 - val_loss: 0.5817 Epoch 5/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5779 - val_loss: 0.5530 Epoch 6/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.5579 - val_loss: 0.5253 Epoch 7/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5411 - val_loss: 0.5445 Epoch 8/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5265 - val_loss: 0.5110 Epoch 9/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5136 - val_loss: 0.5042 Epoch 10/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.5024 - val_loss: 0.4798 Epoch 11/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4927 - val_loss: 0.4648 Epoch 12/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4840 - val_loss: 0.4726 Epoch 13/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4764 - val_loss: 0.4456 Epoch 14/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4700 - val_loss: 0.4380 Epoch 15/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4644 - val_loss: 0.4392 Epoch 16/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4593 - val_loss: 0.4401 Epoch 17/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4548 - val_loss: 0.4267 Epoch 18/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4508 - val_loss: 0.4354 Epoch 19/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4472 - val_loss: 0.4276 Epoch 20/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4437 - val_loss: 0.4317 Epoch 21/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4407 - val_loss: 0.4150 Epoch 22/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4379 - val_loss: 0.4173 Epoch 23/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4351 - val_loss: 0.4187 Epoch 24/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4328 - val_loss: 0.4153 Epoch 25/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4304 - val_loss: 0.4099 Epoch 26/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4283 - val_loss: 0.4065 Epoch 27/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4261 - val_loss: 0.4083 Epoch 28/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4242 - val_loss: 0.4039 Epoch 29/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4224 - val_loss: 0.3960 Epoch 30/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4207 - val_loss: 0.3992 Epoch 31/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4188 - val_loss: 0.3920 Epoch 32/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4174 - val_loss: 0.4008 Epoch 33/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4159 - val_loss: 0.3919 Epoch 34/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.4143 - val_loss: 0.4006 Epoch 35/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4129 - val_loss: 0.3990 Epoch 36/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4116 - val_loss: 0.3882 Epoch 37/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4104 - val_loss: 0.3860 Epoch 38/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4090 - val_loss: 0.3875 Epoch 39/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4079 - val_loss: 0.3806 Epoch 40/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4068 - val_loss: 0.3814 Epoch 41/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4056 - val_loss: 0.3827 Epoch 42/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4045 - val_loss: 0.3786 Epoch 43/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4035 - val_loss: 0.3812 Epoch 44/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4024 - val_loss: 0.3789 Epoch 45/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4015 - val_loss: 0.3787 Epoch 46/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.4005 - val_loss: 0.3805 Epoch 47/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3995 - val_loss: 0.3754 Epoch 48/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3987 - val_loss: 0.3753 Epoch 49/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3977 - val_loss: 0.3772 Epoch 50/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3967 - val_loss: 0.3706 Epoch 51/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3958 - val_loss: 0.3808 Epoch 52/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3950 - val_loss: 0.3865 Epoch 53/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3942 - val_loss: 0.3874 Epoch 54/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3933 - val_loss: 0.3936 Epoch 55/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3927 - val_loss: 0.3897 Epoch 56/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3919 - val_loss: 0.3940 Epoch 57/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3912 - val_loss: 0.3777 Epoch 58/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3905 - val_loss: 0.3831 Epoch 59/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3898 - val_loss: 0.3797 Epoch 60/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3892 - val_loss: 0.3834 Train on 11610 samples, validate on 3870 samples Epoch 1/100 11610/11610 [==============================] - 0s 39us/sample - loss: 1.3072 - val_loss: 5.4533 Epoch 2/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.6424 - val_loss: 2.8452 Epoch 3/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.5206 - val_loss: 2.5899 Epoch 4/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4927 - val_loss: 6.1177 Epoch 5/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4958 - val_loss: 7.5540 Epoch 6/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4943 - val_loss: 3.3703 Epoch 7/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4485 - val_loss: 0.7394 Epoch 8/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4171 - val_loss: 0.4349 Epoch 9/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4102 - val_loss: 0.4125 Epoch 10/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4055 - val_loss: 0.3867 Epoch 11/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4017 - val_loss: 0.3774 Epoch 12/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3982 - val_loss: 0.3820 Epoch 13/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3944 - val_loss: 0.4447 Epoch 14/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3926 - val_loss: 0.3701 Epoch 15/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3900 - val_loss: 0.3717 Epoch 16/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3876 - val_loss: 0.3667 Epoch 17/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3851 - val_loss: 0.3589 Epoch 18/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3829 - val_loss: 0.3570 Epoch 19/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3807 - val_loss: 0.3977 Epoch 20/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3802 - val_loss: 0.4129 Epoch 21/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3783 - val_loss: 0.4629 Epoch 22/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3772 - val_loss: 0.3535 Epoch 23/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3749 - val_loss: 0.3909 Epoch 24/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3742 - val_loss: 0.3567 Epoch 25/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3726 - val_loss: 0.3504 Epoch 26/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3710 - val_loss: 0.4323 Epoch 27/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3716 - val_loss: 0.3593 Epoch 28/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3697 - val_loss: 0.3478 Epoch 29/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3688 - val_loss: 0.3642 Epoch 30/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3681 - val_loss: 0.3518 Epoch 31/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3669 - val_loss: 0.3702 Epoch 32/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3662 - val_loss: 0.3608 Epoch 33/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3656 - val_loss: 0.3444 Epoch 34/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3649 - val_loss: 0.3676 Epoch 35/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3645 - val_loss: 0.4251 Epoch 36/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3644 - val_loss: 0.6289 Epoch 37/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3664 - val_loss: 0.4964 Epoch 38/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3626 - val_loss: 0.9082 Epoch 39/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3679 - val_loss: 0.7426 Epoch 40/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3645 - val_loss: 0.8972 Epoch 41/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3665 - val_loss: 0.6627 Epoch 42/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3618 - val_loss: 0.8439 Epoch 43/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3624 - val_loss: 0.8183 Train on 11610 samples, validate on 3870 samples Epoch 1/100 11610/11610 [==============================] - 0s 38us/sample - loss: 0.7428 - val_loss: 0.5310 Epoch 2/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4772 - val_loss: 0.7309 Epoch 3/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4394 - val_loss: 0.4341 Epoch 4/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4377 - val_loss: 0.9035 Epoch 5/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4206 - val_loss: 2.1735 Epoch 6/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4003 - val_loss: 3.6124 Epoch 7/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4402 - val_loss: 5.5876 Epoch 8/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.4307 - val_loss: 1.7575 Epoch 9/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3921 - val_loss: 0.3940 Epoch 10/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3885 - val_loss: 0.4234 Epoch 11/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3743 - val_loss: 0.4203 Epoch 12/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3730 - val_loss: 0.4551 Epoch 13/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3709 - val_loss: 0.3796 Epoch 14/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3668 - val_loss: 0.3914 Epoch 15/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3675 - val_loss: 0.3904 Epoch 16/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3647 - val_loss: 0.4041 Epoch 17/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3670 - val_loss: 0.4419 Epoch 18/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3622 - val_loss: 0.3666 Epoch 19/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3594 - val_loss: 0.3740 Epoch 20/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3613 - val_loss: 0.3728 Epoch 21/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3586 - val_loss: 0.4143 Epoch 22/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3552 - val_loss: 0.3474 Epoch 23/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3556 - val_loss: 0.3595 Epoch 24/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3694 - val_loss: 0.3725 Epoch 25/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3554 - val_loss: 0.3636 Epoch 26/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3517 - val_loss: 0.3741 Epoch 27/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3532 - val_loss: 0.3486 Epoch 28/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3495 - val_loss: 0.3586 Epoch 29/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3587 - val_loss: 0.3759 Epoch 30/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3484 - val_loss: 0.3318 Epoch 31/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3463 - val_loss: 0.3511 Epoch 32/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3460 - val_loss: 0.3682 Epoch 33/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3453 - val_loss: 0.5032 Epoch 34/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3471 - val_loss: 0.3380 Epoch 35/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3419 - val_loss: 0.3651 Epoch 36/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3407 - val_loss: 0.3318 Epoch 37/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3433 - val_loss: 0.3625 Epoch 38/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3430 - val_loss: 0.3544 Epoch 39/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3370 - val_loss: 0.3249 Epoch 40/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3412 - val_loss: 0.3564 Epoch 41/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3378 - val_loss: 0.3569 Epoch 42/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3343 - val_loss: 0.3182 Epoch 43/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3381 - val_loss: 0.3800 Epoch 44/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3325 - val_loss: 0.3294 Epoch 45/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3336 - val_loss: 0.3423 Epoch 46/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3324 - val_loss: 0.3299 Epoch 47/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3344 - val_loss: 0.3148 Epoch 48/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3298 - val_loss: 0.3498 Epoch 49/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3413 - val_loss: 0.3286 Epoch 50/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3271 - val_loss: 0.5932 Epoch 51/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3313 - val_loss: 0.3845 Epoch 52/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3261 - val_loss: 0.6172 Epoch 53/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3282 - val_loss: 0.6108 Epoch 54/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3302 - val_loss: 0.3378 Epoch 55/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3258 - val_loss: 0.4014 Epoch 56/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3244 - val_loss: 0.3276 Epoch 57/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3222 - val_loss: 0.4102 Train on 11610 samples, validate on 3870 samples Epoch 1/100 11610/11610 [==============================] - 0s 38us/sample - loss: 0.6626 - val_loss: 1.9500 Epoch 2/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4895 - val_loss: 0.4488 Epoch 3/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4463 - val_loss: 4.8228 Epoch 4/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3891 - val_loss: 8.5697 Epoch 5/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4137 - val_loss: 0.3434 Epoch 6/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3676 - val_loss: 0.3476 Epoch 7/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3674 - val_loss: 0.4051 Epoch 8/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.4190 - val_loss: 0.3482 Epoch 9/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3666 - val_loss: 0.3420 Epoch 10/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3602 - val_loss: 0.3352 Epoch 11/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3579 - val_loss: 0.3361 Epoch 12/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3557 - val_loss: 0.3494 Epoch 13/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3565 - val_loss: 0.3343 Epoch 14/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3527 - val_loss: 0.3270 Epoch 15/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3491 - val_loss: 0.3259 Epoch 16/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3452 - val_loss: 0.3255 Epoch 17/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3875 - val_loss: 0.3663 Epoch 18/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3572 - val_loss: 0.3324 Epoch 19/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3551 - val_loss: 0.3254 Epoch 20/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3532 - val_loss: 0.3350 Epoch 21/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3404 - val_loss: 0.3183 Epoch 22/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3382 - val_loss: 0.4119 Epoch 23/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3356 - val_loss: 0.3146 Epoch 24/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3334 - val_loss: 0.3138 Epoch 25/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3315 - val_loss: 0.3144 Epoch 26/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3301 - val_loss: 0.3158 Epoch 27/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3263 - val_loss: 0.3053 Epoch 28/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3251 - val_loss: 0.3817 Epoch 29/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3245 - val_loss: 0.3011 Epoch 30/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3224 - val_loss: 0.3064 Epoch 31/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3211 - val_loss: 0.3338 Epoch 32/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3206 - val_loss: 0.2942 Epoch 33/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3185 - val_loss: 0.3047 Epoch 34/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3182 - val_loss: 0.2964 Epoch 35/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3240 - val_loss: 0.2974 Epoch 36/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3160 - val_loss: 0.2945 Epoch 37/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3143 - val_loss: 0.2956 Epoch 38/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3138 - val_loss: 0.2969 Epoch 39/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3122 - val_loss: 0.2938 Epoch 40/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3106 - val_loss: 0.2912 Epoch 41/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3121 - val_loss: 0.3569 Epoch 42/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3137 - val_loss: 0.2975 Epoch 43/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3093 - val_loss: 0.2939 Epoch 44/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3078 - val_loss: 0.2913 Epoch 45/100 11610/11610 [==============================] - 0s 29us/sample - loss: 0.3082 - val_loss: 0.2980 Epoch 46/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3084 - val_loss: 0.2925 Epoch 47/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3104 - val_loss: 0.2883 Epoch 48/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3075 - val_loss: 0.2887 Epoch 49/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3050 - val_loss: 0.3013 Epoch 50/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3027 - val_loss: 0.3024 Epoch 51/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3049 - val_loss: 0.3493 Epoch 52/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3023 - val_loss: 0.3955 Epoch 53/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3038 - val_loss: 0.2910 Epoch 54/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3037 - val_loss: 0.3976 Epoch 55/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3027 - val_loss: 0.2860 Epoch 56/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3010 - val_loss: 0.2943 Epoch 57/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3011 - val_loss: 0.2864 Epoch 58/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3035 - val_loss: 0.3168 Epoch 59/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3008 - val_loss: 0.2943 Epoch 60/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3017 - val_loss: 0.2994 Epoch 61/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3009 - val_loss: 0.3375 Epoch 62/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3075 - val_loss: 0.3327 Epoch 63/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3022 - val_loss: 0.3012 Epoch 64/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3009 - val_loss: 0.3123 Epoch 65/100 11610/11610 [==============================] - 0s 30us/sample - loss: 0.3008 - val_loss: 0.2923 for learning_rate, history in zip (learning_rates, histories): print ( \"Learning rate:\" , learning_rate) plot_learning_curves(history) Learning rate: 0.0001 Learning rate: 0.0003 Learning rate: 0.001 Learning rate: 0.003 Learning rate: 0.01 Learning rate: 0.03 Let's look at a more sophisticated way to tune hyperparameters. Create a build_model() function that takes three arguments, n_hidden , n_neurons , learning_rate , and builds, compiles and returns a model with the given number of hidden layers, the given number of neurons and the given learning rate. It is good practice to give a reasonable default value to each argument. def build_model (n_hidden = 1 , n_neurons = 30 , learning_rate = 3e-3 ): model = keras . models . Sequential() options = { \"input_shape\" : X_train . shape[ 1 :]} for layer in range (n_hidden + 1 ): model . add(keras . layers . Dense(n_neurons, activation = \"relu\" , ** options)) options = {} model . add(keras . layers . Dense( 1 , ** options)) optimizer = keras . optimizers . SGD(learning_rate) model . compile(loss = \"mse\" , optimizer = optimizer) return model Create a keras.wrappers.scikit_learn.KerasRegressor and pass the build_model function to the constructor. This gives you a Scikit-Learn compatible predictor. Try training it and using it to make predictions. Note that you can pass the n_epochs , callbacks and validation_data to the fit() method. keras_reg = keras . wrappers . scikit_learn . KerasRegressor(build_model) keras_reg . fit(X_train_scaled, y_train, epochs = 100 , validation_data = (X_valid_scaled, y_valid), callbacks = [keras . callbacks . EarlyStopping(patience = 10 )]) Train on 11610 samples, validate on 3870 samples Epoch 1/100 11610/11610 [==============================] - 0s 41us/sample - loss: 0.9440 - val_loss: 9.4997 Epoch 2/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.6070 - val_loss: 34.8291 Epoch 3/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.6444 - val_loss: 2.0556 Epoch 4/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.4426 - val_loss: 0.4424 Epoch 5/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.4072 - val_loss: 0.3828 Epoch 6/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3978 - val_loss: 0.3810 Epoch 7/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3908 - val_loss: 0.3813 Epoch 8/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3859 - val_loss: 0.3877 Epoch 9/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3821 - val_loss: 0.3704 Epoch 10/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3788 - val_loss: 0.3779 Epoch 11/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3761 - val_loss: 0.3823 Epoch 12/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3742 - val_loss: 0.3783 Epoch 13/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3715 - val_loss: 0.3787 Epoch 14/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3689 - val_loss: 0.3870 Epoch 15/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3677 - val_loss: 0.3929 Epoch 16/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.3653 - val_loss: 0.3907 Epoch 17/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3632 - val_loss: 0.3841 Epoch 18/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.3628 - val_loss: 0.3691 Epoch 19/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3606 - val_loss: 0.3888 Epoch 20/100 11610/11610 [==============================] - 0s 31us/sample - loss: 0.3593 - val_loss: 0.3602 Epoch 21/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3577 - val_loss: 0.3654 Epoch 22/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3569 - val_loss: 0.3606 Epoch 23/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3551 - val_loss: 0.4164 Epoch 24/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3544 - val_loss: 0.3564 Epoch 25/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.3533 - val_loss: 0.3717 Epoch 26/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3522 - val_loss: 0.3663 Epoch 27/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3516 - val_loss: 0.3929 Epoch 28/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3508 - val_loss: 0.3429 Epoch 29/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3493 - val_loss: 0.3521 Epoch 30/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.3488 - val_loss: 0.3937 Epoch 31/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3478 - val_loss: 0.3611 Epoch 32/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3467 - val_loss: 0.3792 Epoch 33/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.3455 - val_loss: 0.3740 Epoch 34/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.3452 - val_loss: 0.3629 Epoch 35/100 11610/11610 [==============================] - 0s 33us/sample - loss: 0.3443 - val_loss: 0.3792 Epoch 36/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3432 - val_loss: 0.3710 Epoch 37/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3428 - val_loss: 0.3737 Epoch 38/100 11610/11610 [==============================] - 0s 32us/sample - loss: 0.3420 - val_loss: 0.3575 <tensorflow.python.keras.callbacks.History at 0x1a3b91d9b0> keras_reg . predict(X_test_scaled) array([0.76681733, 1.8267894 , 4.295369 , ..., 1.3182094 , 2.648156 , 4.0545692 ], dtype=float32) Use a sklearn.model_selection.RandomizedSearchCV to search the hyperparameter space of your KerasRegressor . from scipy.stats import reciprocal param_distribs = { \"n_hidden\" : [ 0 , 1 , 2 , 3 ], \"n_neurons\" : np . arange( 1 , 100 ), \"learning_rate\" : reciprocal( 3e-4 , 3e-2 ), } from sklearn.model_selection import RandomizedSearchCV rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter = 10 , cv = 3 , verbose = 2 ) rnd_search_cv . fit(X_train_scaled, y_train, epochs = 100 , validation_data = (X_valid_scaled, y_valid), callbacks = [keras . callbacks . EarlyStopping(patience = 10 )]) Fitting 3 folds for each of 10 candidates, totalling 30 fits [CV] learning_rate=0.0023302047292153563, n_hidden=2, n_neurons=41 ... [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 61us/sample - loss: 1.6032 - val_loss: 1.7070 Epoch 2/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.6348 - val_loss: 0.5833 Epoch 3/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.5404 - val_loss: 0.4916 Epoch 4/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4899 - val_loss: 0.4363 Epoch 5/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4539 - val_loss: 0.4108 Epoch 6/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4299 - val_loss: 0.4061 Epoch 7/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4147 - val_loss: 0.4094 Epoch 8/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4044 - val_loss: 0.4162 Epoch 9/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3966 - val_loss: 0.4136 Epoch 10/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3907 - val_loss: 0.4069 Epoch 11/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3860 - val_loss: 0.4050 Epoch 12/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3811 - val_loss: 0.4054 Epoch 13/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3777 - val_loss: 0.4089 Epoch 14/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3745 - val_loss: 0.4118 Epoch 15/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3712 - val_loss: 0.4270 Epoch 16/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3682 - val_loss: 0.3820 Epoch 17/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3652 - val_loss: 0.3883 Epoch 18/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3628 - val_loss: 0.4042 Epoch 19/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3605 - val_loss: 0.3741 Epoch 20/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3581 - val_loss: 0.3736 Epoch 21/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3557 - val_loss: 0.3662 Epoch 22/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3535 - val_loss: 0.3517 Epoch 23/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3511 - val_loss: 0.3583 Epoch 24/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3493 - val_loss: 0.3570 Epoch 25/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3477 - val_loss: 0.3631 Epoch 26/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3463 - val_loss: 0.3423 Epoch 27/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3446 - val_loss: 0.3695 Epoch 28/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3420 - val_loss: 0.3397 Epoch 29/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3414 - val_loss: 0.3492 Epoch 30/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3404 - val_loss: 0.3500 Epoch 31/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3385 - val_loss: 0.3407 Epoch 32/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3379 - val_loss: 0.3365 Epoch 33/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3365 - val_loss: 0.3482 Epoch 34/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3353 - val_loss: 0.3435 Epoch 35/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3334 - val_loss: 0.3367 Epoch 36/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3329 - val_loss: 0.3609 Epoch 37/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3318 - val_loss: 0.3485 Epoch 38/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3304 - val_loss: 0.3455 Epoch 39/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3301 - val_loss: 0.3264 Epoch 40/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3291 - val_loss: 0.3259 Epoch 41/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3274 - val_loss: 0.3431 Epoch 42/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3265 - val_loss: 0.3371 Epoch 43/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3258 - val_loss: 0.3332 Epoch 44/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3249 - val_loss: 0.3310 Epoch 45/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3240 - val_loss: 0.3318 Epoch 46/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3236 - val_loss: 0.3268 Epoch 47/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3222 - val_loss: 0.3270 Epoch 48/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3215 - val_loss: 0.3296 Epoch 49/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3204 - val_loss: 0.3274 Epoch 50/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3198 - val_loss: 0.3180 Epoch 51/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3185 - val_loss: 0.3408 Epoch 52/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3177 - val_loss: 0.3154 Epoch 53/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3173 - val_loss: 0.3288 Epoch 54/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3154 - val_loss: 0.3332 Epoch 55/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3148 - val_loss: 0.3334 Epoch 56/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3145 - val_loss: 0.3171 Epoch 57/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3140 - val_loss: 0.3272 Epoch 58/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3129 - val_loss: 0.3203 Epoch 59/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3125 - val_loss: 0.3195 Epoch 60/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3119 - val_loss: 0.3197 Epoch 61/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3101 - val_loss: 0.3084 Epoch 62/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3102 - val_loss: 0.3236 Epoch 63/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3092 - val_loss: 0.3236 Epoch 64/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3089 - val_loss: 0.3137 Epoch 65/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3076 - val_loss: 0.3309 Epoch 66/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3071 - val_loss: 0.3164 Epoch 67/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3066 - val_loss: 0.3116 Epoch 68/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3053 - val_loss: 0.3275 Epoch 69/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3058 - val_loss: 0.3333 Epoch 70/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3058 - val_loss: 0.3057 Epoch 71/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3042 - val_loss: 0.3118 Epoch 72/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3034 - val_loss: 0.3156 Epoch 73/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3023 - val_loss: 0.3021 Epoch 74/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3018 - val_loss: 0.3117 Epoch 75/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3012 - val_loss: 0.3137 Epoch 76/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3009 - val_loss: 0.3042 Epoch 77/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3001 - val_loss: 0.3010 Epoch 78/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2996 - val_loss: 0.2999 Epoch 79/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2981 - val_loss: 0.3223 Epoch 80/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2989 - val_loss: 0.3033 Epoch 81/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.2978 - val_loss: 0.3024 Epoch 82/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2972 - val_loss: 0.2972 Epoch 83/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.2968 - val_loss: 0.3024 Epoch 84/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2964 - val_loss: 0.2988 Epoch 85/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.2953 - val_loss: 0.3088 Epoch 86/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.2944 - val_loss: 0.3030 Epoch 87/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2944 - val_loss: 0.2996 Epoch 88/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.2941 - val_loss: 0.2984 Epoch 89/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.2934 - val_loss: 0.3058 Epoch 90/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.2928 - val_loss: 0.3072 Epoch 91/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.2923 - val_loss: 0.3268 Epoch 92/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.2918 - val_loss: 0.3025 3870/3870 [==============================] - 0s 14us/sample - loss: 0.3291 7740/7740 [==============================] - 0s 16us/sample - loss: 0.2901 [CV] learning_rate=0.0023302047292153563, n_hidden=2, n_neurons=41, total= 27.1s [CV] learning_rate=0.0023302047292153563, n_hidden=2, n_neurons=41 ... [Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 27.3s remaining: 0.0s Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 55us/sample - loss: 1.3620 - val_loss: 1.9705 Epoch 2/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5921 - val_loss: 0.6401 Epoch 3/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.5145 - val_loss: 0.4716 Epoch 4/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4709 - val_loss: 0.4411 Epoch 5/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4432 - val_loss: 0.4216 Epoch 6/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4248 - val_loss: 0.3981 Epoch 7/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4126 - val_loss: 0.3831 Epoch 8/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4020 - val_loss: 0.3851 Epoch 9/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3949 - val_loss: 0.4085 Epoch 10/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3881 - val_loss: 0.4479 Epoch 11/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3839 - val_loss: 0.4876 Epoch 12/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3786 - val_loss: 0.5219 Epoch 13/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3754 - val_loss: 0.6181 Epoch 14/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3716 - val_loss: 0.6809 Epoch 15/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3688 - val_loss: 0.7110 Epoch 16/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3663 - val_loss: 0.7682 Epoch 17/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3642 - val_loss: 0.8752 3870/3870 [==============================] - 0s 14us/sample - loss: 0.3741 7740/7740 [==============================] - 0s 16us/sample - loss: 0.3604 [CV] learning_rate=0.0023302047292153563, n_hidden=2, n_neurons=41, total= 5.5s [CV] learning_rate=0.0023302047292153563, n_hidden=2, n_neurons=41 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 55us/sample - loss: 1.5622 - val_loss: 0.6830 Epoch 2/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.6203 - val_loss: 0.5885 Epoch 3/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5482 - val_loss: 0.5091 Epoch 4/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4988 - val_loss: 0.4652 Epoch 5/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4643 - val_loss: 0.4598 Epoch 6/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4398 - val_loss: 0.4594 Epoch 7/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4229 - val_loss: 0.3934 Epoch 8/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4119 - val_loss: 0.5014 Epoch 9/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4040 - val_loss: 0.4281 Epoch 10/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3975 - val_loss: 0.3707 Epoch 11/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3922 - val_loss: 0.4708 Epoch 12/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3880 - val_loss: 0.4576 Epoch 13/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3850 - val_loss: 0.3639 Epoch 14/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3817 - val_loss: 0.4434 Epoch 15/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3793 - val_loss: 0.3625 Epoch 16/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3756 - val_loss: 0.4428 Epoch 17/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3742 - val_loss: 0.3508 Epoch 18/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3716 - val_loss: 0.3632 Epoch 19/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3692 - val_loss: 0.4190 Epoch 20/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3664 - val_loss: 0.4335 Epoch 21/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3653 - val_loss: 0.3813 Epoch 22/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3628 - val_loss: 0.4380 Epoch 23/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3617 - val_loss: 0.3523 Epoch 24/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3600 - val_loss: 0.3579 Epoch 25/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3582 - val_loss: 0.4265 Epoch 26/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3569 - val_loss: 0.4404 Epoch 27/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3556 - val_loss: 0.3602 3870/3870 [==============================] - 0s 15us/sample - loss: 0.3564 7740/7740 [==============================] - 0s 16us/sample - loss: 0.3544 [CV] learning_rate=0.0023302047292153563, n_hidden=2, n_neurons=41, total= 8.3s [CV] learning_rate=0.015956195942385693, n_hidden=0, n_neurons=97 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 47us/sample - loss: 0.7604 - val_loss: 5.4232 Epoch 2/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.6441 - val_loss: 14.1695 Epoch 3/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.6713 - val_loss: 3.7025 Epoch 4/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.5019 - val_loss: 4.6624 Epoch 5/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.8346 - val_loss: 49.8473 Epoch 6/100 7740/7740 [==============================] - 0s 34us/sample - loss: 3.0755 - val_loss: 0.3787 Epoch 7/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4175 - val_loss: 0.3511 Epoch 8/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3728 - val_loss: 0.3492 Epoch 9/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5112 - val_loss: 0.3382 Epoch 10/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3575 - val_loss: 0.3333 Epoch 11/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4000 - val_loss: 0.3305 Epoch 12/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5751 - val_loss: 1.1609 Epoch 13/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4492 - val_loss: 0.3487 Epoch 14/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3481 - val_loss: 0.3288 Epoch 15/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3440 - val_loss: 0.3263 Epoch 16/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3395 - val_loss: 0.3361 Epoch 17/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3359 - val_loss: 0.3362 Epoch 18/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3359 - val_loss: 0.3213 Epoch 19/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3335 - val_loss: 0.3174 Epoch 20/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3284 - val_loss: 0.3236 Epoch 21/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3275 - val_loss: 0.3134 Epoch 22/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3257 - val_loss: 0.3121 Epoch 23/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3243 - val_loss: 0.3127 Epoch 24/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3218 - val_loss: 0.3214 Epoch 25/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3217 - val_loss: 0.3085 Epoch 26/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3191 - val_loss: 0.3113 Epoch 27/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3189 - val_loss: 0.3051 Epoch 28/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3172 - val_loss: 0.3059 Epoch 29/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3154 - val_loss: 0.3059 Epoch 30/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3146 - val_loss: 0.3134 Epoch 31/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3111 - val_loss: 0.3168 Epoch 32/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3134 - val_loss: 0.3356 Epoch 33/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3130 - val_loss: 0.3226 Epoch 34/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3100 - val_loss: 0.3243 Epoch 35/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3097 - val_loss: 0.3224 Epoch 36/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3088 - val_loss: 0.3283 Epoch 37/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3113 - val_loss: 0.3471 3870/3870 [==============================] - 0s 13us/sample - loss: 0.4386 7740/7740 [==============================] - 0s 14us/sample - loss: 0.3676 [CV] learning_rate=0.015956195942385693, n_hidden=0, n_neurons=97, total= 10.0s [CV] learning_rate=0.015956195942385693, n_hidden=0, n_neurons=97 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 46us/sample - loss: 0.7714 - val_loss: 0.7641 Epoch 2/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4561 - val_loss: 1.2963 Epoch 3/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4122 - val_loss: 0.3778 Epoch 4/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4151 - val_loss: 0.4568 Epoch 5/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3952 - val_loss: 0.7350 Epoch 6/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3759 - val_loss: 0.7615 Epoch 7/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3707 - val_loss: 0.5305 Epoch 8/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3696 - val_loss: 0.3510 Epoch 9/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3634 - val_loss: 0.3410 Epoch 10/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3602 - val_loss: 0.3868 Epoch 11/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3531 - val_loss: 0.8745 Epoch 12/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3574 - val_loss: 0.3859 Epoch 13/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3542 - val_loss: 0.6109 Epoch 14/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3512 - val_loss: 0.4101 Epoch 15/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3496 - val_loss: 0.3618 Epoch 16/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3458 - val_loss: 0.3277 Epoch 17/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3454 - val_loss: 0.4292 Epoch 18/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3431 - val_loss: 0.3737 Epoch 19/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3434 - val_loss: 0.6402 Epoch 20/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3444 - val_loss: 0.3269 Epoch 21/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3388 - val_loss: 0.6092 Epoch 22/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3431 - val_loss: 0.3750 Epoch 23/100 7740/7740 [==============================] - 0s 43us/sample - loss: 0.3417 - val_loss: 0.3615 Epoch 24/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3409 - val_loss: 0.6085 Epoch 25/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3331 - val_loss: 0.4557 Epoch 26/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3308 - val_loss: 0.4441 Epoch 27/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3303 - val_loss: 0.3698 Epoch 28/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3282 - val_loss: 0.3496 Epoch 29/100 7740/7740 [==============================] - 0s 45us/sample - loss: 0.3314 - val_loss: 0.3630 Epoch 30/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3258 - val_loss: 0.3160 Epoch 31/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3255 - val_loss: 0.3887 Epoch 32/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3229 - val_loss: 0.7654 Epoch 33/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3269 - val_loss: 0.3289 Epoch 34/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3192 - val_loss: 0.3337 Epoch 35/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3304 - val_loss: 0.4950 Epoch 36/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3192 - val_loss: 1.9769 Epoch 37/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3214 - val_loss: 0.6508 Epoch 38/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3190 - val_loss: 0.3250 Epoch 39/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3191 - val_loss: 0.7641 Epoch 40/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3161 - val_loss: 0.4083 3870/3870 [==============================] - 0s 13us/sample - loss: 0.3752 7740/7740 [==============================] - 0s 14us/sample - loss: 0.3483 [CV] learning_rate=0.015956195942385693, n_hidden=0, n_neurons=97, total= 11.3s [CV] learning_rate=0.015956195942385693, n_hidden=0, n_neurons=97 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 46us/sample - loss: 0.7449 - val_loss: 1.8796 Epoch 2/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4798 - val_loss: 0.5772 Epoch 3/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4285 - val_loss: 0.3831 Epoch 4/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4161 - val_loss: 1.4244 Epoch 5/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4231 - val_loss: 18.9345 Epoch 6/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3970 - val_loss: 6.8569 Epoch 7/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.6343 - val_loss: 3.4721 Epoch 8/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4484 - val_loss: 103.4562 Epoch 9/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5382 - val_loss: 0.3930 Epoch 10/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3965 - val_loss: 0.3660 Epoch 11/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4080 - val_loss: 0.3647 Epoch 12/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3822 - val_loss: 0.3534 Epoch 13/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3735 - val_loss: 0.3537 Epoch 14/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3672 - val_loss: 0.3453 Epoch 15/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3628 - val_loss: 0.3444 Epoch 16/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3766 - val_loss: 0.3401 Epoch 17/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3711 - val_loss: 0.3460 Epoch 18/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3623 - val_loss: 0.3325 Epoch 19/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3570 - val_loss: 0.3644 Epoch 20/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3536 - val_loss: 0.3517 Epoch 21/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3496 - val_loss: 0.3682 Epoch 22/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3599 - val_loss: 0.3256 Epoch 23/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3469 - val_loss: 0.3334 Epoch 24/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3439 - val_loss: 0.3432 Epoch 25/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3469 - val_loss: 0.3219 Epoch 26/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3416 - val_loss: 0.3368 Epoch 27/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3394 - val_loss: 0.3201 Epoch 28/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3370 - val_loss: 0.3207 Epoch 29/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3346 - val_loss: 0.3202 Epoch 30/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3315 - val_loss: 0.3141 Epoch 31/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3335 - val_loss: 0.3171 Epoch 32/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3306 - val_loss: 0.3172 Epoch 33/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3389 - val_loss: 0.3142 Epoch 34/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3332 - val_loss: 0.3638 Epoch 35/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3315 - val_loss: 0.3186 Epoch 36/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3256 - val_loss: 0.3102 Epoch 37/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3233 - val_loss: 0.3238 Epoch 38/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3291 - val_loss: 0.3299 Epoch 39/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3339 - val_loss: 0.3433 Epoch 40/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3250 - val_loss: 0.3063 Epoch 41/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3228 - val_loss: 0.3480 Epoch 42/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3186 - val_loss: 0.3669 Epoch 43/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3251 - val_loss: 0.3058 Epoch 44/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3247 - val_loss: 0.3364 Epoch 45/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3180 - val_loss: 0.3104 Epoch 46/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3232 - val_loss: 0.3273 Epoch 47/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3166 - val_loss: 0.3027 Epoch 48/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3136 - val_loss: 0.3203 Epoch 49/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3131 - val_loss: 0.3446 Epoch 50/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3147 - val_loss: 0.3076 Epoch 51/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3226 - val_loss: 0.3362 Epoch 52/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3121 - val_loss: 0.3097 Epoch 53/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3116 - val_loss: 0.4066 Epoch 54/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3176 - val_loss: 0.3047 Epoch 55/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3096 - val_loss: 0.3028 Epoch 56/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3083 - val_loss: 0.3614 Epoch 57/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3074 - val_loss: 0.3032 3870/3870 [==============================] - 0s 13us/sample - loss: 0.3190 7740/7740 [==============================] - 0s 14us/sample - loss: 0.3051 [CV] learning_rate=0.015956195942385693, n_hidden=0, n_neurons=97, total= 15.3s [CV] learning_rate=0.0017196854145436866, n_hidden=2, n_neurons=77 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 56us/sample - loss: 1.5431 - val_loss: 2.1386 Epoch 2/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.6880 - val_loss: 0.5965 Epoch 3/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.5931 - val_loss: 0.5344 Epoch 4/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.5405 - val_loss: 0.5002 Epoch 5/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4996 - val_loss: 0.5025 Epoch 6/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4679 - val_loss: 0.4393 Epoch 7/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4435 - val_loss: 0.4388 Epoch 8/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4256 - val_loss: 0.3981 Epoch 9/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4115 - val_loss: 0.4108 Epoch 10/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4016 - val_loss: 0.4239 Epoch 11/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3936 - val_loss: 0.4047 Epoch 12/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3870 - val_loss: 0.4046 Epoch 13/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3808 - val_loss: 0.3774 Epoch 14/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3757 - val_loss: 0.4053 Epoch 15/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3706 - val_loss: 0.3574 Epoch 16/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3669 - val_loss: 0.3605 Epoch 17/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3628 - val_loss: 0.4119 Epoch 18/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3604 - val_loss: 0.4028 Epoch 19/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3572 - val_loss: 0.3772 Epoch 20/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3547 - val_loss: 0.3834 Epoch 21/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3522 - val_loss: 0.3464 Epoch 22/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3494 - val_loss: 0.3959 Epoch 23/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3477 - val_loss: 0.3378 Epoch 24/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3456 - val_loss: 0.4059 Epoch 25/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3446 - val_loss: 0.3477 Epoch 26/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3420 - val_loss: 0.3920 Epoch 27/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3410 - val_loss: 0.3325 Epoch 28/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3389 - val_loss: 0.3782 Epoch 29/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3373 - val_loss: 0.3309 Epoch 30/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3366 - val_loss: 0.3642 Epoch 31/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3350 - val_loss: 0.3590 Epoch 32/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3333 - val_loss: 0.3305 Epoch 33/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3319 - val_loss: 0.3564 Epoch 34/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3305 - val_loss: 0.3445 Epoch 35/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3300 - val_loss: 0.3723 Epoch 36/100 7740/7740 [==============================] - 0s 42us/sample - loss: 0.3287 - val_loss: 0.3593 Epoch 37/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3278 - val_loss: 0.3276 Epoch 38/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3272 - val_loss: 0.4318 Epoch 39/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3264 - val_loss: 0.3511 Epoch 40/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3252 - val_loss: 0.3221 Epoch 41/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3240 - val_loss: 0.3791 Epoch 42/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3236 - val_loss: 0.3305 Epoch 43/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3225 - val_loss: 0.3361 Epoch 44/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3210 - val_loss: 0.3781 Epoch 45/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3205 - val_loss: 0.3291 Epoch 46/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3197 - val_loss: 0.3419 Epoch 47/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3189 - val_loss: 0.3624 Epoch 48/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3179 - val_loss: 0.3237 Epoch 49/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3168 - val_loss: 0.3165 Epoch 50/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3160 - val_loss: 0.3233 Epoch 51/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3148 - val_loss: 0.4150 Epoch 52/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3152 - val_loss: 0.3100 Epoch 53/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3138 - val_loss: 0.4137 Epoch 54/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3140 - val_loss: 0.3866 Epoch 55/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3125 - val_loss: 0.3258 Epoch 56/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3112 - val_loss: 0.3279 Epoch 57/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3109 - val_loss: 0.4006 Epoch 58/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3111 - val_loss: 0.3077 Epoch 59/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3094 - val_loss: 0.3585 Epoch 60/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3097 - val_loss: 0.3600 Epoch 61/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3089 - val_loss: 0.3933 Epoch 62/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3080 - val_loss: 0.3050 Epoch 63/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3066 - val_loss: 0.3039 Epoch 64/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3057 - val_loss: 0.3046 Epoch 65/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3053 - val_loss: 0.3040 Epoch 66/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3047 - val_loss: 0.3039 Epoch 67/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3036 - val_loss: 0.4107 Epoch 68/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3036 - val_loss: 0.3624 Epoch 69/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3031 - val_loss: 0.4021 Epoch 70/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3026 - val_loss: 0.3481 Epoch 71/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3021 - val_loss: 0.5111 Epoch 72/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3034 - val_loss: 0.3270 Epoch 73/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2998 - val_loss: 0.4186 3870/3870 [==============================] - 0s 15us/sample - loss: 0.3326 7740/7740 [==============================] - 0s 17us/sample - loss: 0.2985 [CV] learning_rate=0.0017196854145436866, n_hidden=2, n_neurons=77, total= 22.5s [CV] learning_rate=0.0017196854145436866, n_hidden=2, n_neurons=77 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 56us/sample - loss: 1.6301 - val_loss: 4.2039 Epoch 2/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.6970 - val_loss: 0.6768 Epoch 3/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.6090 - val_loss: 0.8804 Epoch 4/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.5529 - val_loss: 1.6374 Epoch 5/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.5127 - val_loss: 2.1626 Epoch 6/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4821 - val_loss: 2.8482 Epoch 7/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4595 - val_loss: 2.3233 Epoch 8/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4420 - val_loss: 2.1937 Epoch 9/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4277 - val_loss: 2.1441 Epoch 10/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.4166 - val_loss: 1.2412 Epoch 11/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4067 - val_loss: 1.0166 Epoch 12/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3987 - val_loss: 0.7848 3870/3870 [==============================] - 0s 15us/sample - loss: 0.4227 7740/7740 [==============================] - 0s 17us/sample - loss: 0.3932 [CV] learning_rate=0.0017196854145436866, n_hidden=2, n_neurons=77, total= 4.1s [CV] learning_rate=0.0017196854145436866, n_hidden=2, n_neurons=77 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 56us/sample - loss: 1.5410 - val_loss: 1.7610 Epoch 2/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.6859 - val_loss: 0.6030 Epoch 3/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.6079 - val_loss: 0.5475 Epoch 4/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.5578 - val_loss: 0.5058 Epoch 5/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.5189 - val_loss: 0.4733 Epoch 6/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4871 - val_loss: 0.4425 Epoch 7/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4617 - val_loss: 0.4225 Epoch 8/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4422 - val_loss: 0.4064 Epoch 9/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4270 - val_loss: 0.4141 Epoch 10/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4156 - val_loss: 0.4001 Epoch 11/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4056 - val_loss: 0.4111 Epoch 12/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3978 - val_loss: 0.3833 Epoch 13/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3928 - val_loss: 0.4107 Epoch 14/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3881 - val_loss: 0.3695 Epoch 15/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3824 - val_loss: 0.4339 Epoch 16/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3784 - val_loss: 0.4015 Epoch 17/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3751 - val_loss: 0.3944 Epoch 18/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3715 - val_loss: 0.3888 Epoch 19/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3681 - val_loss: 0.4058 Epoch 20/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3654 - val_loss: 0.3471 Epoch 21/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3638 - val_loss: 0.3469 Epoch 22/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3600 - val_loss: 0.3725 Epoch 23/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3581 - val_loss: 0.4448 Epoch 24/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3571 - val_loss: 0.3868 Epoch 25/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3549 - val_loss: 0.3871 Epoch 26/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3532 - val_loss: 0.3699 Epoch 27/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3510 - val_loss: 0.3456 Epoch 28/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3492 - val_loss: 0.4108 Epoch 29/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3481 - val_loss: 0.3753 Epoch 30/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3461 - val_loss: 0.3960 Epoch 31/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3445 - val_loss: 0.3945 Epoch 32/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3434 - val_loss: 0.3682 Epoch 33/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3419 - val_loss: 0.3298 Epoch 34/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3411 - val_loss: 0.3609 Epoch 35/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3398 - val_loss: 0.3399 Epoch 36/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3385 - val_loss: 0.3297 Epoch 37/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3368 - val_loss: 0.3665 Epoch 38/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3364 - val_loss: 0.3196 Epoch 39/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3353 - val_loss: 0.4043 Epoch 40/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3344 - val_loss: 0.3602 Epoch 41/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3329 - val_loss: 0.3659 Epoch 42/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3321 - val_loss: 0.3848 Epoch 43/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3310 - val_loss: 0.3145 Epoch 44/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3296 - val_loss: 0.3836 Epoch 45/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3294 - val_loss: 0.3203 Epoch 46/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3272 - val_loss: 0.3455 Epoch 47/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3271 - val_loss: 0.3557 Epoch 48/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3262 - val_loss: 0.3176 Epoch 49/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3252 - val_loss: 0.3626 Epoch 50/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3246 - val_loss: 0.3258 Epoch 51/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3235 - val_loss: 0.3191 Epoch 52/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3231 - val_loss: 0.3160 Epoch 53/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3216 - val_loss: 0.3293 3870/3870 [==============================] - 0s 15us/sample - loss: 0.3243 7740/7740 [==============================] - 0s 16us/sample - loss: 0.3189 [CV] learning_rate=0.0017196854145436866, n_hidden=2, n_neurons=77, total= 16.3s [CV] learning_rate=0.003284284607688981, n_hidden=2, n_neurons=75 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 56us/sample - loss: 1.4842 - val_loss: 2.1657 Epoch 2/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.5991 - val_loss: 1.9354 Epoch 3/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.5146 - val_loss: 0.6279 Epoch 4/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4586 - val_loss: 0.5010 Epoch 5/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4262 - val_loss: 0.3873 Epoch 6/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4064 - val_loss: 0.3757 Epoch 7/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3914 - val_loss: 0.3797 Epoch 8/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3825 - val_loss: 0.3684 Epoch 9/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3742 - val_loss: 0.3503 Epoch 10/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3679 - val_loss: 0.3745 Epoch 11/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3622 - val_loss: 0.3999 Epoch 12/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3582 - val_loss: 0.3889 Epoch 13/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3541 - val_loss: 0.3470 Epoch 14/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3500 - val_loss: 0.4274 Epoch 15/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3486 - val_loss: 0.3568 Epoch 16/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3443 - val_loss: 0.4425 Epoch 17/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3429 - val_loss: 0.3352 Epoch 18/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3390 - val_loss: 0.3316 Epoch 19/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3365 - val_loss: 0.4038 Epoch 20/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3357 - val_loss: 0.3435 Epoch 21/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3330 - val_loss: 0.3226 Epoch 22/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3316 - val_loss: 0.3796 Epoch 23/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3293 - val_loss: 0.3240 Epoch 24/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3274 - val_loss: 0.3713 Epoch 25/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3268 - val_loss: 0.3210 Epoch 26/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3225 - val_loss: 0.3229 Epoch 27/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3219 - val_loss: 0.3197 Epoch 28/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3205 - val_loss: 0.3354 Epoch 29/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3195 - val_loss: 0.3137 Epoch 30/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3180 - val_loss: 0.3713 Epoch 31/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3170 - val_loss: 0.3105 Epoch 32/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3153 - val_loss: 0.3809 Epoch 33/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3137 - val_loss: 0.3351 Epoch 34/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3127 - val_loss: 0.3326 Epoch 35/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3109 - val_loss: 0.3667 Epoch 36/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3089 - val_loss: 0.3167 Epoch 37/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3073 - val_loss: 0.3544 Epoch 38/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3082 - val_loss: 0.3108 Epoch 39/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3065 - val_loss: 0.3324 Epoch 40/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3063 - val_loss: 0.3016 Epoch 41/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3038 - val_loss: 0.3123 Epoch 42/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3025 - val_loss: 0.3372 Epoch 43/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3027 - val_loss: 0.3443 Epoch 44/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3021 - val_loss: 0.3409 Epoch 45/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3002 - val_loss: 0.3001 Epoch 46/100 7740/7740 [==============================] - 0s 43us/sample - loss: 0.2992 - val_loss: 0.3370 Epoch 47/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2981 - val_loss: 0.3304 Epoch 48/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2979 - val_loss: 0.3872 Epoch 49/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2984 - val_loss: 0.3408 Epoch 50/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2955 - val_loss: 0.3122 Epoch 51/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2942 - val_loss: 0.3237 Epoch 52/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2932 - val_loss: 0.3119 Epoch 53/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2924 - val_loss: 0.3206 Epoch 54/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2910 - val_loss: 0.4156 Epoch 55/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2920 - val_loss: 0.3537 3870/3870 [==============================] - 0s 15us/sample - loss: 0.3294 7740/7740 [==============================] - 0s 17us/sample - loss: 0.2875 [CV] learning_rate=0.003284284607688981, n_hidden=2, n_neurons=75, total= 17.3s [CV] learning_rate=0.003284284607688981, n_hidden=2, n_neurons=75 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 56us/sample - loss: 1.0847 - val_loss: 5.8122 Epoch 2/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.6135 - val_loss: 0.7386 Epoch 3/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.5218 - val_loss: 0.6025 Epoch 4/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4632 - val_loss: 0.7112 Epoch 5/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4265 - val_loss: 0.5877 Epoch 6/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4039 - val_loss: 0.4176 Epoch 7/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3898 - val_loss: 0.3684 Epoch 8/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3788 - val_loss: 0.4108 Epoch 9/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3707 - val_loss: 0.4843 Epoch 10/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3641 - val_loss: 0.6541 Epoch 11/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3576 - val_loss: 0.7952 Epoch 12/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3545 - val_loss: 0.8378 Epoch 13/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3505 - val_loss: 0.8978 Epoch 14/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3467 - val_loss: 0.8281 Epoch 15/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3450 - val_loss: 0.9674 Epoch 16/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3422 - val_loss: 0.9459 Epoch 17/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3400 - val_loss: 0.8946 3870/3870 [==============================] - 0s 15us/sample - loss: 0.3573 7740/7740 [==============================] - 0s 16us/sample - loss: 0.3355 [CV] learning_rate=0.003284284607688981, n_hidden=2, n_neurons=75, total= 5.7s [CV] learning_rate=0.003284284607688981, n_hidden=2, n_neurons=75 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 56us/sample - loss: 1.3023 - val_loss: 2.6927 Epoch 2/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.5547 - val_loss: 0.5447 Epoch 3/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4631 - val_loss: 0.4596 Epoch 4/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4296 - val_loss: 0.4099 Epoch 5/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4112 - val_loss: 0.4182 Epoch 6/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3992 - val_loss: 0.4050 Epoch 7/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3907 - val_loss: 0.3833 Epoch 8/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3824 - val_loss: 0.4306 Epoch 9/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3773 - val_loss: 0.3806 Epoch 10/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3720 - val_loss: 0.3851 Epoch 11/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3677 - val_loss: 0.3504 Epoch 12/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3632 - val_loss: 0.3865 Epoch 13/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3596 - val_loss: 0.4215 Epoch 14/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3577 - val_loss: 0.3906 Epoch 15/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3538 - val_loss: 0.4588 Epoch 16/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3525 - val_loss: 0.3350 Epoch 17/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3488 - val_loss: 0.4020 Epoch 18/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3459 - val_loss: 0.4451 Epoch 19/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3445 - val_loss: 0.4082 Epoch 20/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3429 - val_loss: 0.3690 Epoch 21/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3413 - val_loss: 0.3326 Epoch 22/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3389 - val_loss: 0.3475 Epoch 23/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3376 - val_loss: 0.3381 Epoch 24/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3352 - val_loss: 0.3333 Epoch 25/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3340 - val_loss: 0.4147 Epoch 26/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3325 - val_loss: 0.4207 Epoch 27/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3319 - val_loss: 0.3328 Epoch 28/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3302 - val_loss: 0.3656 Epoch 29/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3288 - val_loss: 0.3180 Epoch 30/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3266 - val_loss: 0.3994 Epoch 31/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3269 - val_loss: 0.3344 Epoch 32/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3252 - val_loss: 0.3281 Epoch 33/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3226 - val_loss: 0.3339 Epoch 34/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3233 - val_loss: 0.3283 Epoch 35/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3208 - val_loss: 0.3834 Epoch 36/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3209 - val_loss: 0.3260 Epoch 37/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3187 - val_loss: 0.3149 Epoch 38/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3175 - val_loss: 0.3136 Epoch 39/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3159 - val_loss: 0.4119 Epoch 40/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3158 - val_loss: 0.3252 Epoch 41/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3144 - val_loss: 0.3174 Epoch 42/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3129 - val_loss: 0.3911 Epoch 43/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3131 - val_loss: 0.3167 Epoch 44/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3117 - val_loss: 0.4159 Epoch 45/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3113 - val_loss: 0.3414 Epoch 46/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3078 - val_loss: 0.3597 Epoch 47/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3089 - val_loss: 0.3062 Epoch 48/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3074 - val_loss: 0.3133 Epoch 49/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3052 - val_loss: 0.3587 Epoch 50/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3060 - val_loss: 0.3166 Epoch 51/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3038 - val_loss: 0.3144 Epoch 52/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3038 - val_loss: 0.3803 Epoch 53/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3024 - val_loss: 0.3023 Epoch 54/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3012 - val_loss: 0.3206 Epoch 55/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3006 - val_loss: 0.3453 Epoch 56/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3008 - val_loss: 0.3081 Epoch 57/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2989 - val_loss: 0.3795 Epoch 58/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2982 - val_loss: 0.3208 Epoch 59/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2973 - val_loss: 0.3284 Epoch 60/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2957 - val_loss: 0.3058 Epoch 61/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2966 - val_loss: 0.3155 Epoch 62/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2947 - val_loss: 0.3156 Epoch 63/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2948 - val_loss: 0.3417 3870/3870 [==============================] - 0s 15us/sample - loss: 0.3149 7740/7740 [==============================] - 0s 16us/sample - loss: 0.2935 [CV] learning_rate=0.003284284607688981, n_hidden=2, n_neurons=75, total= 19.2s [CV] learning_rate=0.004038022680351922, n_hidden=2, n_neurons=69 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 56us/sample - loss: 1.1144 - val_loss: 1.6181 Epoch 2/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.5747 - val_loss: 1.2753 Epoch 3/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4971 - val_loss: 0.5712 Epoch 4/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4514 - val_loss: 0.5843 Epoch 5/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4202 - val_loss: 0.3850 Epoch 6/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4007 - val_loss: 0.3774 Epoch 7/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3869 - val_loss: 0.4252 Epoch 8/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3780 - val_loss: 0.3768 Epoch 9/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3698 - val_loss: 0.3744 Epoch 10/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3627 - val_loss: 0.3604 Epoch 11/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3577 - val_loss: 0.4952 Epoch 12/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3563 - val_loss: 0.4150 Epoch 13/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3503 - val_loss: 0.3910 Epoch 14/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3469 - val_loss: 0.3373 Epoch 15/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3428 - val_loss: 0.3744 Epoch 16/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3414 - val_loss: 0.3325 Epoch 17/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3371 - val_loss: 0.4917 Epoch 18/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3410 - val_loss: 0.6252 Epoch 19/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3394 - val_loss: 1.0122 Epoch 20/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3407 - val_loss: 0.9887 Epoch 21/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3384 - val_loss: 0.7173 Epoch 22/100 7740/7740 [==============================] - 0s 43us/sample - loss: 0.3325 - val_loss: 0.3328 Epoch 23/100 7740/7740 [==============================] - 0s 45us/sample - loss: 0.3257 - val_loss: 0.3620 Epoch 24/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3255 - val_loss: 0.3204 Epoch 25/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3224 - val_loss: 0.3355 Epoch 26/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3205 - val_loss: 0.3574 Epoch 27/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3197 - val_loss: 0.3340 Epoch 28/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3172 - val_loss: 0.3134 Epoch 29/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3155 - val_loss: 0.3252 Epoch 30/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3151 - val_loss: 0.3228 Epoch 31/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3131 - val_loss: 0.3122 Epoch 32/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3109 - val_loss: 0.3072 Epoch 33/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3095 - val_loss: 0.4399 Epoch 34/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3111 - val_loss: 0.5012 Epoch 35/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3116 - val_loss: 0.4788 Epoch 36/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3081 - val_loss: 0.3058 Epoch 37/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3061 - val_loss: 0.3707 Epoch 38/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3038 - val_loss: 0.3291 Epoch 39/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3021 - val_loss: 0.3020 Epoch 40/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3014 - val_loss: 0.3682 Epoch 41/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3014 - val_loss: 0.3114 Epoch 42/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2999 - val_loss: 0.3104 Epoch 43/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2986 - val_loss: 0.3037 Epoch 44/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2966 - val_loss: 0.3074 Epoch 45/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2953 - val_loss: 0.3298 Epoch 46/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2956 - val_loss: 0.4312 Epoch 47/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2953 - val_loss: 0.4607 Epoch 48/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2959 - val_loss: 0.3917 Epoch 49/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2955 - val_loss: 0.3009 Epoch 50/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2912 - val_loss: 0.3113 Epoch 51/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2909 - val_loss: 0.3125 Epoch 52/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2903 - val_loss: 0.3306 Epoch 53/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2884 - val_loss: 0.2966 Epoch 54/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2876 - val_loss: 0.3536 Epoch 55/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2877 - val_loss: 0.4683 Epoch 56/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2902 - val_loss: 0.4054 Epoch 57/100 7740/7740 [==============================] - 0s 42us/sample - loss: 0.2866 - val_loss: 0.3190 Epoch 58/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.2854 - val_loss: 0.3752 Epoch 59/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2839 - val_loss: 0.2895 Epoch 60/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2828 - val_loss: 0.3034 Epoch 61/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2829 - val_loss: 0.3833 Epoch 62/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2839 - val_loss: 0.2852 Epoch 63/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2806 - val_loss: 0.2916 Epoch 64/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2787 - val_loss: 0.2897 Epoch 65/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2794 - val_loss: 0.2838 Epoch 66/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2785 - val_loss: 0.3772 Epoch 67/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2787 - val_loss: 0.3277 Epoch 68/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2771 - val_loss: 0.5036 Epoch 69/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2791 - val_loss: 0.3969 Epoch 70/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2765 - val_loss: 0.6491 Epoch 71/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2857 - val_loss: 0.6069 Epoch 72/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2822 - val_loss: 0.8506 Epoch 73/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2840 - val_loss: 0.4954 Epoch 74/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2785 - val_loss: 0.4551 Epoch 75/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2754 - val_loss: 0.2963 3870/3870 [==============================] - 0s 15us/sample - loss: 0.3199 7740/7740 [==============================] - 0s 16us/sample - loss: 0.2753 [CV] learning_rate=0.004038022680351922, n_hidden=2, n_neurons=69, total= 23.2s [CV] learning_rate=0.004038022680351922, n_hidden=2, n_neurons=69 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 55us/sample - loss: 1.1371 - val_loss: 1.3452 Epoch 2/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.5859 - val_loss: 0.5920 Epoch 3/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.5003 - val_loss: 0.4443 Epoch 4/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4476 - val_loss: 0.4072 Epoch 5/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4162 - val_loss: 0.3839 Epoch 6/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3974 - val_loss: 0.3671 Epoch 7/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3837 - val_loss: 0.3693 Epoch 8/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3740 - val_loss: 0.3850 Epoch 9/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3688 - val_loss: 0.4103 Epoch 10/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3634 - val_loss: 0.4479 Epoch 11/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3587 - val_loss: 0.5044 Epoch 12/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3554 - val_loss: 0.5236 Epoch 13/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3508 - val_loss: 0.5970 Epoch 14/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3474 - val_loss: 0.6589 Epoch 15/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3454 - val_loss: 0.6735 Epoch 16/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3421 - val_loss: 0.7284 3870/3870 [==============================] - 0s 15us/sample - loss: 0.3575 7740/7740 [==============================] - 0s 16us/sample - loss: 0.3373 [CV] learning_rate=0.004038022680351922, n_hidden=2, n_neurons=69, total= 5.2s [CV] learning_rate=0.004038022680351922, n_hidden=2, n_neurons=69 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 56us/sample - loss: 1.2970 - val_loss: 7.5366 Epoch 2/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.6151 - val_loss: 2.4204 Epoch 3/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.5010 - val_loss: 0.4373 Epoch 4/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4434 - val_loss: 0.4774 Epoch 5/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4185 - val_loss: 0.4510 Epoch 6/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4012 - val_loss: 0.4292 Epoch 7/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3888 - val_loss: 0.3930 Epoch 8/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3802 - val_loss: 0.4631 Epoch 9/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3740 - val_loss: 0.3844 Epoch 10/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3680 - val_loss: 0.4246 Epoch 11/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3648 - val_loss: 0.4520 Epoch 12/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3609 - val_loss: 0.4205 Epoch 13/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3570 - val_loss: 0.3746 Epoch 14/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3529 - val_loss: 0.3670 Epoch 15/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3511 - val_loss: 0.3522 Epoch 16/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3471 - val_loss: 0.4065 Epoch 17/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3453 - val_loss: 0.3991 Epoch 18/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3426 - val_loss: 0.3844 Epoch 19/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3402 - val_loss: 0.3834 Epoch 20/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3375 - val_loss: 0.4403 Epoch 21/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3362 - val_loss: 0.3458 Epoch 22/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3335 - val_loss: 0.3672 Epoch 23/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3317 - val_loss: 0.3860 Epoch 24/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3304 - val_loss: 0.3250 Epoch 25/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3278 - val_loss: 0.3844 Epoch 26/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3265 - val_loss: 0.4097 Epoch 27/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3250 - val_loss: 0.3139 Epoch 28/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3238 - val_loss: 0.3452 Epoch 29/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3215 - val_loss: 0.3300 Epoch 30/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3198 - val_loss: 0.3281 Epoch 31/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3173 - val_loss: 0.3994 Epoch 32/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3172 - val_loss: 0.3340 Epoch 33/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3153 - val_loss: 0.3991 Epoch 34/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3152 - val_loss: 0.3153 Epoch 35/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3128 - val_loss: 0.3425 Epoch 36/100 7740/7740 [==============================] - 0s 48us/sample - loss: 0.3115 - val_loss: 0.3047 Epoch 37/100 7740/7740 [==============================] - 0s 44us/sample - loss: 0.3098 - val_loss: 0.3154 Epoch 38/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3085 - val_loss: 0.3139 Epoch 39/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3074 - val_loss: 0.3774 Epoch 40/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3071 - val_loss: 0.3249 Epoch 41/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3071 - val_loss: 0.3420 Epoch 42/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3026 - val_loss: 0.3788 Epoch 43/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3027 - val_loss: 0.3213 Epoch 44/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3014 - val_loss: 0.3695 Epoch 45/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3001 - val_loss: 0.2987 Epoch 46/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2987 - val_loss: 0.3171 Epoch 47/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2981 - val_loss: 0.3631 Epoch 48/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2975 - val_loss: 0.2914 Epoch 49/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2959 - val_loss: 0.3695 Epoch 50/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.2942 - val_loss: 0.2942 Epoch 51/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2925 - val_loss: 0.3925 Epoch 52/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2939 - val_loss: 0.2917 Epoch 53/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2916 - val_loss: 0.3588 Epoch 54/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.2899 - val_loss: 0.3232 Epoch 55/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2914 - val_loss: 0.2941 Epoch 56/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2878 - val_loss: 0.2984 Epoch 57/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2871 - val_loss: 0.3576 Epoch 58/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.2862 - val_loss: 0.3397 3870/3870 [==============================] - 0s 15us/sample - loss: 0.3052 7740/7740 [==============================] - 0s 17us/sample - loss: 0.2829 [CV] learning_rate=0.004038022680351922, n_hidden=2, n_neurons=69, total= 18.3s [CV] learning_rate=0.0006477779307312751, n_hidden=1, n_neurons=59 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 51us/sample - loss: 2.7840 - val_loss: 3.1844 Epoch 2/100 7740/7740 [==============================] - 0s 36us/sample - loss: 1.2596 - val_loss: 0.9812 Epoch 3/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.8758 - val_loss: 0.7441 Epoch 4/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.7312 - val_loss: 0.6454 Epoch 5/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.6688 - val_loss: 0.6013 Epoch 6/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6341 - val_loss: 0.5979 Epoch 7/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.6103 - val_loss: 0.5533 Epoch 8/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5899 - val_loss: 0.5331 Epoch 9/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5719 - val_loss: 0.5214 Epoch 10/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5560 - val_loss: 0.5139 Epoch 11/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5418 - val_loss: 0.4965 Epoch 12/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5286 - val_loss: 0.4943 Epoch 13/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5171 - val_loss: 0.4726 Epoch 14/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5061 - val_loss: 0.4608 Epoch 15/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4962 - val_loss: 0.4600 Epoch 16/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4876 - val_loss: 0.4459 Epoch 17/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4794 - val_loss: 0.4467 Epoch 18/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4722 - val_loss: 0.4322 Epoch 19/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4656 - val_loss: 0.4305 Epoch 20/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4597 - val_loss: 0.4219 Epoch 21/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4540 - val_loss: 0.4187 Epoch 22/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4492 - val_loss: 0.4126 Epoch 23/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4442 - val_loss: 0.4112 Epoch 24/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4402 - val_loss: 0.4057 Epoch 25/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4363 - val_loss: 0.4048 Epoch 26/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4327 - val_loss: 0.4011 Epoch 27/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4291 - val_loss: 0.3973 Epoch 28/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4259 - val_loss: 0.3977 Epoch 29/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4230 - val_loss: 0.3935 Epoch 30/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4201 - val_loss: 0.3995 Epoch 31/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4176 - val_loss: 0.3938 Epoch 32/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4149 - val_loss: 0.3924 Epoch 33/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4125 - val_loss: 0.3842 Epoch 34/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4102 - val_loss: 0.3922 Epoch 35/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4080 - val_loss: 0.3810 Epoch 36/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4059 - val_loss: 0.3883 Epoch 37/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4039 - val_loss: 0.3848 Epoch 38/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4018 - val_loss: 0.3784 Epoch 39/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4001 - val_loss: 0.3841 Epoch 40/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3984 - val_loss: 0.3793 Epoch 41/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3966 - val_loss: 0.3837 Epoch 42/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3951 - val_loss: 0.3736 Epoch 43/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3934 - val_loss: 0.3825 Epoch 44/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3921 - val_loss: 0.3693 Epoch 45/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3905 - val_loss: 0.3694 Epoch 46/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3891 - val_loss: 0.3673 Epoch 47/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3875 - val_loss: 0.3753 Epoch 48/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3862 - val_loss: 0.3868 Epoch 49/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3850 - val_loss: 0.3735 Epoch 50/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3837 - val_loss: 0.3745 Epoch 51/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3825 - val_loss: 0.3742 Epoch 52/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3814 - val_loss: 0.3672 Epoch 53/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3800 - val_loss: 0.3624 Epoch 54/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3787 - val_loss: 0.3769 Epoch 55/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3778 - val_loss: 0.3609 Epoch 56/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3766 - val_loss: 0.3719 Epoch 57/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3757 - val_loss: 0.3732 Epoch 58/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3748 - val_loss: 0.3588 Epoch 59/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3736 - val_loss: 0.3681 Epoch 60/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3726 - val_loss: 0.3557 Epoch 61/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3716 - val_loss: 0.3600 Epoch 62/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3706 - val_loss: 0.3524 Epoch 63/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3699 - val_loss: 0.3550 Epoch 64/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3689 - val_loss: 0.3609 Epoch 65/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3681 - val_loss: 0.3504 Epoch 66/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3670 - val_loss: 0.3500 Epoch 67/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3662 - val_loss: 0.3657 Epoch 68/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3655 - val_loss: 0.3561 Epoch 69/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3647 - val_loss: 0.3559 Epoch 70/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3638 - val_loss: 0.3566 Epoch 71/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3631 - val_loss: 0.3482 Epoch 72/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3623 - val_loss: 0.3550 Epoch 73/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3617 - val_loss: 0.3494 Epoch 74/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3609 - val_loss: 0.3559 Epoch 75/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3602 - val_loss: 0.3543 Epoch 76/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3595 - val_loss: 0.3439 Epoch 77/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3588 - val_loss: 0.3552 Epoch 78/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3582 - val_loss: 0.3423 Epoch 79/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3576 - val_loss: 0.3480 Epoch 80/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3569 - val_loss: 0.3429 Epoch 81/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3562 - val_loss: 0.3474 Epoch 82/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3555 - val_loss: 0.3482 Epoch 83/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3551 - val_loss: 0.3444 Epoch 84/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3545 - val_loss: 0.3417 Epoch 85/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3538 - val_loss: 0.3400 Epoch 86/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3533 - val_loss: 0.3438 Epoch 87/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3526 - val_loss: 0.3545 Epoch 88/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3523 - val_loss: 0.3430 Epoch 89/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3515 - val_loss: 0.3515 Epoch 90/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3511 - val_loss: 0.3432 Epoch 91/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3505 - val_loss: 0.3404 Epoch 92/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3499 - val_loss: 0.3428 Epoch 93/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3496 - val_loss: 0.3470 Epoch 94/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3490 - val_loss: 0.3530 Epoch 95/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3485 - val_loss: 0.3673 3870/3870 [==============================] - 0s 13us/sample - loss: 0.3662 7740/7740 [==============================] - 0s 15us/sample - loss: 0.3478 [CV] learning_rate=0.0006477779307312751, n_hidden=1, n_neurons=59, total= 27.3s [CV] learning_rate=0.0006477779307312751, n_hidden=1, n_neurons=59 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 51us/sample - loss: 2.9243 - val_loss: 18.8442 Epoch 2/100 7740/7740 [==============================] - 0s 36us/sample - loss: 1.2074 - val_loss: 16.0418 Epoch 3/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.8573 - val_loss: 11.0685 Epoch 4/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.7260 - val_loss: 7.3959 Epoch 5/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6690 - val_loss: 5.0236 Epoch 6/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6350 - val_loss: 3.4955 Epoch 7/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6096 - val_loss: 2.3247 Epoch 8/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5878 - val_loss: 1.5633 Epoch 9/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5687 - val_loss: 1.0776 Epoch 10/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5517 - val_loss: 0.7538 Epoch 11/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5365 - val_loss: 0.5741 Epoch 12/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5228 - val_loss: 0.5018 Epoch 13/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5106 - val_loss: 0.4949 Epoch 14/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4997 - val_loss: 0.5297 Epoch 15/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4899 - val_loss: 0.5825 Epoch 16/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4812 - val_loss: 0.6560 Epoch 17/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.4736 - val_loss: 0.7104 Epoch 18/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4666 - val_loss: 0.7667 Epoch 19/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4604 - val_loss: 0.7889 Epoch 20/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4546 - val_loss: 0.8336 Epoch 21/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4495 - val_loss: 0.8446 Epoch 22/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4448 - val_loss: 0.8488 Epoch 23/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4403 - val_loss: 0.8735 3870/3870 [==============================] - 0s 14us/sample - loss: 0.4598 7740/7740 [==============================] - 0s 15us/sample - loss: 0.4382 [CV] learning_rate=0.0006477779307312751, n_hidden=1, n_neurons=59, total= 6.9s [CV] learning_rate=0.0006477779307312751, n_hidden=1, n_neurons=59 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 51us/sample - loss: 2.6866 - val_loss: 8.0515 Epoch 2/100 7740/7740 [==============================] - 0s 37us/sample - loss: 1.0829 - val_loss: 1.5668 Epoch 3/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.7425 - val_loss: 0.6838 Epoch 4/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6664 - val_loss: 0.6391 Epoch 5/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6360 - val_loss: 0.6036 Epoch 6/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6139 - val_loss: 0.5849 Epoch 7/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5957 - val_loss: 0.5632 Epoch 8/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5793 - val_loss: 0.5505 Epoch 9/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5648 - val_loss: 0.5358 Epoch 10/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5513 - val_loss: 0.5225 Epoch 11/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5389 - val_loss: 0.5139 Epoch 12/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5273 - val_loss: 0.4968 Epoch 13/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5164 - val_loss: 0.4871 Epoch 14/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.5063 - val_loss: 0.4761 Epoch 15/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4972 - val_loss: 0.4666 Epoch 16/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4882 - val_loss: 0.4606 Epoch 17/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4801 - val_loss: 0.4513 Epoch 18/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4728 - val_loss: 0.4440 Epoch 19/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4654 - val_loss: 0.4390 Epoch 20/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4594 - val_loss: 0.4303 Epoch 21/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4531 - val_loss: 0.4275 Epoch 22/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4476 - val_loss: 0.4231 Epoch 23/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4425 - val_loss: 0.4187 Epoch 24/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4376 - val_loss: 0.4136 Epoch 25/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4333 - val_loss: 0.4054 Epoch 26/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4293 - val_loss: 0.4118 Epoch 27/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4256 - val_loss: 0.4052 Epoch 28/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4221 - val_loss: 0.3981 Epoch 29/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4190 - val_loss: 0.3979 Epoch 30/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4159 - val_loss: 0.3940 Epoch 31/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4132 - val_loss: 0.3969 Epoch 32/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4106 - val_loss: 0.3897 Epoch 33/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4080 - val_loss: 0.3888 Epoch 34/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4058 - val_loss: 0.3832 Epoch 35/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4036 - val_loss: 0.3949 Epoch 36/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4017 - val_loss: 0.3838 Epoch 37/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3996 - val_loss: 0.3866 Epoch 38/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3978 - val_loss: 0.3872 Epoch 39/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3961 - val_loss: 0.3955 Epoch 40/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3944 - val_loss: 0.3855 Epoch 41/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3929 - val_loss: 0.3826 Epoch 42/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3914 - val_loss: 0.3714 Epoch 43/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3896 - val_loss: 0.3914 Epoch 44/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3885 - val_loss: 0.3794 Epoch 45/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3870 - val_loss: 0.3651 Epoch 46/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3854 - val_loss: 0.3952 Epoch 47/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3843 - val_loss: 0.3603 Epoch 48/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3831 - val_loss: 0.3862 Epoch 49/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3819 - val_loss: 0.3959 Epoch 50/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3807 - val_loss: 0.3726 Epoch 51/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3797 - val_loss: 0.3636 Epoch 52/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3783 - val_loss: 0.3939 Epoch 53/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3776 - val_loss: 0.3632 Epoch 54/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3760 - val_loss: 0.3598 Epoch 55/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3753 - val_loss: 0.3850 Epoch 56/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3745 - val_loss: 0.3665 Epoch 57/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3738 - val_loss: 0.3574 Epoch 58/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3724 - val_loss: 0.3859 Epoch 59/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3720 - val_loss: 0.3780 Epoch 60/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3710 - val_loss: 0.3745 Epoch 61/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3703 - val_loss: 0.3562 Epoch 62/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3694 - val_loss: 0.3705 Epoch 63/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3685 - val_loss: 0.3532 Epoch 64/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3675 - val_loss: 0.3531 Epoch 65/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3670 - val_loss: 0.3522 Epoch 66/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3660 - val_loss: 0.3824 Epoch 67/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3654 - val_loss: 0.3472 Epoch 68/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3649 - val_loss: 0.3528 Epoch 69/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3639 - val_loss: 0.3488 Epoch 70/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3634 - val_loss: 0.3521 Epoch 71/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3628 - val_loss: 0.3633 Epoch 72/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3621 - val_loss: 0.3724 Epoch 73/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3615 - val_loss: 0.3504 Epoch 74/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3606 - val_loss: 0.3757 Epoch 75/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3601 - val_loss: 0.3428 Epoch 76/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3597 - val_loss: 0.3600 Epoch 77/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3590 - val_loss: 0.3468 Epoch 78/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3584 - val_loss: 0.3391 Epoch 79/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3576 - val_loss: 0.3450 Epoch 80/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3571 - val_loss: 0.3658 Epoch 81/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3566 - val_loss: 0.3612 Epoch 82/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3561 - val_loss: 0.3525 Epoch 83/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3554 - val_loss: 0.3548 Epoch 84/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3550 - val_loss: 0.3449 Epoch 85/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3542 - val_loss: 0.3533 Epoch 86/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3542 - val_loss: 0.3527 Epoch 87/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3534 - val_loss: 0.3655 Epoch 88/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3530 - val_loss: 0.3569 3870/3870 [==============================] - 0s 14us/sample - loss: 0.3494 7740/7740 [==============================] - 0s 14us/sample - loss: 0.3521 [CV] learning_rate=0.0006477779307312751, n_hidden=1, n_neurons=59, total= 24.8s [CV] learning_rate=0.0032123503761513073, n_hidden=0, n_neurons=87 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 46us/sample - loss: 1.3665 - val_loss: 30.0074 Epoch 2/100 7740/7740 [==============================] - 0s 34us/sample - loss: 1.0455 - val_loss: 22.4981 Epoch 3/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.8411 - val_loss: 0.5801 Epoch 4/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.5291 - val_loss: 0.5369 Epoch 5/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4956 - val_loss: 0.5124 Epoch 6/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4722 - val_loss: 0.4677 Epoch 7/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4550 - val_loss: 0.4384 Epoch 8/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4421 - val_loss: 0.4157 Epoch 9/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4312 - val_loss: 0.3990 Epoch 10/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4227 - val_loss: 0.3904 Epoch 11/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4157 - val_loss: 0.3844 Epoch 12/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4095 - val_loss: 0.3846 Epoch 13/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4053 - val_loss: 0.3783 Epoch 14/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4016 - val_loss: 0.3811 Epoch 15/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3976 - val_loss: 0.3855 Epoch 16/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3946 - val_loss: 0.3793 Epoch 17/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3917 - val_loss: 0.3790 Epoch 18/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3892 - val_loss: 0.3795 Epoch 19/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3868 - val_loss: 0.3751 Epoch 20/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3844 - val_loss: 0.3905 Epoch 21/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3830 - val_loss: 0.3758 Epoch 22/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3806 - val_loss: 0.3810 Epoch 23/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3790 - val_loss: 0.3810 Epoch 24/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3773 - val_loss: 0.3796 Epoch 25/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3763 - val_loss: 0.3838 Epoch 26/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3739 - val_loss: 0.3877 Epoch 27/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3731 - val_loss: 0.3776 Epoch 28/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3716 - val_loss: 0.3778 Epoch 29/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3702 - val_loss: 0.3812 3870/3870 [==============================] - 0s 13us/sample - loss: 0.3857 7740/7740 [==============================] - 0s 13us/sample - loss: 0.3677 [CV] learning_rate=0.0032123503761513073, n_hidden=0, n_neurons=87, total= 7.9s [CV] learning_rate=0.0032123503761513073, n_hidden=0, n_neurons=87 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 46us/sample - loss: 1.3253 - val_loss: 1.7528 Epoch 2/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.6961 - val_loss: 0.6533 Epoch 3/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.6193 - val_loss: 1.3104 Epoch 4/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5629 - val_loss: 2.2476 Epoch 5/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5220 - val_loss: 2.6183 Epoch 6/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4918 - val_loss: 2.7712 Epoch 7/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4692 - val_loss: 2.4734 Epoch 8/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4537 - val_loss: 2.0569 Epoch 9/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4415 - val_loss: 1.6704 Epoch 10/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4329 - val_loss: 1.3330 Epoch 11/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4249 - val_loss: 0.9791 Epoch 12/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4199 - val_loss: 0.8599 3870/3870 [==============================] - 0s 13us/sample - loss: 0.4479 7740/7740 [==============================] - 0s 14us/sample - loss: 0.4163 [CV] learning_rate=0.0032123503761513073, n_hidden=0, n_neurons=87, total= 3.5s [CV] learning_rate=0.0032123503761513073, n_hidden=0, n_neurons=87 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 47us/sample - loss: 1.5445 - val_loss: 15.2564 Epoch 2/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.9030 - val_loss: 5.7446 Epoch 3/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.6692 - val_loss: 0.6443 Epoch 4/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5637 - val_loss: 0.5236 Epoch 5/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5269 - val_loss: 0.4840 Epoch 6/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4986 - val_loss: 0.4984 Epoch 7/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4779 - val_loss: 0.4490 Epoch 8/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4620 - val_loss: 0.4321 Epoch 9/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4504 - val_loss: 0.4292 Epoch 10/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4414 - val_loss: 0.4454 Epoch 11/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4341 - val_loss: 0.4018 Epoch 12/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4281 - val_loss: 0.4089 Epoch 13/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4226 - val_loss: 0.4120 Epoch 14/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4179 - val_loss: 0.4455 Epoch 15/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4138 - val_loss: 0.4541 Epoch 16/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4108 - val_loss: 0.4184 Epoch 17/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4075 - val_loss: 0.4369 Epoch 18/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4044 - val_loss: 0.3751 Epoch 19/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4026 - val_loss: 0.3771 Epoch 20/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3994 - val_loss: 0.4666 Epoch 21/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3988 - val_loss: 0.4131 Epoch 22/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3952 - val_loss: 0.3811 Epoch 23/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3930 - val_loss: 0.3736 Epoch 24/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3915 - val_loss: 0.3913 Epoch 25/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3897 - val_loss: 0.3709 Epoch 26/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3880 - val_loss: 0.3942 Epoch 27/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3863 - val_loss: 0.3592 Epoch 28/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3848 - val_loss: 0.3652 Epoch 29/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3830 - val_loss: 0.3596 Epoch 30/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3815 - val_loss: 0.4866 Epoch 31/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3811 - val_loss: 0.3524 Epoch 32/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3783 - val_loss: 0.5189 Epoch 33/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3787 - val_loss: 0.3859 Epoch 34/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3761 - val_loss: 0.3516 Epoch 35/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3747 - val_loss: 0.5188 Epoch 36/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3748 - val_loss: 0.3542 Epoch 37/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3734 - val_loss: 0.5031 Epoch 38/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3737 - val_loss: 0.3494 Epoch 39/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3713 - val_loss: 0.3880 Epoch 40/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3697 - val_loss: 0.3591 Epoch 41/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3686 - val_loss: 0.4488 Epoch 42/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3683 - val_loss: 0.3543 Epoch 43/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3671 - val_loss: 0.3664 Epoch 44/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3662 - val_loss: 0.3420 Epoch 45/100 7740/7740 [==============================] - 0s 32us/sample - loss: 0.3657 - val_loss: 0.3827 Epoch 46/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3648 - val_loss: 0.3587 Epoch 47/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3640 - val_loss: 0.3558 Epoch 48/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3629 - val_loss: 0.5029 Epoch 49/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3633 - val_loss: 0.3404 Epoch 50/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3614 - val_loss: 0.5297 Epoch 51/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3627 - val_loss: 0.3581 Epoch 52/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3605 - val_loss: 0.4724 Epoch 53/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3612 - val_loss: 0.3414 Epoch 54/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3593 - val_loss: 0.3880 Epoch 55/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3583 - val_loss: 0.3962 Epoch 56/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3576 - val_loss: 0.4286 Epoch 57/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3569 - val_loss: 0.3945 Epoch 58/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3561 - val_loss: 0.3448 Epoch 59/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3563 - val_loss: 0.4321 3870/3870 [==============================] - 0s 13us/sample - loss: 0.3558 7740/7740 [==============================] - 0s 14us/sample - loss: 0.3563 [CV] learning_rate=0.0032123503761513073, n_hidden=0, n_neurons=87, total= 15.7s [CV] learning_rate=0.004576909498448105, n_hidden=2, n_neurons=87 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 58us/sample - loss: 1.0841 - val_loss: 1.6728 Epoch 2/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.5127 - val_loss: 3.2044 Epoch 3/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.4594 - val_loss: 0.4278 Epoch 4/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.4003 - val_loss: 0.4229 Epoch 5/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3793 - val_loss: 0.4236 Epoch 6/100 7740/7740 [==============================] - 0s 42us/sample - loss: 0.3653 - val_loss: 0.4169 Epoch 7/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3580 - val_loss: 0.4311 Epoch 8/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3524 - val_loss: 0.3587 Epoch 9/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3437 - val_loss: 0.4140 Epoch 10/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3405 - val_loss: 0.3898 Epoch 11/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3361 - val_loss: 0.3298 Epoch 12/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3320 - val_loss: 0.3523 Epoch 13/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3285 - val_loss: 0.3785 Epoch 14/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3275 - val_loss: 0.3206 Epoch 15/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3239 - val_loss: 0.3414 Epoch 16/100 7740/7740 [==============================] - 0s 42us/sample - loss: 0.3224 - val_loss: 0.3518 Epoch 17/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3189 - val_loss: 0.3833 Epoch 18/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3168 - val_loss: 0.3239 Epoch 19/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3165 - val_loss: 0.4082 Epoch 20/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3133 - val_loss: 0.3160 Epoch 21/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3114 - val_loss: 0.3317 Epoch 22/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3098 - val_loss: 0.3171 Epoch 23/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3073 - val_loss: 0.3239 Epoch 24/100 7740/7740 [==============================] - 0s 44us/sample - loss: 0.3062 - val_loss: 0.3073 Epoch 25/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3044 - val_loss: 0.3243 Epoch 26/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3034 - val_loss: 0.3801 Epoch 27/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3025 - val_loss: 0.3579 Epoch 28/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3026 - val_loss: 0.3355 Epoch 29/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3003 - val_loss: 0.3455 Epoch 30/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.2983 - val_loss: 0.3128 Epoch 31/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.2955 - val_loss: 0.3165 Epoch 32/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.2940 - val_loss: 0.3149 Epoch 33/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.2909 - val_loss: 0.3912 Epoch 34/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.2930 - val_loss: 0.3209 3870/3870 [==============================] - 0s 16us/sample - loss: 0.3271 7740/7740 [==============================] - 0s 17us/sample - loss: 0.2891 [CV] learning_rate=0.004576909498448105, n_hidden=2, n_neurons=87, total= 11.3s [CV] learning_rate=0.004576909498448105, n_hidden=2, n_neurons=87 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 58us/sample - loss: 1.0489 - val_loss: 1.0151 Epoch 2/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.5291 - val_loss: 0.5506 Epoch 3/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.4577 - val_loss: 0.4543 Epoch 4/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.4166 - val_loss: 0.4211 Epoch 5/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3932 - val_loss: 0.4991 Epoch 6/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3774 - val_loss: 0.6786 Epoch 7/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3659 - val_loss: 0.9418 Epoch 8/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3593 - val_loss: 0.8760 Epoch 9/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3531 - val_loss: 0.9404 Epoch 10/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3499 - val_loss: 1.0715 Epoch 11/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3454 - val_loss: 0.9127 Epoch 12/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3412 - val_loss: 1.0588 Epoch 13/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3385 - val_loss: 0.9105 Epoch 14/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3345 - val_loss: 0.9548 3870/3870 [==============================] - 0s 16us/sample - loss: 0.3628 7740/7740 [==============================] - 0s 17us/sample - loss: 0.3311 [CV] learning_rate=0.004576909498448105, n_hidden=2, n_neurons=87, total= 4.9s [CV] learning_rate=0.004576909498448105, n_hidden=2, n_neurons=87 .... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 58us/sample - loss: 0.9595 - val_loss: 10.8814 Epoch 2/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.6306 - val_loss: 2.7405 Epoch 3/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.4783 - val_loss: 0.4214 Epoch 4/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.4204 - val_loss: 0.3964 Epoch 5/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3979 - val_loss: 0.4372 Epoch 6/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3852 - val_loss: 0.4389 Epoch 7/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3759 - val_loss: 0.4287 Epoch 8/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3691 - val_loss: 0.4075 Epoch 9/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3640 - val_loss: 0.4152 Epoch 10/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3600 - val_loss: 0.4284 Epoch 11/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3548 - val_loss: 0.3699 Epoch 12/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3526 - val_loss: 0.4052 Epoch 13/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3497 - val_loss: 0.3468 Epoch 14/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3462 - val_loss: 0.3865 Epoch 15/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3427 - val_loss: 0.3764 Epoch 16/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3404 - val_loss: 0.3697 Epoch 17/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3379 - val_loss: 0.3259 Epoch 18/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3358 - val_loss: 0.3488 Epoch 19/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3335 - val_loss: 0.3473 Epoch 20/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3308 - val_loss: 0.3886 Epoch 21/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3314 - val_loss: 0.3202 Epoch 22/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3279 - val_loss: 0.3620 Epoch 23/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3263 - val_loss: 0.3380 Epoch 24/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3246 - val_loss: 0.4124 Epoch 25/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3234 - val_loss: 0.3171 Epoch 26/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3224 - val_loss: 0.3410 Epoch 27/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3195 - val_loss: 0.3358 Epoch 28/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3169 - val_loss: 0.3893 Epoch 29/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3175 - val_loss: 0.3208 Epoch 30/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3168 - val_loss: 0.3512 Epoch 31/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3147 - val_loss: 0.3236 Epoch 32/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3111 - val_loss: 0.3870 Epoch 33/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3105 - val_loss: 0.4080 Epoch 34/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.3108 - val_loss: 0.3167 Epoch 35/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3102 - val_loss: 0.3398 Epoch 36/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.3071 - val_loss: 0.3612 Epoch 37/100 7740/7740 [==============================] - 0s 42us/sample - loss: 0.3062 - val_loss: 0.3460 Epoch 38/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3046 - val_loss: 0.4104 Epoch 39/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3079 - val_loss: 0.2997 Epoch 40/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3056 - val_loss: 0.3803 Epoch 41/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3041 - val_loss: 0.3085 Epoch 42/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3026 - val_loss: 0.3959 Epoch 43/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.3006 - val_loss: 0.3498 Epoch 44/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.2981 - val_loss: 0.3024 Epoch 45/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.2986 - val_loss: 0.3475 Epoch 46/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.2949 - val_loss: 0.3191 Epoch 47/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.2946 - val_loss: 0.3201 Epoch 48/100 7740/7740 [==============================] - 0s 40us/sample - loss: 0.2937 - val_loss: 0.3001 Epoch 49/100 7740/7740 [==============================] - 0s 41us/sample - loss: 0.2929 - val_loss: 0.3431 3870/3870 [==============================] - 0s 15us/sample - loss: 0.3356 7740/7740 [==============================] - 0s 16us/sample - loss: 0.3133 [CV] learning_rate=0.004576909498448105, n_hidden=2, n_neurons=87, total= 15.6s [CV] learning_rate=0.0011714615165291644, n_hidden=0, n_neurons=56 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 46us/sample - loss: 2.4510 - val_loss: 5.1025 Epoch 2/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.9274 - val_loss: 0.8434 Epoch 3/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.7575 - val_loss: 0.7224 Epoch 4/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.7072 - val_loss: 0.6691 Epoch 5/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.6692 - val_loss: 0.6309 Epoch 6/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.6383 - val_loss: 0.6009 Epoch 7/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.6099 - val_loss: 0.5716 Epoch 8/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.5841 - val_loss: 0.5599 Epoch 9/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.5598 - val_loss: 0.5321 Epoch 10/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.5414 - val_loss: 0.5070 Epoch 11/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.5225 - val_loss: 0.5337 Epoch 12/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.5066 - val_loss: 0.4755 Epoch 13/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4927 - val_loss: 0.4675 Epoch 14/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4805 - val_loss: 0.4664 Epoch 15/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4693 - val_loss: 0.4675 Epoch 16/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4607 - val_loss: 0.4314 Epoch 17/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4532 - val_loss: 0.4594 Epoch 18/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4460 - val_loss: 0.4296 Epoch 19/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4400 - val_loss: 0.4329 Epoch 20/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4350 - val_loss: 0.4289 Epoch 21/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4304 - val_loss: 0.4344 Epoch 22/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4264 - val_loss: 0.4139 Epoch 23/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4227 - val_loss: 0.4219 Epoch 24/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4191 - val_loss: 0.4107 Epoch 25/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4165 - val_loss: 0.4236 Epoch 26/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4137 - val_loss: 0.4210 Epoch 27/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4110 - val_loss: 0.4112 Epoch 28/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4091 - val_loss: 0.4074 Epoch 29/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4068 - val_loss: 0.4051 Epoch 30/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4047 - val_loss: 0.3898 Epoch 31/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4025 - val_loss: 0.4200 Epoch 32/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4012 - val_loss: 0.3944 Epoch 33/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3991 - val_loss: 0.3930 Epoch 34/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3976 - val_loss: 0.4333 Epoch 35/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3957 - val_loss: 0.3851 Epoch 36/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3948 - val_loss: 0.3853 Epoch 37/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3930 - val_loss: 0.3914 Epoch 38/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3915 - val_loss: 0.4464 Epoch 39/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3907 - val_loss: 0.3995 Epoch 40/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3896 - val_loss: 0.4028 Epoch 41/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3882 - val_loss: 0.3802 Epoch 42/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3859 - val_loss: 0.3964 Epoch 43/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3854 - val_loss: 0.4256 Epoch 44/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3844 - val_loss: 0.3978 Epoch 45/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3836 - val_loss: 0.3725 Epoch 46/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3819 - val_loss: 0.4094 Epoch 47/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3822 - val_loss: 0.3933 Epoch 48/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3808 - val_loss: 0.3953 Epoch 49/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3797 - val_loss: 0.4138 Epoch 50/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3788 - val_loss: 0.3889 Epoch 51/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3782 - val_loss: 0.3666 Epoch 52/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3773 - val_loss: 0.4196 Epoch 53/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3768 - val_loss: 0.3661 Epoch 54/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3759 - val_loss: 0.3684 Epoch 55/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3753 - val_loss: 0.3827 Epoch 56/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3739 - val_loss: 0.3700 Epoch 57/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3738 - val_loss: 0.4040 Epoch 58/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3734 - val_loss: 0.4116 Epoch 59/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3726 - val_loss: 0.3979 Epoch 60/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3716 - val_loss: 0.3631 Epoch 61/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3714 - val_loss: 0.4177 Epoch 62/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3709 - val_loss: 0.3661 Epoch 63/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3700 - val_loss: 0.4091 Epoch 64/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3696 - val_loss: 0.3683 Epoch 65/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3692 - val_loss: 0.3736 Epoch 66/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3687 - val_loss: 0.3813 Epoch 67/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3679 - val_loss: 0.3978 Epoch 68/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3676 - val_loss: 0.3629 Epoch 69/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3667 - val_loss: 0.3627 Epoch 70/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3669 - val_loss: 0.3584 Epoch 71/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3658 - val_loss: 0.3676 Epoch 72/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3658 - val_loss: 0.3675 Epoch 73/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3650 - val_loss: 0.4163 Epoch 74/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3647 - val_loss: 0.3869 Epoch 75/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3644 - val_loss: 0.3578 Epoch 76/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3634 - val_loss: 0.4146 Epoch 77/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3635 - val_loss: 0.3891 Epoch 78/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3629 - val_loss: 0.3620 Epoch 79/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3623 - val_loss: 0.3643 Epoch 80/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3620 - val_loss: 0.3540 Epoch 81/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3616 - val_loss: 0.4140 Epoch 82/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3612 - val_loss: 0.3953 Epoch 83/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3608 - val_loss: 0.4081 Epoch 84/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3603 - val_loss: 0.3990 Epoch 85/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3602 - val_loss: 0.3479 Epoch 86/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3590 - val_loss: 0.4146 Epoch 87/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3596 - val_loss: 0.3958 Epoch 88/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3588 - val_loss: 0.3643 Epoch 89/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3581 - val_loss: 0.3519 Epoch 90/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3577 - val_loss: 0.4248 Epoch 91/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3577 - val_loss: 0.3980 Epoch 92/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3572 - val_loss: 0.3431 Epoch 93/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3568 - val_loss: 0.4402 Epoch 94/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3570 - val_loss: 0.3865 Epoch 95/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3560 - val_loss: 0.3469 Epoch 96/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3558 - val_loss: 0.3514 Epoch 97/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3556 - val_loss: 0.4285 Epoch 98/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3553 - val_loss: 0.3403 Epoch 99/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3549 - val_loss: 0.3584 Epoch 100/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3539 - val_loss: 0.3579 3870/3870 [==============================] - 0s 12us/sample - loss: 0.3695 7740/7740 [==============================] - 0s 14us/sample - loss: 0.3534 [CV] learning_rate=0.0011714615165291644, n_hidden=0, n_neurons=56, total= 26.8s [CV] learning_rate=0.0011714615165291644, n_hidden=0, n_neurons=56 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 46us/sample - loss: 2.2833 - val_loss: 9.3171 Epoch 2/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.8871 - val_loss: 6.2420 Epoch 3/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.7421 - val_loss: 3.4939 Epoch 4/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.6852 - val_loss: 1.9102 Epoch 5/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.6445 - val_loss: 1.0602 Epoch 6/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.6116 - val_loss: 0.6847 Epoch 7/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5845 - val_loss: 0.5604 Epoch 8/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5618 - val_loss: 0.5700 Epoch 9/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5430 - val_loss: 0.6534 Epoch 10/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5268 - val_loss: 0.7650 Epoch 11/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5133 - val_loss: 0.8208 Epoch 12/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5014 - val_loss: 0.8494 Epoch 13/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4916 - val_loss: 0.8949 Epoch 14/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4826 - val_loss: 0.9046 Epoch 15/100 7740/7740 [==============================] - 0s 32us/sample - loss: 0.4747 - val_loss: 0.8815 Epoch 16/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4677 - val_loss: 0.8587 Epoch 17/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4616 - val_loss: 0.7927 3870/3870 [==============================] - 0s 12us/sample - loss: 0.4803 7740/7740 [==============================] - 0s 14us/sample - loss: 0.4579 [CV] learning_rate=0.0011714615165291644, n_hidden=0, n_neurons=56, total= 4.7s [CV] learning_rate=0.0011714615165291644, n_hidden=0, n_neurons=56 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 47us/sample - loss: 2.0724 - val_loss: 1.4705 Epoch 2/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.7958 - val_loss: 0.7538 Epoch 3/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.6896 - val_loss: 0.6508 Epoch 4/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.6477 - val_loss: 0.6159 Epoch 5/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.6167 - val_loss: 0.5844 Epoch 6/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5906 - val_loss: 0.5521 Epoch 7/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.5675 - val_loss: 0.5322 Epoch 8/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5478 - val_loss: 0.5150 Epoch 9/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5305 - val_loss: 0.5061 Epoch 10/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.5161 - val_loss: 0.4909 Epoch 11/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.5025 - val_loss: 0.4726 Epoch 12/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4914 - val_loss: 0.4612 Epoch 13/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4815 - val_loss: 0.4534 Epoch 14/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4728 - val_loss: 0.4438 Epoch 15/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4650 - val_loss: 0.4357 Epoch 16/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4585 - val_loss: 0.4313 Epoch 17/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4522 - val_loss: 0.4274 Epoch 18/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4470 - val_loss: 0.4222 Epoch 19/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4419 - val_loss: 0.4129 Epoch 20/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4377 - val_loss: 0.4086 Epoch 21/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4337 - val_loss: 0.4057 Epoch 22/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4300 - val_loss: 0.4034 Epoch 23/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4266 - val_loss: 0.3982 Epoch 24/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4237 - val_loss: 0.3950 Epoch 25/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4206 - val_loss: 0.4003 Epoch 26/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4182 - val_loss: 0.3956 Epoch 27/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4158 - val_loss: 0.3966 Epoch 28/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4134 - val_loss: 0.3945 Epoch 29/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4114 - val_loss: 0.3836 Epoch 30/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4094 - val_loss: 0.3941 Epoch 31/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4074 - val_loss: 0.3814 Epoch 32/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4053 - val_loss: 0.3792 Epoch 33/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4041 - val_loss: 0.3928 Epoch 34/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.4027 - val_loss: 0.3802 Epoch 35/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.4010 - val_loss: 0.3835 Epoch 36/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3997 - val_loss: 0.3741 Epoch 37/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3981 - val_loss: 0.3885 Epoch 38/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3974 - val_loss: 0.3860 Epoch 39/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3962 - val_loss: 0.3697 Epoch 40/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3949 - val_loss: 0.3734 Epoch 41/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3940 - val_loss: 0.3706 Epoch 42/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3928 - val_loss: 0.3909 Epoch 43/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3920 - val_loss: 0.3772 Epoch 44/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3908 - val_loss: 0.3750 Epoch 45/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3900 - val_loss: 0.3648 Epoch 46/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3891 - val_loss: 0.3807 Epoch 47/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3887 - val_loss: 0.3625 Epoch 48/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3876 - val_loss: 0.3746 Epoch 49/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3867 - val_loss: 0.3754 Epoch 50/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3864 - val_loss: 0.3645 Epoch 51/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3851 - val_loss: 0.3951 Epoch 52/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3845 - val_loss: 0.3832 Epoch 53/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3841 - val_loss: 0.3613 Epoch 54/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3829 - val_loss: 0.3953 Epoch 55/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3825 - val_loss: 0.3757 Epoch 56/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3818 - val_loss: 0.3647 Epoch 57/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3810 - val_loss: 0.3959 Epoch 58/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3807 - val_loss: 0.3563 Epoch 59/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3802 - val_loss: 0.3882 Epoch 60/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3795 - val_loss: 0.3784 Epoch 61/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3786 - val_loss: 0.3908 Epoch 62/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3785 - val_loss: 0.3719 Epoch 63/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3778 - val_loss: 0.3788 Epoch 64/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3774 - val_loss: 0.3566 Epoch 65/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3766 - val_loss: 0.3630 Epoch 66/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3764 - val_loss: 0.3537 Epoch 67/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3756 - val_loss: 0.3850 Epoch 68/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3755 - val_loss: 0.3769 Epoch 69/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3749 - val_loss: 0.3656 Epoch 70/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3744 - val_loss: 0.3584 Epoch 71/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3739 - val_loss: 0.3706 Epoch 72/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3735 - val_loss: 0.3533 Epoch 73/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3731 - val_loss: 0.3556 Epoch 74/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3724 - val_loss: 0.3960 Epoch 75/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3724 - val_loss: 0.3755 Epoch 76/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3718 - val_loss: 0.3667 Epoch 77/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3715 - val_loss: 0.3575 Epoch 78/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3708 - val_loss: 0.3851 Epoch 79/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3707 - val_loss: 0.3694 Epoch 80/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3702 - val_loss: 0.3824 Epoch 81/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3700 - val_loss: 0.3673 Epoch 82/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3696 - val_loss: 0.3491 Epoch 83/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3691 - val_loss: 0.3479 Epoch 84/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3683 - val_loss: 0.3513 Epoch 85/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3683 - val_loss: 0.3882 Epoch 86/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3679 - val_loss: 0.3776 Epoch 87/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3677 - val_loss: 0.3607 Epoch 88/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3670 - val_loss: 0.3911 Epoch 89/100 7740/7740 [==============================] - 0s 33us/sample - loss: 0.3670 - val_loss: 0.3468 Epoch 90/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3667 - val_loss: 0.3470 Epoch 91/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3661 - val_loss: 0.3466 Epoch 92/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3658 - val_loss: 0.3778 Epoch 93/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3654 - val_loss: 0.3447 Epoch 94/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.3649 - val_loss: 0.3542 Epoch 95/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3647 - val_loss: 0.3634 Epoch 96/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3646 - val_loss: 0.3474 Epoch 97/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3640 - val_loss: 0.3834 Epoch 98/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3640 - val_loss: 0.3535 Epoch 99/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3636 - val_loss: 0.3502 Epoch 100/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3630 - val_loss: 0.3472 3870/3870 [==============================] - 0s 13us/sample - loss: 0.3650 7740/7740 [==============================] - 0s 14us/sample - loss: 0.3623 [CV] learning_rate=0.0011714615165291644, n_hidden=0, n_neurons=56, total= 26.5s [CV] learning_rate=0.0012578066689117673, n_hidden=1, n_neurons=60 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 52us/sample - loss: 2.0678 - val_loss: 1.4867 Epoch 2/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.7582 - val_loss: 0.7377 Epoch 3/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.6402 - val_loss: 0.5802 Epoch 4/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5891 - val_loss: 0.5485 Epoch 5/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.5528 - val_loss: 0.5201 Epoch 6/100 7740/7740 [==============================] - 0s 39us/sample - loss: 0.5260 - val_loss: 0.4890 Epoch 7/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.5025 - val_loss: 0.4603 Epoch 8/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4839 - val_loss: 0.4542 Epoch 9/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4686 - val_loss: 0.4417 Epoch 10/100 7740/7740 [==============================] - 0s 38us/sample - loss: 0.4565 - val_loss: 0.4411 Epoch 11/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4466 - val_loss: 0.4201 Epoch 12/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4381 - val_loss: 0.4113 Epoch 13/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4314 - val_loss: 0.4170 Epoch 14/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4250 - val_loss: 0.4002 Epoch 15/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4195 - val_loss: 0.4316 Epoch 16/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4149 - val_loss: 0.3887 Epoch 17/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4103 - val_loss: 0.3882 Epoch 18/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4060 - val_loss: 0.4235 Epoch 19/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.4030 - val_loss: 0.4081 Epoch 20/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3990 - val_loss: 0.4274 Epoch 21/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3962 - val_loss: 0.3974 Epoch 22/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3929 - val_loss: 0.3794 Epoch 23/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3899 - val_loss: 0.4050 Epoch 24/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3879 - val_loss: 0.3742 Epoch 25/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3849 - val_loss: 0.3693 Epoch 26/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3824 - val_loss: 0.3617 Epoch 27/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3800 - val_loss: 0.3715 Epoch 28/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3780 - val_loss: 0.3569 Epoch 29/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3758 - val_loss: 0.3695 Epoch 30/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3735 - val_loss: 0.3967 Epoch 31/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3723 - val_loss: 0.3551 Epoch 32/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3700 - val_loss: 0.3600 Epoch 33/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3680 - val_loss: 0.3780 Epoch 34/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3665 - val_loss: 0.3552 Epoch 35/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3650 - val_loss: 0.3626 Epoch 36/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3631 - val_loss: 0.3454 Epoch 37/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3617 - val_loss: 0.3514 Epoch 38/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3603 - val_loss: 0.3436 Epoch 39/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3589 - val_loss: 0.3621 Epoch 40/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3575 - val_loss: 0.3398 Epoch 41/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3559 - val_loss: 0.3806 Epoch 42/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3551 - val_loss: 0.3377 Epoch 43/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3535 - val_loss: 0.3823 Epoch 44/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3525 - val_loss: 0.3376 Epoch 45/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3513 - val_loss: 0.3599 Epoch 46/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3497 - val_loss: 0.3378 Epoch 47/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3489 - val_loss: 0.3848 Epoch 48/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3482 - val_loss: 0.3528 Epoch 49/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3468 - val_loss: 0.3474 Epoch 50/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3459 - val_loss: 0.3603 Epoch 51/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3451 - val_loss: 0.3538 Epoch 52/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3440 - val_loss: 0.3315 Epoch 53/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3433 - val_loss: 0.3428 Epoch 54/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3425 - val_loss: 0.3466 Epoch 55/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3414 - val_loss: 0.3303 Epoch 56/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3408 - val_loss: 0.3706 Epoch 57/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3401 - val_loss: 0.3334 Epoch 58/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3386 - val_loss: 0.3698 Epoch 59/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3383 - val_loss: 0.3272 Epoch 60/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3373 - val_loss: 0.4134 Epoch 61/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3370 - val_loss: 0.3250 Epoch 62/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3361 - val_loss: 0.4060 Epoch 63/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3357 - val_loss: 0.3332 Epoch 64/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3347 - val_loss: 0.3328 Epoch 65/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3341 - val_loss: 0.3264 Epoch 66/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3335 - val_loss: 0.3295 Epoch 67/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3329 - val_loss: 0.3599 Epoch 68/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3321 - val_loss: 0.3459 Epoch 69/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3313 - val_loss: 0.3226 Epoch 70/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3308 - val_loss: 0.3513 Epoch 71/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3302 - val_loss: 0.3653 Epoch 72/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3300 - val_loss: 0.3384 Epoch 73/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3289 - val_loss: 0.3897 Epoch 74/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3286 - val_loss: 0.3191 Epoch 75/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3282 - val_loss: 0.3525 Epoch 76/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3274 - val_loss: 0.3197 Epoch 77/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3268 - val_loss: 0.4065 Epoch 78/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3270 - val_loss: 0.3257 Epoch 79/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3256 - val_loss: 0.3260 Epoch 80/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3253 - val_loss: 0.3684 Epoch 81/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3251 - val_loss: 0.3202 Epoch 82/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3245 - val_loss: 0.4489 Epoch 83/100 7740/7740 [==============================] - 0s 35us/sample - loss: 0.3246 - val_loss: 0.3344 Epoch 84/100 7740/7740 [==============================] - 0s 34us/sample - loss: 0.3239 - val_loss: 0.4869 3870/3870 [==============================] - 0s 13us/sample - loss: 0.3442 7740/7740 [==============================] - 0s 15us/sample - loss: 0.3227 [CV] learning_rate=0.0012578066689117673, n_hidden=1, n_neurons=60, total= 23.7s [CV] learning_rate=0.0012578066689117673, n_hidden=1, n_neurons=60 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 50us/sample - loss: 1.7817 - val_loss: 9.1129 Epoch 2/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.7881 - val_loss: 4.1277 Epoch 3/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6598 - val_loss: 1.6439 Epoch 4/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6022 - val_loss: 0.8057 Epoch 5/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5604 - val_loss: 0.5135 Epoch 6/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5278 - val_loss: 0.5437 Epoch 7/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5023 - val_loss: 0.7848 Epoch 8/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4825 - val_loss: 0.9492 Epoch 9/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4660 - val_loss: 1.0810 Epoch 10/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4530 - val_loss: 1.1596 Epoch 11/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4423 - val_loss: 1.1965 Epoch 12/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4338 - val_loss: 1.2383 Epoch 13/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.4256 - val_loss: 1.1621 Epoch 14/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4193 - val_loss: 1.0986 Epoch 15/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4133 - val_loss: 1.0196 3870/3870 [==============================] - 0s 14us/sample - loss: 0.4393 7740/7740 [==============================] - 0s 15us/sample - loss: 0.4098 [CV] learning_rate=0.0012578066689117673, n_hidden=1, n_neurons=60, total= 4.6s [CV] learning_rate=0.0012578066689117673, n_hidden=1, n_neurons=60 ... Train on 7740 samples, validate on 3870 samples Epoch 1/100 7740/7740 [==============================] - 0s 51us/sample - loss: 2.1548 - val_loss: 2.2374 Epoch 2/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.8591 - val_loss: 0.7902 Epoch 3/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.7066 - val_loss: 0.6506 Epoch 4/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6480 - val_loss: 0.6075 Epoch 5/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.6086 - val_loss: 0.5687 Epoch 6/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5752 - val_loss: 0.5391 Epoch 7/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5474 - val_loss: 0.5183 Epoch 8/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5239 - val_loss: 0.4882 Epoch 9/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.5039 - val_loss: 0.4673 Epoch 10/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4862 - val_loss: 0.4519 Epoch 11/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4721 - val_loss: 0.4395 Epoch 12/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4603 - val_loss: 0.4273 Epoch 13/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4498 - val_loss: 0.4201 Epoch 14/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4415 - val_loss: 0.4119 Epoch 15/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4346 - val_loss: 0.4132 Epoch 16/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4276 - val_loss: 0.3996 Epoch 17/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4230 - val_loss: 0.4010 Epoch 18/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4180 - val_loss: 0.3988 Epoch 19/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4138 - val_loss: 0.3940 Epoch 20/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4100 - val_loss: 0.3974 Epoch 21/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4067 - val_loss: 0.4028 Epoch 22/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4044 - val_loss: 0.4049 Epoch 23/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.4006 - val_loss: 0.3939 Epoch 24/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3971 - val_loss: 0.3815 Epoch 25/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3951 - val_loss: 0.3838 Epoch 26/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3922 - val_loss: 0.3768 Epoch 27/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3902 - val_loss: 0.3716 Epoch 28/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3872 - val_loss: 0.3778 Epoch 29/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3859 - val_loss: 0.3884 Epoch 30/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3841 - val_loss: 0.3782 Epoch 31/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3822 - val_loss: 0.3698 Epoch 32/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3807 - val_loss: 0.3677 Epoch 33/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3786 - val_loss: 0.3782 Epoch 34/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3770 - val_loss: 0.3646 Epoch 35/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3757 - val_loss: 0.3699 Epoch 36/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3742 - val_loss: 0.3663 Epoch 37/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3728 - val_loss: 0.3685 Epoch 38/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3710 - val_loss: 0.3635 Epoch 39/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3699 - val_loss: 0.3762 Epoch 40/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3693 - val_loss: 0.3657 Epoch 41/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3679 - val_loss: 0.3567 Epoch 42/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3662 - val_loss: 0.3659 Epoch 43/100 7740/7740 [==============================] - 0s 37us/sample - loss: 0.3652 - val_loss: 0.3720 Epoch 44/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3645 - val_loss: 0.3576 Epoch 45/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3633 - val_loss: 0.3676 Epoch 46/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3623 - val_loss: 0.3505 Epoch 47/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3613 - val_loss: 0.3488 Epoch 48/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3604 - val_loss: 0.3592 Epoch 49/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3598 - val_loss: 0.3445 Epoch 50/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3590 - val_loss: 0.3681 Epoch 51/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3581 - val_loss: 0.3576 Epoch 52/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3569 - val_loss: 0.3442 Epoch 53/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3563 - val_loss: 0.3443 Epoch 54/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3558 - val_loss: 0.3566 Epoch 55/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3552 - val_loss: 0.3392 Epoch 56/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3546 - val_loss: 0.3462 Epoch 57/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3537 - val_loss: 0.3421 Epoch 58/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3527 - val_loss: 0.3472 Epoch 59/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3526 - val_loss: 0.3521 Epoch 60/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3519 - val_loss: 0.3422 Epoch 61/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3509 - val_loss: 0.3555 Epoch 62/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3506 - val_loss: 0.3409 Epoch 63/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3500 - val_loss: 0.3394 Epoch 64/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3493 - val_loss: 0.3418 Epoch 65/100 7740/7740 [==============================] - 0s 36us/sample - loss: 0.3486 - val_loss: 0.3506 3870/3870 [==============================] - 0s 14us/sample - loss: 0.3470 7740/7740 [==============================] - 0s 15us/sample - loss: 0.3468 [CV] learning_rate=0.0012578066689117673, n_hidden=1, n_neurons=60, total= 18.6s [Parallel(n_jobs=1)]: Done 30 out of 30 | elapsed: 7.3min finished Train on 11610 samples, validate on 3870 samples Epoch 1/100 11610/11610 [==============================] - 1s 46us/sample - loss: 0.8992 - val_loss: 2.7901 Epoch 2/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.4828 - val_loss: 0.4578 Epoch 3/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.4082 - val_loss: 0.3782 Epoch 4/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3842 - val_loss: 0.4179 Epoch 5/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3707 - val_loss: 0.3564 Epoch 6/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3624 - val_loss: 0.3539 Epoch 7/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3541 - val_loss: 0.3745 Epoch 8/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3493 - val_loss: 0.4061 Epoch 9/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3448 - val_loss: 0.3380 Epoch 10/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3405 - val_loss: 0.3352 Epoch 11/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.3361 - val_loss: 0.4455 Epoch 12/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3349 - val_loss: 0.6610 Epoch 13/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3335 - val_loss: 0.4351 Epoch 14/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3284 - val_loss: 0.3318 Epoch 15/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3254 - val_loss: 0.3144 Epoch 16/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.3223 - val_loss: 0.4413 Epoch 17/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3226 - val_loss: 0.3295 Epoch 18/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.3182 - val_loss: 0.4551 Epoch 19/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.3182 - val_loss: 0.7770 Epoch 20/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3176 - val_loss: 0.8286 Epoch 21/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.3168 - val_loss: 0.4178 Epoch 22/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3118 - val_loss: 0.3867 Epoch 23/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.3082 - val_loss: 0.3094 Epoch 24/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.3063 - val_loss: 0.3018 Epoch 25/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3046 - val_loss: 0.3086 Epoch 26/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3035 - val_loss: 0.2955 Epoch 27/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.3016 - val_loss: 0.2874 Epoch 28/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2998 - val_loss: 0.3067 Epoch 29/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2987 - val_loss: 0.2855 Epoch 30/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2985 - val_loss: 0.4065 Epoch 31/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2964 - val_loss: 0.4888 Epoch 32/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2971 - val_loss: 0.6767 Epoch 33/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.2964 - val_loss: 0.4657 Epoch 34/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.2943 - val_loss: 0.3589 Epoch 35/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2922 - val_loss: 0.3978 Epoch 36/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2909 - val_loss: 0.6603 Epoch 37/100 11610/11610 [==============================] - 0s 36us/sample - loss: 0.2923 - val_loss: 0.4158 Epoch 38/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2899 - val_loss: 0.3226 Epoch 39/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2873 - val_loss: 0.2782 Epoch 40/100 11610/11610 [==============================] - 0s 36us/sample - loss: 0.2866 - val_loss: 0.3285 Epoch 41/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2850 - val_loss: 0.4074 Epoch 42/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2858 - val_loss: 0.3352 Epoch 43/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2827 - val_loss: 0.2797 Epoch 44/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.2824 - val_loss: 0.7323 Epoch 45/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2838 - val_loss: 0.7834 Epoch 46/100 11610/11610 [==============================] - 0s 34us/sample - loss: 0.2888 - val_loss: 0.6376 Epoch 47/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2844 - val_loss: 0.5744 Epoch 48/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2819 - val_loss: 0.3071 Epoch 49/100 11610/11610 [==============================] - 0s 35us/sample - loss: 0.2790 - val_loss: 0.3484 RandomizedSearchCV(cv=3, error_score='raise-deprecating', estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x11213ed68>, fit_params=None, iid='warn', n_iter=10, n_jobs=None, param_distributions={'n_hidden': [0, 1, 2, 3], 'n_neurons': array([ 1, 2, ..., 98, 99]), 'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a3bcea4a8>}, pre_dispatch='2*n_jobs', random_state=None, refit=True, return_train_score='warn', scoring=None, verbose=2) rnd_search_cv . best_params_ {'learning_rate': 0.004038022680351922, 'n_hidden': 2, 'n_neurons': 69} rnd_search_cv . best_score_ -0.3275334420977329 rnd_search_cv . best_estimator_ <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor at 0x1a3d960588> Evaluate the best model found on the test set. You can either use the best estimator's score() method, or get its underlying Keras model via its model attribute, and call this model's evaluate() method. Note that the estimator returns the negative mean square error (it's a score, not a loss, so higher is better). rnd_search_cv . score(X_test_scaled, y_test) 5160/5160 [==============================] - 0s 19us/sample - loss: 0.2968 -0.2968322378604911 model = rnd_search_cv . best_estimator_ . model model . evaluate(X_test_scaled, y_test) 5160/5160 [==============================] - 0s 15us/sample - loss: 0.2968 0.2968322378604911 Finally, save the best Keras model found. Tip : it is available via the best estimator's model attribute, and just need to call its save() method. model . save( \"my_fine_tuned_housing_model.h5\" )","title":"Hyperparameter search"}]}